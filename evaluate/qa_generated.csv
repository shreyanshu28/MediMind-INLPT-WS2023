context,question,ground_truth
"BACKGROUND/AIM: Breast cancer is a leading worldwide cause of female      cancer-related morbidity and mortality. Since molecular characteristics       increasingly guide disease management, demystifying breast tumor miRNA signature       emerges as an essential step toward personalized care. This study aimed to       investigate the variations in circulating miRNA expression profiles between       breast cancer subtypes and healthy controls and to identify relevant target genes       and molecular functions. MATERIALS AND METHODS: MiRNA expression was tested by       miScript™ miRNA PCR Array Human Cancer Pathway Finder kit, and subsequently, a       machine learning approach was applied for miRNA profiling of the various breast       cancer molecular subtypes. RESULTS: Serum samples from patients with primary       breast cancer (n=66) and healthy controls (n=16) were analyzed. MiR-21 was the       single common molecule among all breast cancer subtypes. Furthermore, several       miRNAs were found to be differentially expressed explicitly in the different       subtypes; luminal A (miR-23b, miR-142, miR-29a, miR-181d, miR-16, miR-29b,       miR-155, miR-181c), luminal B (miR-148a, let-7d, miR-92a, miR-34c, let-7b,       miR-15a), HER2+ (miR-125b, miR-134, miR-98, miR-143, miR-138, miR-135b) and       triple negative breast cancer (miR-17, miR-150, miR-210, miR-372, let-7f,       miR-191, miR-133b, miR-146b, miR-7). Finally, miRNA-associated target genes and       molecular functions were identified. CONCLUSION: Applying a machine learning       approach to delineate miRNA signatures of various breast cancer molecular       subtypes allows further understanding of molecular disease characteristics that       can prove clinically relevant.",What is the aim of the study and what methods were used to investigate the variations in circulating miRNA expression profiles between breast cancer subtypes and healthy controls?,Answer: The aim of the study was to investigate the variations in circulating miRNA expression profiles between breast cancer subtypes and healthy controls and to identify relevant target genes and molecular functions. The methods used to achieve this goal included testing miRNA expression using the miScript™ miRNA PCR Array Human Cancer Pathway Finder kit and subsequently applying a machine learning approach for miRNA profiling of the various breast cancer molecular subtypes.
"A 54-year-old man was admitted to the intensive care unit with an aneurysmal      subarachnoid hemorrhage and subsequently underwent mechanical ventilation and       received neuromuscular blocking drugs to control refractory elevated intracranial       pressure. During quantitative EEG monitoring, an automated alert was triggered by       the train of four peripheral nerve stimulation artifacts. Real-time feedback was       made possible due to remote monitoring. This case illustrates how computerized,       automated artificial intelligence algorithms can be used beyond typical seizure       detection in the intensive care unit for remote monitoring to benefit patient       care.","What is the significance of computerized, automated artificial intelligence algorithms in the intensive care unit for remote monitoring in the context of a 54-year-old man with an aneurysmal subarachnoid hemorrhage who underwent mechanical ventilation and received neuromuscular blocking drugs to control refractory elevated intracranial pressure?","Answer: The significance of computerized, automated artificial intelligence algorithms in the intensive care unit for remote monitoring in the context of a 54-year-old man with an aneurysmal subarachnoid hemorrhage who underwent mechanical ventilation and received neuromuscular blocking drugs to control refractory elevated intracranial pressure is that they can be used beyond typical seizure detection to provide real-time feedback and benefit patient care."
"A large body of research links criminality to cognitive intelligence and      personality traits. This study examined the link between emotional intelligence       (EI) and criminal behavior. One hundred Egyptian adult male offenders who have       been sentenced for theft, drug dealing or murder and 100 nonoffenders were       administered the Bar-On Emotional Quotient Inventory (EQ-i). The offenders had       lower levels of EI than the nonoffenders. In addition, EI varied as a function of       the types of offenses. Namely, it decreased in magnitude with crime severity       (lowest for murder, higher for drug dealing, and highest for theft). These       results converged with the direct/ indirect aggression theory suggesting that       indirect aggression requires more social intelligence than physical aggression.       Forensic intervention programs should therefore include EI training, especially       when violence is involved.",What is the relationship between emotional intelligence and criminal behavior?,"Answer: The study found that emotional intelligence (EI) is negatively correlated with criminal behavior. Specifically, offenders had lower levels of EI than nonoffenders, and the level of EI decreased with the severity of the crime. This suggests that indirect aggression requires more social intelligence than physical aggression, and therefore forensic intervention programs should include EI training, especially when violence is involved."
"Stimulated Raman histology (SRH) is an ex vivo optical imaging method that      enables microscopic examination of fresh tissue intraoperatively. The       conventional intraoperative method uses frozen section analysis, which is labor       and time intensive, introduces artifacts that limit diagnostic accuracy, and       consumes tissue. SRH imaging allows rapid microscopic imaging of fresh tissue,       avoids tissue loss, and enables remote telepathology review. This improves access       to expert neuropathology consultation in both low- and high-resource practices.       We clinically validated SRH by performing a blinded, retrospective two-arm       telepathology study to clinically validate SRH for telepathology at our       institution. Using surgical specimens from 47 subjects, we generated a data set       composed of 47 SRH images and 47 matched whole slide images (WSIs) of       formalin-fixed, paraffin-embedded tissue stained with hematoxylin and eosin, with       associated intraoperative clinicoradiologic information and structured diagnostic       questions. We compared diagnostic concordance between WSI and SRH-rendered       diagnoses. Also, we compared the 1-year median turnaround time (TAT) of       intraoperative conventional neuropathology frozen sections with prospectively       rendered SRH-telepathology TAT. All SRH images were of sufficient quality for       diagnostic review. A review of SRH images showed high accuracy in distinguishing       glial from nonglial tumors (96.5% SRH vs 98% WSIs) and predicting final diagnosis       (85.9% SRH vs 93.1% WSIs). SRH-based diagnosis and WSI-permanent section       diagnosis had high concordance (κ = 0.76). The median TAT for prospectively       SRH-rendered diagnosis was 3.7 minutes, approximately 10-fold shorter than the       median frozen section TAT (31 minutes). The SRH-imaging procedure did not affect       ancillary studies. SRH generates diagnostic virtual histologic images with       accuracy comparable to conventional hematoxylin and eosin-based methods in a       rapid manner. Our study represents the largest and most rigorous clinical       validation of SRH to date. It supports the feasibility of implementing SRH as a       rapid method for intraoperative diagnosis complementary to conventional pathology       laboratory methods.","What is the purpose of the study and what was the outcome of the blinded, retrospective two-arm telepathology study that clinically validated stimulated Raman histology (SRH) for telepathology at the institution?","Answer: The purpose of the study was to clinically validate SRH for telepathology at the institution. The outcome of the blinded, retrospective two-arm telepathology study showed high accuracy in distinguishing glial from nonglial tumors (96.5% SRH vs 98% WSIs) and predicting final diagnosis (85.9% SRH vs 93.1% WSIs). SRH-based diagnosis and WSI-permanent section diagnosis had high concordance (κ = 0.76). The median TAT for prospectively SRH-rendered diagnosis was 3.7 minutes, approximately 10-fold shorter than the median frozen section TAT (31 minutes). The SRH-imaging procedure did not affect ancillary studies. The study supports the feasibility of implementing SRH as a rapid method for intraoperative diagnosis complementary to conventional pathology laboratory methods."
"BACKGROUND: Artificial intelligence is gaining traction in automated medical      imaging analysis. Development of more accurate magnetic resonance imaging (MRI)       predictors of successful clinical outcomes is necessary to better define       indications for surgery, improve clinical outcomes with targeted minimally       invasive and endoscopic procedures, and realize cost savings by avoiding more       invasive spine care. OBJECTIVE: To demonstrate the ability for deep learning       neural network models to identify features in MRI DICOM datasets that represent       varying intensities or severities of common spinal pathologies and injuries and       to demonstrate the feasibility of generating automated verbal MRI reports       comparable to those produced by reading radiologists. METHODS: A 3-dimensional       (3D) anatomical model of the lumbar spine was fitted to each of the patient's       MRIs by a team of technicians. MRI T1, T2, sagittal, axial, and transverse       reconstruction image series were used to train segmentation models by the       intersection of the 3D model through these image sequences. Class definitions       were extracted from the radiologist report for the central canal: (0) no disc       bulge/protrusion/canal stenosis, (1) disc bulge without canal stenosis, (2) disc       bulge resulting in canal stenosis, and (3) disc herniation/protrusion/extrusion       resulting in canal stenosis. Both the left and right neural foramina were       assessed with either (0) neural foraminal stenosis absent, or (1) neural foramina       stenosis present. Reporting criteria for the pathologies at each disc level and,       when available, the grading of severity were extracted, and a natural language       processing model was used to generate a verbal and written report. These data       were then used to train a set of very deep convolutional neural network models,       optimizing for minimal binary cross-entropy for each classification. RESULTS: The       initial prediction validation of the implemented deep learning algorithm was done       on 20% of the dataset, which was not used for artificial intelligence training.       Of the 17,800 total disc locations for which MRI images and radiology reports       were available, 14,720 were used to train the model, and 3560 were used to       validate against. The convergence of validation accuracy achieved with the deep       learning algorithm for the foraminal stenosis detector was 81% (sensitivity =       72.4.4%, specificity = 83.1%) after 25 complete iterations through the entire       training dataset (epoch). The accuracy was 86.2% (sensitivity = 91.1%,       specificity = 82.5%) for the central stenosis detector and 85.2% (sensitivity =       81.8%, specificity = 87.4%) for the disc herniation detector. CONCLUSIONS: Deep       learning algorithms may be used for routine reporting in spine MRI. There was a       minimal disparity among accuracy, sensitivity, and specificity, indicating that       the data were not overfitted to the training set. We concluded that variability       in the training data tends to reduce overfitting and overtraining as the deep       neural network models learn to focus on the common pathologies. Future studies       should demonstrate the accuracy of deep neural network models and the predictive       value of favorable clinical outcomes with intervention and surgery. LEVEL OF       EVIDENCE: 3. CLINICAL RELEVANCE: Feasibility, clinical teaching, and evaluation       study.",What is the objective of the study and how does it plan to achieve it?,"The objective of the study is to demonstrate the ability for deep learning neural network models to identify features in MRI DICOM datasets that represent varying intensities or severities of common spinal pathologies and injuries and to demonstrate the feasibility of generating automated verbal MRI reports comparable to those produced by reading radiologists. The study plans to achieve this objective by using a 3-dimensional anatomical model of the lumbar spine to fit each patient's MRI images. MRI T1, T2, sagittal, axial, and transverse reconstruction image series were used to train segmentation models by the intersection of the 3D model through these image sequences. Class definitions were extracted from the radiologist report for the central canal and the neural foramina. Reporting criteria for the pathologies at each disc level and, when available, the grading of severity were extracted, and a natural language processing model was used to generate a verbal and written report. These data were then used to train a set of very deep convolutional neural network models, optimizing for minimal binary cross-entropy for each classification."
"Illicit drug trafficking and in particular amphetamine-type stimulants continue      to be a major problem in Australia. With the constant evolution of illicit drugs       markets, it is necessary to gain as much knowledge about them to disrupt or       reduce their impact. Illicit drug specimens can be analysed to generate forensic       intelligence and understand criminal activities. Part of this analysis involves       the evaluation of similarity scores between illicit drug profiles to interpret       the link value. Most studies utilise one of two prominent score evaluation       approaches, i.e. deterministic or Bayesian. In previous work, the notion of a       dual approach was suggested, which emphasised the complementary nature of the two       mentioned approaches. The aim of this study was to assess the operational       capability of a dual approach in evaluating similarity scores between illicit       drug profiles. Utilising a practical example, link values were generated       individually from both approaches, then compared in parallel. As a result, it was       possible to generate more informed hypotheses, relating to specimen linkage, due       to the greater wealth of information available from the two approaches working       concurrently. Additionally, it was shown that applying only one approach led to       less information being generated during analysis as well as potentially important       links between illicit drug specimens being missed.","What is the operational capability of a dual approach in evaluating similarity scores between illicit drug profiles? 
Rewritten question:
Can a dual approach provide more informed hypotheses and generate more wealth of information during the analysis of illicit drug profiles compared to using only one approach?","Yes, a dual approach can provide more informed hypotheses and generate more wealth of information during the analysis of illicit drug profiles compared to using only one approach."
"The advent of computer graphic processing units, improvement in mathematical      models and availability of big data has allowed artificial intelligence (AI)       using machine learning (ML) and deep learning (DL) techniques to achieve robust       performance for broad applications in social-media, the internet of things, the       automotive industry and healthcare. DL systems in particular provide improved       capability in image, speech and motion recognition as well as in natural language       processing. In medicine, significant progress of AI and DL systems has been       demonstrated in image-centric specialties such as radiology, dermatology,       pathology and ophthalmology. New studies, including pre-registered prospective       clinical trials, have shown DL systems are accurate and effective in detecting       diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD),       retinopathy of prematurity, refractive error and in identifying cardiovascular       risk factors and diseases, from digital fundus photographs. There is also       increasing attention on the use of AI and DL systems in identifying disease       features, progression and treatment response for retinal diseases such as       neovascular AMD and diabetic macular edema using optical coherence tomography       (OCT). Additionally, the application of ML to visual fields may be useful in       detecting glaucoma progression. There are limited studies that incorporate       clinical data including electronic health records, in AL and DL algorithms, and       no prospective studies to demonstrate that AI and DL algorithms can predict the       development of clinical eye disease. This article describes global eye disease       burden, unmet needs and common conditions of public health importance for which       AI and DL systems may be applicable. Technical and clinical aspects to build a DL       system to address those needs, and the potential challenges for clinical adoption       are discussed. AI, ML and DL will likely play a crucial role in clinical       ophthalmology practice, with implications for screening, diagnosis and follow up       of the major causes of vision impairment in the setting of ageing populations       globally.","What is the current state of artificial intelligence (AI) and machine learning (ML) in the field of ophthalmology, and what are the potential challenges for clinical adoption?","






















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































"
"Contrary to the criticism that mysterious, unaccountable black-box software      systems threaten to make the logic of critical decisions inscrutable, we argue       that algorithms are fundamentally understandable pieces of technology. Software       systems are designed to interact with the world in a controlled way and built or       operated for a specific purpose, subject to choices and assumptions. Traditional       power structures can and do turn systems into opaque black boxes, but       technologies can always be understood at a higher level, intensionally in terms       of their designs and operational goals and extensionally in terms of their       inputs, outputs and outcomes. The mechanisms of a system's operation can always       be examined and explained, but a focus on machinery obscures the key issue of       power dynamics. While structural inscrutability frustrates users and oversight       entities, system creators and operators always determine that the technologies       they deploy are fit for certain uses, making no system wholly inscrutable. We       investigate the contours of inscrutability and opacity, the way they arise from       power dynamics surrounding software systems, and the value of proposed remedies       from disparate disciplines, especially computer ethics and privacy by design. We       conclude that policy should not accede to the idea that some systems are of       necessity inscrutable. Effective governance of algorithms comes from demanding       rigorous science and engineering in system design, operation and evaluation to       make systems verifiably trustworthy. Rather than seeking explanations for each       behaviour of a computer system, policies should formalize and make known the       assumptions, choices, and adequacy determinations associated with a system.This       article is part of the theme issue 'Governing artificial intelligence: ethical,       legal, and technical opportunities and challenges'.","What is the main argument of the article ""Governing artificial intelligence: ethical, legal, and technical opportunities and challenges""?","Answer: The main argument of the article is that algorithms are fundamentally understandable pieces of technology, and that policy should not accede to the idea that some systems are of necessity inscrutable. Effective governance of algorithms comes from demanding rigorous science and engineering in system design, operation, and evaluation to make systems verifiably trustworthy. Rather than seeking explanations for each behavior of a computer system, policies should formalize and make known the assumptions, choices, and adequacy determinations associated with a system."
"MicroRNAs are small regulatory RNAs that decrease gene expression after      transcription in various biological disciplines. In bioinformatics, identifying       microRNAs and predicting their functionalities is critical. Finding motifs is one       of the most well-known and important methods for identifying the functionalities       of microRNAs. Several motif discovery techniques have been proposed, some of       which rely on artificial intelligence-based techniques. However, in the case of       few or no training data, their accuracy is low. In this research, we propose a       new computational approach, called DiMo, for identifying motifs in microRNAs and       generally macromolecules of small length. We employ word embedding techniques and       deep learning models to improve the accuracy of motif discovery results. Also, we       rely on transfer learning models to pre-train a model and use it in cases of a       lack of (enough) training data. We compare our approach with five       state-of-the-art works using three real-world datasets. DiMo outperforms the       selected related works in terms of precision, recall, accuracy and f1-score.",What is the proposed computational approach called and how does it improve the accuracy of motif discovery results in microRNAs and macromolecules of small length?,"Answer: The proposed computational approach is called DiMo. It employs word embedding techniques and deep learning models to improve the accuracy of motif discovery results. Additionally, it relies on transfer learning models to pre-train a model and use it in cases of a lack of (enough) training data."
"The capability of forensic sciences to fight crime, especially against organized      criminal groups, becomes relevant in the recent economic downturn and the war on       terrorism. In view of these societal challenges, the methods of combating crime       should experience critical changes in order to improve the effectiveness and       efficiency of the current resources available. It is obvious that authorities       have serious difficulties combating criminal groups of transnational nature.       These are characterized as well structured organizations with international       connections, abundant financial resources and comprised of members with       significant and diverse expertise. One common practice among organized criminal       groups is the use of forged documents that allow for the commission of illegal       cross-border activities. Law enforcement can target these movements to identify       counterfeits and establish links between these groups. Information on document       falsification can become relevant to generate forensic intelligence and to design       new strategies against criminal activities of this nature and magnitude. This       article discusses a methodology for improving the development of forensic       intelligence in the discipline of questioned document analysis. More       specifically, it focuses on document forgeries and falsification types used by       criminal groups. It also describes the structure of international criminal       organizations that use document counterfeits as means to conduct unlawful       activities. The model presented is partially based on practical applications of       the system that have resulted in satisfactory outcomes in our laboratory.","What is the relevance of forensic sciences in combating crime, especially against organized criminal groups, in the recent economic downturn and the war on terrorism? How can the methods of combating crime be improved to increase effectiveness and efficiency in the face of societal challenges? What are the common practices among organized criminal groups and how can law enforcement target these movements to identify counterfeits and establish links between these groups? How can information on document falsification be used to generate forensic intelligence and design new strategies against criminal activities of this nature and magnitude? What is the structure of international criminal organizations that use document counterfeits as means to conduct unlawful activities? What is the model presented in the article and how has it resulted in satisfactory outcomes in the laboratory?","Answer: The relevance of forensic sciences in combating crime, especially against organized criminal groups, in the recent economic downturn and the war on terrorism is significant. The methods of combating crime should experience critical changes in order to improve the effectiveness and efficiency of the current resources available. One common practice among organized criminal groups is the use of forged documents that allow for the commission of illegal cross-border activities. Law enforcement can target these movements to identify counterfeits and establish links between these groups. Information on document falsification can be used to generate forensic intelligence and design new strategies against criminal activities of this nature and magnitude. The structure of international criminal organizations that use document counterfeits as means to conduct unlawful activities is well-structured organizations with international connections, abundant financial resources, and comprised of members with significant and diverse expertise. The model presented in the article is a methodology for improving the development of forensic intelligence in the discipline of questioned document analysis. It focuses on document forgeries and falsification types used by criminal groups. The model has resulted in satisfactory outcomes in the laboratory through practical applications."
"BACKGROUND: Intelligent physical robots based on artificial intelligence have      been argued to bring about dramatic changes in health care services. Previous       research has examined the use of intelligent physical robots in the health care       context from different perspectives; however, an overview of the antecedents and       consequences of intelligent physical robot use in health care is lacking in the       literature. OBJECTIVE: In this paper, we aimed to provide an overview of the       antecedents and consequences of intelligent physical robot use in health care and       to propose potential agendas for future research through a systematic literature       review. METHODS: We conducted a systematic literature review on intelligent       physical robots in the health care field following the guidelines of PRISMA       (Preferred Reporting Items for Systematic Reviews and Meta-Analyses). Literature       searches were conducted in 5 databases (PubMed, Scopus, PsycINFO, Embase, and       CINAHL) in May 2021, focusing on studies using intelligent physical robots for       health care purposes. Subsequently, the quality of the included studies was       assessed using the Mixed Methods Appraisal Tool. We performed an exploratory       content analysis and synthesized the findings extracted from the included       articles. RESULTS: A total of 94 research articles were included in the review.       Intelligent physical robots, including mechanoid, humanoid, android, and       animalistic robots, have been used in hospitals, nursing homes, mental health       care centers, laboratories, and patients' homes by both end customers and health       care professionals. The antecedents for intelligent physical robot use are       categorized into individual-, organization-, and robot-related factors.       Intelligent physical robot use in the health care context leads to both       non-health-related consequences (emotional outcomes, attitude and evaluation       outcomes, and behavioral outcomes) and consequences for (physical, mental, and       social) health promotion for individual users. Accordingly, an integrative       framework was proposed to obtain an overview of the antecedents and consequences       of intelligent physical robot use in the health care context. CONCLUSIONS: This       study contributes to the literature by summarizing current knowledge in the field       of intelligent physical robot use in health care, by identifying the antecedents       and the consequences of intelligent physical robot use, and by proposing       potential future research agendas in the specific area based on the research       findings in the literature and the identified knowledge gaps.","What are the antecedents and consequences of intelligent physical robot use in the health care context, and what are potential future research agendas in this area?","Answer: The antecedents for intelligent physical robot use in the health care context include individual-, organization-, and robot-related factors. The consequences of intelligent physical robot use in the health care context are both non-health-related (emotional outcomes, attitude and evaluation outcomes, and behavioral outcomes) and consequences for (physical, mental, and social) health promotion for individual users. Potential future research agendas in this area include examining the impact of intelligent physical robot use on patient outcomes, exploring the role of robots in enhancing the quality of care, and investigating the potential of robots to support the work of healthcare professionals."
"Extreme difficulties in species identification of illegally sourced wood with      conventional tools have accelerated illicit logging activities, leading to the       destruction of natural resources in India. In this regard, the study primarily       focused on developing a DNA barcode database for 41 commercial timber tree       species which are highly vulnerable to adulteration in south India. The developed       DNA barcode database was validated using an integrated approach involving wood       anatomical features of traded wood samples collected from south India. Traded       wood samples were primarily identified using wood anatomical features using IAWA       list of microscopic features for hardwood identification. Consortium of Barcode       of Life (CBOL) recommended barcode gene regions (rbcL, matK & psbA-trnH) were       employed for developing DNA barcode database. Secondly, we employed artificial       intelligence (AI) analytical platform, Waikato Environment for Knowledge Analysis       (WEKA) for analyzing DNA barcode sequence database which could append precision,       speed, and accuracy for the entire identification process. Among the four       classification algorithms implemented in the machine learning algorithm (WEKA),       best performance was shown by SMO, which could clearly allocate individual       samples to their respective sequence database of biological reference materials       (BRM) with 100 % accuracy, indicating its efficiency in authenticating the traded       timber species. Major advantage of AI is the ability to analyze huge data sets       with more precision and also provides a large platform for rapid authentication       of species, which subsequently reduces human labor and time. SUPPLEMENTARY       INFORMATION: The online version contains supplementary material available at       10.1007/s13205-023-03604-0.",What is the study's primary focus and how was the DNA barcode database developed for 41 commercial timber tree species in south India?,"The study's primary focus is on developing a DNA barcode database for 41 commercial timber tree species which are highly vulnerable to adulteration in south India. The DNA barcode database was developed using an integrated approach involving wood anatomical features of traded wood samples collected from south India. Traded wood samples were primarily identified using wood anatomical features using IAWA list of microscopic features for hardwood identification. Consortium of Barcode of Life (CBOL) recommended barcode gene regions (rbcL, matK & psbA-trnH) were employed for developing DNA barcode database. Secondly, an artificial intelligence (AI) analytical platform, Waikato Environment for Knowledge Analysis (WEKA) was employed for analyzing DNA barcode sequence database which could append precision, speed, and accuracy for the entire identification process. Among the four classification algorithms implemented in the machine learning algorithm (WEKA), best performance was shown by SMO, which could clearly allocate individual samples to their respective sequence database of biological reference materials (BRM) with 100 % accuracy, indicating its efficiency in authenticating the traded timber species."
"The Internet of Things (IoT), big data, and artificial intelligence (AI) are all      key technologies that influence the formation and implementation of digital       medical services. Building Internet of Medical Things (IoMT) systems that combine       advanced sensors with AI-powered insights is critical for intelligent medical       systems. This paper presents an IoMT framework for brain magnetic resonance       imaging (MRI) analysis to lessen the unavoidable diagnosis and therapy faults       that occur in human clinical settings for the accurate detection of cerebral       microbleeds (CMBs). The problems in accurate CMB detection include that CMBs are       tiny dots 5-10 mm in diameter; they are similar to healthy tissues and are       exceedingly difficult to identify, necessitating specialist guidance in remote       and underdeveloped medical centers. Secondly, in the existing studies,       computer-aided diagnostic (CAD) systems are designed for accurate CMB detection,       however, their proposed approaches consist of two stages. Potential candidate       CMBs from the complete MRI image are selected in the first stage and then passed       to the phase of false-positive reduction. These pre-and post-processing steps       make it difficult to build a completely automated CAD system for CMB that can       produce results without human intervention. Hence, as a key goal of this work, an       end-to-end enhanced UNet-based model for effective CMB detection and segmentation       for IoMT devices is proposed. The proposed system requires no pre-processing or       post-processing steps for CMB segmentation, and no existing research localizes       each CMB pixel from the complete MRI image input. The findings indicate that the       suggested method outperforms in detecting CMBs in the presence of contrast       variations and similarities with other normal tissues and yields a good dice       score of 0.70, an accuracy of 99 %, as well as a false-positive rate of 0.002 %.       © 2017 Elsevier Inc. All rights reserved.",What is the proposed solution for accurate detection of cerebral microbleeds (CMBs) in human clinical settings using Internet of Medical Things (IoMT) systems and artificial intelligence (AI)?,"Answer: The proposed solution for accurate detection of cerebral microbleeds (CMBs) in human clinical settings using Internet of Medical Things (IoMT) systems and artificial intelligence (AI) is an end-to-end enhanced UNet-based model for effective CMB detection and segmentation. This model requires no pre-processing or post-processing steps for CMB segmentation and no existing research localizes each CMB pixel from the complete MRI image input. The findings indicate that the suggested method outperforms in detecting CMBs in the presence of contrast variations and similarities with other normal tissues and yields a good dice score of 0.70, an accuracy of 99 %, as well as a false-positive rate of 0.002 %."
"Clinical informatics can support quality improvement and patient safety in the      pediatric intensive care unit (PICU) in several ways including data extraction,       analysis, and decision support enabled by electronic health records (EHRs), and       databases and registries. Clinical decision support (CDS), embedded in EHRs, now       an integral part of the workflow in the PICU, includes several tools and is       increasingly leveraging artificial intelligence (AI). Understanding the       opportunities and challenges can improve the engagement of clinicians with the       design, validation, and implementation of CDS, improve satisfaction with CDS, and       improve patient safety, care quality, and value.","What are the ways in which clinical informatics can support quality improvement and patient safety in the pediatric intensive care unit (PICU) through data extraction, analysis, and decision support enabled by electronic health records (EHRs), and databases and registries?","Answer: Clinical informatics can support quality improvement and patient safety in the pediatric intensive care unit (PICU) through several ways including data extraction, analysis, and decision support enabled by electronic health records (EHRs), and databases and registries. Clinical decision support (CDS), embedded in EHRs, now an integral part of the workflow in the PICU, includes several tools and is increasingly leveraging artificial intelligence (AI). Understanding the opportunities and challenges can improve the engagement of clinicians with the design, validation, and implementation of CDS, improve satisfaction with CDS, and improve patient safety, care quality, and value."
"We report on a child, aged 4(7/12) years, with borderline intelligence quotient,      normal brain magnetic resonance imaging, and focal epilepsy. The polysomnographic       electroencephalogram recording revealed asynchronous central spikes at both brain       hemispheres resembling the features observed in focal idiopathic epileptic       syndromes. Array comparative genomic hybridization analysis revealed a 32-kb       partial deletion of the DEP domain-containing protein 5 (DEPDC5) gene, involved       in a wide spectrum of inherited focal epileptic syndromes. The parental origin of       the deletion could not be fully ascertained because the pregnancy had been       achieved through anonymous egg donation and insemination by intracytoplasmic       sperm injection. However, we demonstrate that the deletion, shared by all       alternatively spliced isoforms of DEPDC5, produces a transcript presumably       generating a DEPDC5 protein missing the entire DEP domain. Our findings suggest       that partial deletion of DEPDC5 may be sufficient to cause the focal epilepsy in       our patient, highlighting the importance of the DEP domain in DEPDC5 function.       This study expands the phenotypic spectrum of DEPDC5 to sporadic forms of focal       idiopathic epilepsy and underscores the fact that partial deletions, albeit       probably very rare, are part of the genetic spectrum of DEPDC5 mutations.","What is the significance of the partial deletion of the DEP domain-containing protein 5 (DEPDC5) gene in a child with borderline intelligence quotient, normal brain magnetic resonance imaging, and focal epilepsy?","Answer: The partial deletion of the DEP domain-containing protein 5 (DEPDC5) gene in a child with borderline intelligence quotient, normal brain magnetic resonance imaging, and focal epilepsy is significant because it produces a transcript that generates a DEPDC5 protein missing the entire DEP domain. This suggests that partial deletion of DEPDC5 may be sufficient to cause the focal epilepsy in the child. The study also expands the phenotypic spectrum of DEPDC5 to sporadic forms of focal idiopathic epilepsy and underscores the fact that partial deletions, albeit probably very rare, are part of the genetic spectrum of DEPDC5 mutations."
"The need for assessment of social skills in clinical and neurotypical populations      has led to the widespread, and still increasing use of the 'Reading the Mind in       the Eyes Test' (RMET) developed more than two decades ago by Simon Baron-Cohen       and colleagues for evaluation of social cognition in autism. By analyzing most       recent clinical and brain imaging data, we illuminate a set of factors decisive       for using the RMET. Converging evidence indicates: (i) In neurotypical       individuals, RMET scores are tightly correlated with other social skills       (empathy, emotional intelligence, and body language reading); (ii) The RMET       assesses recognition of facial affect, but also heavily relies on receptive       language skills, semantic knowledge, and memory; (iii) RMET performance is       underwritten by the large-scale ensembles of neural networks inside and       well-outside the social brain; (iv) The RMET is limited in its capacity to       differentiate between neuropsychiatric conditions as well as between stages and       severity of a single disorder, though it reliably distinguishes individuals with       altered social cognition or elevated pathological traits from neurotypical       persons; (v) Merely gender (as a social construct) rather than neurobiological       sex influences performance on the RMET; (vi) RMET scores do not substantially       decline in healthy aging, and they are higher with higher education level,       cognitive abilities, literacy, and mental well-being; (vii) Accuracy on the RMET,       and engagement of the social brain, are greater when emotions are expressed and       recognized by individuals with similar cultural/ethnic background. Further       research is required to better inform usage of the RMET as a tool for swift and       reliable examination of social cognition. In light of comparable visual input       from the RMET images and faces covered by masks due to COVID-19 regulations, the       analysis is of value for keeping efficient social interaction during the current       pandemic, in particular, in professional settings related to social       communication.",What are the factors that influence the use of the 'Reading the Mind in the Eyes Test' (RMET) for evaluating social cognition in autism and other clinical and neurotypical populations?,"Answer: The 'Reading the Mind in the Eyes Test' (RMET) is a widely used tool for evaluating social cognition in autism and other clinical and neurotypical populations. The factors that influence its use include the correlation of RMET scores with other social skills, the assessment of facial affect, the reliance on receptive language skills, semantic knowledge, and memory, the involvement of large-scale neural networks, the limited capacity to differentiate between neuropsychiatric conditions and stages and severity of a single disorder, the influence of gender on performance, the stability of RMET scores in healthy aging, the higher scores with higher education level, cognitive abilities, literacy, and mental well-being, and the greater accuracy and engagement of the social brain when emotions are expressed and recognized by individuals with similar cultural/ethnic background."
"Ultimately, new (digital) techniques and artificial intelligence (AI)      applications are changing the working environment in urology. This can be an       opportunity for further development, but also a change which is not desired.       Adjustments to work processes may be necessary. So-called disruptive processes       lead to fundamental changes. In the context of the digital transformation, our       way of working is changing. Classic hierarchies, working hours, and working       environments are dissolving in favor of creative and flexible working models and       corporate structures. Clinics and practices in urology must prepare themselves       for changing requirements and be able to provide answers.",What are the potential benefits and drawbacks of the digital transformation in urology?,"
Answer: The digital transformation in urology presents both potential benefits and drawbacks. On the one hand, new digital techniques and AI applications can improve the efficiency and accuracy of medical procedures, allowing for better patient outcomes. Additionally, the use of telemedicine and remote monitoring can increase access to care and reduce costs. However, the digital transformation can also lead to changes in the working environment that may not be desired, such as the dissolution of classic hierarchies and working environments. Adjustments to work processes may be necessary, and there may be concerns about data privacy and security. Overall, the digital transformation in urology presents both opportunities and challenges that must be carefully considered and addressed."
"Evidence from neuropsychology and neuroimaging indicate that the pre-frontal      cortex (PFC) plays an important role in human memory. Although frontal patients       are able to form new memories, these memories appear qualitatively different from       those of controls by lacking distinctiveness. Neuroimaging studies of memory       indicate activation in the PFC under deep encoding conditions, and under       conditions of semantic elaboration. Based on these results, we hypothesize that       the PFC enhances memory by extracting differences and commonalities in the       studied material. To test this hypothesis, we carried out an experimental       investigation to test the relationship between the PFC-dependent factors and       semantic factors associated with common and specific features of words. These       experiments were performed using Free-Recall of word lists with healthy adults,       exploiting the correlation between PFC function and fluid intelligence. As       predicted, a correlation was found between fluid intelligence and the       Von-Restorff effect (better memory for semantic isolates, e.g., isolate ""cat""       within category members of ""fruit""). Moreover, memory for the semantic isolate       was found to depend on the isolate's serial position. The isolate item tends to       be recalled first, in comparison to non-isolates, suggesting that the process       interacts with short term memory. These results are captured within a       computational model of free recall, which includes a PFC mechanism that is       sensitive to both commonality and distinctiveness, sustaining a trade-off between       the two.",What is the role of the pre-frontal cortex (PFC) in human memory and how does it enhance memory by extracting differences and commonalities in the studied material?,"Answer: The pre-frontal cortex (PFC) plays an important role in human memory by enhancing memory by extracting differences and commonalities in the studied material. This is supported by evidence from neuropsychology and neuroimaging, which indicate that the PFC is activated under deep encoding conditions and during semantic elaboration. Experimental investigations have shown a correlation between PFC-dependent factors and semantic factors associated with common and specific features of words. Specifically, memory for semantic isolates (e.g., ""cat"" within category members of ""fruit"") tends to be better recalled first, in comparison to non-isolates, suggesting that the process interacts with short-term memory. These results are captured within a computational model of free recall, which includes a PFC mechanism that is sensitive to both commonality and distinctiveness, sustaining a trade-off between the two."
"Although various models of ethical decision making (EDM) have implicitly called      upon constructs governed by working memory capacity (WMC), a study examining this       relationship specifically has not been conducted. Using a sense making framework       of EDM, we examined the relationship between WMC and various sensemaking       processes contributing to EDM. Participants completed an online assessment       comprised of a demographic survey, intelligence test, various EDM measures, and       the Automated Operation Span task to determine WMC. Results indicated that WMC       accounted for unique variance above and beyond ethics education, exposure to       ethical issues, and intelligence in several sensemaking processes. Additionally,       a marginally significant effect of WMC was also found with reference to EDM.       Individual differences in WMC appear likely to play an important role in the       ethical decision-making process, and future researchers may wish to consider       their potential influences.",What is the relationship between working memory capacity (WMC) and various sensemaking processes contributing to ethical decision making (EDM)?,"Answer: The study found that WMC accounted for unique variance above and beyond ethics education, exposure to ethical issues, and intelligence in several sensemaking processes. Additionally, a marginally significant effect of WMC was also found with reference to EDM. Individual differences in WMC appear likely to play an important role in the ethical decision-making process, and future researchers may wish to consider their potential influences."
"When risky child and family circumstances cannot be resolved at home, (temporary)      24-h out-of-home placement of the child may be an alternative strategy. To       identify specific placement risks and needs, care professionals must have       information about the child and his or her family, care history, and       social-cultural characteristics at admission to out-of-home care. However, to       date information on case characteristics and particular their similarities and       differences across the three main types of out-of-home settings (namely foster       care, family-style group care, and residential care) is largely lacking. This       review compiles and compares characteristics of school-aged children of average       intelligence and their families at the time of each child's admission to one of       the three care modalities. A scoping review technique that provides a broad       search strategy and ensures sufficient coverage of the available literature is       used. Based on the 36 studies included, there is consensus that the majority of       normally intelligent children in care demonstrate severe developmental and       behavioral problems. However, the severeness as well as the kinds of defining       characteristics present differ among the children in foster care, family-style       group care, and residential care. The review also identifies several existing       knowledge gaps regarding relevant risk factors. Future research is recommended to       fill these gaps and determine the developmental pathway in relation to children's       risks and needs at admission. This will contribute to the development of an       evidence-based risks and needs assessment tool that will enable care       professionals to make informed referrals to a specific type of out-of-home care       when such a placement is required.","What are the characteristics of school-aged children of average intelligence and their families at the time of admission to out-of-home care, and how do they differ among the three main types of out-of-home settings (foster care, family-style group care, and residential care)?","Answer: Based on the 36 studies included in the review, there is consensus that the majority of normally intelligent children in care demonstrate severe developmental and behavioral problems. However, the severeness as well as the kinds of defining characteristics present differ among the children in foster care, family-style group care, and residential care. The review also identifies several existing knowledge gaps regarding relevant risk factors. Future research is recommended to fill these gaps and determine the developmental pathway in relation to children's risks and needs at admission. This will contribute to the development of an evidence-based risks and needs assessment tool that will enable care professionals to make informed referrals to a specific type of out-of-home care when such a placement is required."
"BACKGROUND: Children with perinatal HIV (pHIV) may display distinct long-term      cognitive phenotypes. We used group-based trajectory modeling to identify       clusters of children with pHIV after similar developmental trajectories and       predictors of belonging to select cognitive trajectory groups. METHODS:       Participants included children, 4-17 years of age, with pHIV in Thailand and       Cambodia. Cognitive measures included translated versions of Intelligence       Quotient tests, Color Trails Tests and Beery-Buktenica Developmental Test of       Visual-Motor Integration conducted semiannually over 3-6 years. The best fit of       trajectory groups was determined using maximum likelihood estimation.       Multivariate logistic regression identified baseline factors associated with       belonging to the lowest scoring trajectory group. RESULTS: Group-based trajectory       analyses revealed a 3-cluster classification for each cognitive test, labeled as       high, medium and low scoring groups. Most trajectory group scores remained stable       across age. Verbal IQ declined in all 3 trajectory groups and the high scoring       group for Children's Color Trails Test 1 and 2 showed an increase in scores       across age. Children in the lowest scoring trajectory group were more likely to       present at an older age and report lower household income. CONCLUSIONS:       Group-based trajectory modeling succinctly classifies cohort heterogeneity in       cognitive outcomes in pHIV. Most trajectories remained stable across age       suggesting that cognitive potential is likely determined at an early age with the       exception of a small subgroup of children who displayed developmental gains in       select cognitive domains and may represent those with better cognitive reserve.       Poverty and longer duration of untreated HIV may predispose children with pHIV to       suboptimal cognitive development.",What are the cognitive phenotypes observed in children with perinatal HIV (pHIV) and how do they differ from those without pHIV?,"Answer: The cognitive phenotypes observed in children with perinatal HIV (pHIV) may differ from those without pHIV. Group-based trajectory modeling was used to identify clusters of children with pHIV after similar developmental trajectories and predictors of belonging to select cognitive trajectory groups. The results revealed a 3-cluster classification for each cognitive test, labeled as high, medium and low scoring groups. Most trajectory group scores remained stable across age, except for a small subgroup of children who displayed developmental gains in select cognitive domains. Verbal IQ declined in all 3 trajectory groups and the high scoring group for Children's Color Trails Test 1 and 2 showed an increase in scores across age. Children in the lowest scoring trajectory group were more likely to present at an older age and report lower household income."
"In this review, we aim to assess the current state of science in relation to the      integration of patient-generated health data (PGHD) and patient-reported outcomes       (PROs) into routine clinical care with a focus on surgical oncology populations.       We will also describe the critical role of artificial intelligence and       machine-learning methodology in the efficient translation of PGHD, PROs, and       traditional outcome measures into meaningful patient care models.","What is the current state of science in relation to the integration of patient-generated health data and patient-reported outcomes into routine clinical care, with a focus on surgical oncology populations? How does artificial intelligence and machine learning methodology play a critical role in the efficient translation of PGHD, PROs, and traditional outcome measures into meaningful patient care models?","Answer: The current state of science in relation to the integration of patient-generated health data and patient-reported outcomes into routine clinical care, with a focus on surgical oncology populations, is still in its early stages of development. However, there is growing interest in the use of these data sources to improve patient outcomes and enhance the quality of care. Artificial intelligence and machine learning methodology play a critical role in the efficient translation of PGHD, PROs, and traditional outcome measures into meaningful patient care models by enabling the automated analysis and interpretation of large amounts of data. This can help clinicians identify patterns and trends that may not be immediately apparent, and make more informed decisions about patient care. Additionally, these technologies can help to personalize care plans based on individual patient needs and preferences, leading to better outcomes and increased patient satisfaction."
"BACKGROUND: Neuroimaging differentiation of glioblastoma, primary central nervous      system lymphoma (PCNSL) and solitary brain metastasis (BM) remains challenging in       specific cases showing similar appearances or atypical features. Overall,       advanced MRI protocols have high diagnostic reliability, but their limited       worldwide availability, coupled with the overlapping of specific neuroimaging       features among tumor subgroups, represent significant drawbacks and entail       disparities in the planning and management of these oncological patients.       OBJECTIVE: To evaluate the classification performance metrics of a deep learning       algorithm trained on T1-weighted gadolinium-enhanced (T1Gd) MRI scans of       glioblastomas, atypical PCNSLs and BMs. MATERIALS AND METHODS: We enrolled 121       patients (glioblastoma: n=47; PCNSL: n=37; BM: n=37) who had undergone       preoperative T1Gd-MRI and histopathological confirmation. Each lesion was       segmented, and all ROIs were exported in a DICOM dataset. The patient cohort was       then split in a training and hold-out test sets following a 70/30 ratio. A       Resnet101 model, a deep neural network (DNN), was trained on the training set and       validated on the hold-out test set to differentiate glioblastomas, PCNSLs and BMs       on T1Gd-MRI scans. RESULTS: The DNN achieved optimal classification performance       in distinguishing PCNSLs (AUC: 0.98; 95%CI: 0.95 - 1.00) and glioblastomas (AUC:       0.90; 95%CI: 0.81 - 0.97) and moderate ability in differentiating BMs (AUC: 0.81;       95%CI: 0.70 - 0.95). This performance may allow clinicians to correctly identify       patients eligible for lesion biopsy or surgical resection. CONCLUSION: We trained       and internally validated a deep learning model able to reliably differentiate       ambiguous cases of PCNSLs, glioblastoma and BMs by means of T1Gd-MRI. The       proposed predictive model may provide a low-cost, easily-accessible and       high-speed decision-making support for eligibility to diagnostic brain biopsy or       maximal tumor resection in atypical cases.","What is the classification performance of a deep learning algorithm trained on T1-weighted gadolinium-enhanced (T1Gd) MRI scans of glioblastomas, atypical primary central nervous system lymphoma (PCNSLs) and solitary brain metastasis (BMs)?",The deep learning algorithm achieved optimal classification performance in distinguishing PCNSLs and glioblastomas and moderate ability in differentiating BMs.
"This article examines the duality of the Black psychology movement in the United      States as both a distinctly American and a postcolonial approach to mental       health. The Westside Community Mental Health Center in San Francisco served as       the organizational hub for the Association for Black Psychologists (ABPsi) in the       1970s. The Westside clinicians understood forensic psychology as a kind of       preventative care as California, more so than any other state, was seduced by the       eugenic dream of human improvement through therapeutic interventions in schools       and prisons intended to correct the wayward deviant. Their community's mental       wellbeing required dismantling the interlinked disciplinary apparatus which       disproportionately surveyed, tracked, and confined young Black men. These       psychologists mounted a legal challenge to the use of intelligence testing to       sort Black children in schools, seeking to replace standardized tests with       ""dynamic assessments"" inspired by Israeli psychologist Reuven Feuerstein's work       with refugee children. They consulted on the voir dire process in the highly       politicized Angela Davis trial to minimize the presence of racially prejudiced       jurors. They offered expert testimony on the psychological damage of solitary       confinement on behalf of prison activists. The Westside team artfully developed       and deployed the psychological concept of ""bias"" in their confrontations with       local manifestations of the American carceral state. In their theoretical       writings, these psychologists reflected upon their historical positionality,       understanding themselves as products of the decolonial moment. Bay Area       encounters with Third World internationalism, the Black Panther Party (BPP), the       Nation of Islam, and community-led substance abuse programs shaped clinical care       at Westside and inspired the Afrocentric consciousness many came to espouse.       ABPsi initially had a significant impact on the historically white American       Psychological Association's training practices. However, the two organizations       split over the IQ controversy at a moment when psychologists became increasingly       enmeshed in the criminal justice system.",What was the role of the Westside Community Mental Health Center in the development of the Association for Black Psychologists (ABPsi) and how did it contribute to the Black psychology movement in the United States?,"The Westside Community Mental Health Center in San Francisco served as the organizational hub for the Association for Black Psychologists (ABPsi) in the 1970s. The Westside clinicians understood forensic psychology as a kind of preventative care as California, more so than any other state, was seduced by the eugenic dream of human improvement through therapeutic interventions in schools and prisons intended to correct the wayward deviant. Their community's mental wellbeing required dismantling the interlinked disciplinary apparatus which disproportionately surveyed, tracked, and confined young Black men. These psychologists mounted a legal challenge to the use of intelligence testing to sort Black children in schools, seeking to replace standardized tests with ""dynamic assessments"" inspired by Israeli psychologist Reuven Feuerstein's work with refugee children. They consulted on the voir dire process in the highly politicized Angela Davis trial to minimize the presence of racially prejudiced jurors. They offered expert testimony on the psychological damage of solitary confinement on behalf of prison activists. The Westside team artfully developed and deployed the psychological concept of ""bias"" in their confrontations with local manifestations of the American carceral state. In their theoretical writings, these psychologists reflected upon their historical positionality, understanding themselves as products of the decolonial moment. Bay Area encounters with Third World internationalism, the Black Panther Party (BPP), the Nation of Islam, and community-led substance abuse programs shaped clinical care at Westside and inspired the Afrocentric consciousness many came to espouse. ABPsi initially had a significant impact on the historically white American Psychological Association's training practices. However, the two organizations split over the IQ controversy at a moment when psychologists became increasingly enmeshed in the criminal justice system."
"INTRODUCTION: Cultural intelligence and social compatibility are two acquired      processes that their education and reinforcement between dormitory's students who       have inter cultural interactions with each other can conclude with results that       tension diminution, inter cultural contrast and conflict, social divisions and       consequently healthy and peaceful relationships and governance and finally mental       peace, and health are of its most important. Hence, the research has been       occurring in order to the determination of cultural intelligence relationship       with the social compatibility of Isfahan University of Medical Sciences       dormitories resident students in 2012. MATERIALS AND METHODS: The research method       is descriptive-correlation, and its population is composed of all Isfahan       University of Medical Sciences dormitories resident students in 2012 that were       totally 2500 persons. The two steps sampling method have been used, group       sampling and random sampling has been occurring at first and second steps and       totally 447 persons were selected. Research data were collected via Earley and       Ang cultural intelligence questionnaire with 0.76 Cronbach's alpha Coefficient       and California social compatibility standard questionnaire with higher than 0.70       Cronbach's alpha factor. Questionnaire data have been analyzed with the SPSS       software and results have been presented in the shape of descriptions and       statistics. FINDINGS: Results showed that there is a direct significant       relationship (P < 0.001) between cultural intelligence and the social adjustment       in students living in Isfahan University of Medical Sciences dormitories and also       there is a direct significant relationship in the level of (P < 0.05) between       cognitive and motivational dimensions of cultural intelligence; however, there is       no significant relationship between cognitive and behavioral dimensions of       cultural intelligence and social adjustment (P > 0.05). CONCLUSION: Cultural       intelligence and cognitive and motivational addition in dimensions of students       living in Isfahan University of Medical Sciences dormitories increase their       social integration, therefore, cultural intelligence and social adjustment of       students can be increased through planning and we can try for their mental health       by this way.",What is the relationship between cultural intelligence and social adjustment among students living in Isfahan University of Medical Sciences dormitories?,"The research has found a direct significant relationship (P < 0.001) between cultural intelligence and social adjustment among students living in Isfahan University of Medical Sciences dormitories. Additionally, there is a direct significant relationship (P < 0.05) between the cognitive and motivational dimensions of cultural intelligence and social adjustment, but there is no significant relationship between the cognitive and behavioral dimensions of cultural intelligence and social adjustment (P > 0.05)."
"OBJECTIVES: Despite documented benefits and physicians' recommendations to      participate in cardiac rehabilitation (CR) programs, the average dropout rate       remains between 12-56%. This study's goal was to demonstrate that using       personalized interventions can significantly increase patient adherence. METHOD:       Ninety-five patients (ages 18-90) eligible for the CR program were randomly       recruited and received personalized interventions using the Well-Beat system.       Adherence levels were compared to those of a historical control group. The       Well-Beat system provided Sheba CR Health Care Provider (HCP) guidelines for       personalized patient-therapist dialogue. The system also generated ongoing       personalized text messages for each patient sent twice a week and related each       patient's dynamic profile to their daily behavior, creating continuity, and       reinforcing the desired behavior. RESULTS: A significant increase in patient       adherence to the CR program: Three months after initiation, 76% remained active       compared to the historical average of 24% in the matched control group (log-rank       p-value = 0.001). CONCLUSIONS: Using an Artificial Intelligence (AI)-based engine       that generated recommendations and messages made it possible to improve patient       adherence without increasing HCP load, benefiting all. Presenting customized       patient insights to the HCP and generating personalized communications along with       action motivating text messages can also be useful for remote care.",What is the goal of the study and how does the Well-Beat system contribute to achieving this goal?,"The goal of the study is to demonstrate that using personalized interventions can significantly increase patient adherence to the CR program. The Well-Beat system contributes to achieving this goal by providing Sheba CR Health Care Provider (HCP) guidelines for personalized patient-therapist dialogue, generating ongoing personalized text messages for each patient sent twice a week, and related each patient's dynamic profile to their daily behavior, creating continuity, and reinforcing the desired behavior."
"In materials science, the investigation of a large and complex experimental space      is time-consuming and thus may induce bias to exclude potential solutions where       little to no knowledge is available. This work presents the development of a       highly hydrophobic material from an amphiphilic polymer through a novel, adaptive       artificial intelligence approach. The hydrophobicity arises from the random       packing of short polymer fibers into paper, a highly entropic, multistep process.       Using Bayesian optimization, the algorithm is able to efficiently navigate the       parameter space without bias, including areas which a human experimenter would       not address. This resulted in additional knowledge gain, which can then be       applied to the fabrication process, resulting in a highly hydrophobic material       (static water contact angle 135°) from an amphiphilic polymer (contact angle of       90°) through a simple and scalable filtration-based method. This presents a       potential pathway for surface modification using the short polymer fibers to       create fluorine-free hydrophobic surfaces on a larger scale.",What is the significance of using Bayesian optimization in the development of a highly hydrophobic material from an amphiphilic polymer?,"Answer: The significance of using Bayesian optimization in the development of a highly hydrophobic material from an amphiphilic polymer is that it allows for efficient navigation of the parameter space without bias, including areas which a human experimenter would not address. This results in additional knowledge gain, which can then be applied to the fabrication process, resulting in a highly hydrophobic material with a static water contact angle of 135° from an amphiphilic polymer with a contact angle of 90° through a simple and scalable filtration-based method. This presents a potential pathway for surface modification using the short polymer fibers to create fluorine-free hydrophobic surfaces on a larger scale."
"Artificial intelligence (AI) has experienced substantial progress over the last      ten years in many fields of application, including healthcare. In hepatology and       pancreatology, major attention to date has been paid to its application to the       assisted or even automated interpretation of radiological images, where AI can       generate accurate and reproducible imaging diagnosis, reducing the physicians'       workload. AI can provide automatic or semi-automatic segmentation and       registration of the liver and pancreatic glands and lesions. Furthermore, using       radiomics, AI can introduce new quantitative information which is not visible to       the human eye to radiological reports. AI has been applied in the detection and       characterization of focal lesions and diffuse diseases of the liver and pancreas,       such as neoplasms, chronic hepatic disease, or acute or chronic pancreatitis,       among others. These solutions have been applied to different imaging techniques       commonly used to diagnose liver and pancreatic diseases, such as ultrasound,       endoscopic ultrasonography, computerized tomography (CT), magnetic resonance       imaging, and positron emission tomography/CT. However, AI is also applied in this       context to many other relevant steps involved in a comprehensive clinical       scenario to manage a gastroenterological patient. AI can also be applied to       choose the most convenient test prescription, to improve image quality or       accelerate its acquisition, and to predict patient prognosis and treatment       response. In this review, we summarize the current evidence on the application of       AI to hepatic and pancreatic radiology, not only in regard to the interpretation       of images, but also to all the steps involved in the radiological workflow in a       broader sense. Lastly, we discuss the challenges and future directions of the       clinical application of AI methods.","What is the current state of artificial intelligence (AI) in hepatology and pancreatology, specifically in the field of radiology, and what are the challenges and future directions for its clinical application?","Answer: Artificial intelligence (AI) has made significant progress in hepatology and pancreatology, particularly in the field of radiology. AI can assist or even automate the interpretation of radiological images, providing accurate and reproducible imaging diagnosis, reducing the workload of physicians. AI can also provide automatic or semi-automatic segmentation and registration of the liver and pancreatic glands and lesions, as well as introduce new quantitative information through radiomics. AI has been applied to the detection and characterization of focal lesions and diffuse diseases of the liver and pancreas, such as neoplasms, chronic hepatic disease, or acute or chronic pancreatitis, among others. AI can also be used in other relevant steps involved in a comprehensive clinical scenario to manage a gastroenterological patient, such as choosing the most convenient test prescription, improving image quality or accelerating its acquisition, and predicting patient prognosis and treatment response. However, there are challenges to the clinical application of AI methods, such as ensuring the accuracy and reliability of AI algorithms, addressing concerns about data privacy and security, and ensuring that AI is used ethically and responsibly. Future directions for the clinical application of AI in hepatology and pancreatology include further development of AI algorithms to improve accuracy and reliability, expanding the use of AI to other imaging techniques, and integrating AI into clinical workflows to improve efficiency and patient outcomes."
"Identifying chemical odors rapidly and accurately is critical in a variety of      fields. Due to the limited human sense of smell, much effort has been dedicated       to the development of electronic sensing devices. Despite some recent progress,       such devices are still no match for the capabilities of biological (animal)       olfactory sensors, which are light, robust, versatile, and sensitive.       Consequently, scientists are turning to a new approach: Bio-Hybrid sensors. These       sensors combine animal biological sensors with electronic components to achieve       maximum detection and classification while conveying a comprehensible signal to       the end user. In this work, we created a bio-hybrid odor discriminator utilizing       the desert locust's primary olfactory apparatus - its antennae, together with       simple electroantennogram technology and artificial intelligence tools for signal       analysis. Our discriminator is able to differentiate between at least eight pure       odors and two mixtures of different odorants, independently of odorant       concentration. With four orders of magnitude higher sensitivity than gas       chromatography-mass spectrometry, it is able to detect the presence of less than       1 ng of volatile compounds and, compared to other bio-hybrid sensors available       today, it can be easily operated by an unskilled individual. This study thus       opens up the future for robust and simple bio-hybrid robotic sensing devices that       can be widely deployed.",What is the purpose of the study and what is the significance of the results obtained in the creation of a bio-hybrid odor discriminator?,"The purpose of the study is to develop a bio-hybrid odor discriminator that can differentiate between at least eight pure odors and two mixtures of different odorants, independently of odorant concentration. The significance of the results obtained in the creation of this bio-hybrid odor discriminator is that it is able to detect the presence of less than 1 ng of volatile compounds and can be easily operated by an unskilled individual. This makes it a robust and simple bio-hybrid robotic sensing device that can be widely deployed."
"BACKGROUND: Deep learning-based head and neck lymph node level (HN_LNL)      autodelineation is of high relevance to radiotherapy research and clinical       treatment planning but still underinvestigated in academic literature. In       particular, there is no publicly available open-source solution for large-scale       autosegmentation of HN_LNL in the research setting. METHODS: An expert-delineated       cohort of 35 planning CTs was used for training of an nnU-net       3D-fullres/2D-ensemble model for autosegmentation of 20 different HN_LNL. A       second cohort acquired at the same institution later in time served as the test       set (n = 20). In a completely blinded evaluation, 3 clinical experts rated the       quality of deep learning autosegmentations in a head-to-head comparison with       expert-created contours. For a subgroup of 10 cases, intraobserver variability       was compared to the average deep learning autosegmentation accuracy on the       original and recontoured set of expert segmentations. A postprocessing step to       adjust craniocaudal boundaries of level autosegmentations to the CT slice plane       was introduced and the effect of autocontour consistency with CT slice plane       orientation on geometric accuracy and expert rating was investigated. RESULTS:       Blinded expert ratings for deep learning segmentations and expert-created       contours were not significantly different. Deep learning segmentations with slice       plane adjustment were rated numerically higher (mean, 81.0 vs. 79.6, p = 0.185)       and deep learning segmentations without slice plane adjustment were rated       numerically lower (77.2 vs. 79.6, p = 0.167) than manually drawn contours. In a       head-to-head comparison, deep learning segmentations with CT slice plane       adjustment were rated significantly better than deep learning contours without       slice plane adjustment (81.0 vs. 77.2, p = 0.004). Geometric accuracy of deep       learning segmentations was not different from intraobserver variability (mean       Dice per level, 0.76 vs. 0.77, p = 0.307). Clinical significance of contour       consistency with CT slice plane orientation was not represented by geometric       accuracy metrics (volumetric Dice, 0.78 vs. 0.78, p = 0.703). CONCLUSIONS: We       show that a nnU-net 3D-fullres/2D-ensemble model can be used for highly accurate       autodelineation of HN_LNL using only a limited training dataset that is ideally       suited for large-scale standardized autodelineation of HN_LNL in the research       setting. Geometric accuracy metrics are only an imperfect surrogate for blinded       expert rating.",What is the difference between the accuracy of deep learning-based head and neck lymph node level (HN_LNL) autodelineation and expert-created contours in a blinded evaluation?,Answer: The difference between the accuracy of deep learning-based head and neck lymph node level (HN_LNL) autodelineation and expert-created contours in a blinded evaluation was not significant.
"BACKGROUND: A cheap and minimum-invasive method for early identification of      Alzheimer's disease (AD) pathogenesis is key to disease management and the       success of emerging treatments targeting the prodromal phases of the disease.       OBJECTIVE: To develop a machine learning-based blood panel to predict the       progression from mild cognitive impairment (MCI) to dementia due to AD within a       four-year time-to-conversion horizon. METHODS: We created over one billion models       to predict the probability of conversion from MCI to dementia due to AD and chose       the best-performing one. We used Alzheimer's Disease Neuroimaging Initiative       (ADNI) data of 379 MCI individuals in the baseline visit, from which 176       converted to AD dementia. RESULTS: We developed a machine learning-based panel       composed of 12 plasma proteins (ApoB, Calcitonin, C-peptide, CRP, IGFBP-2,       Interleukin-3, Interleukin-8, PARC, Serotransferrin, THP, TLSP 1-309, and TN-C),       and which yielded an AUC of 0.91, accuracy of 0.91, sensitivity of 0.84, and       specificity of 0.98 for predicting the risk of MCI patients converting to       dementia due to AD in a horizon of up to four years. CONCLUSION: The proposed       machine learning model was able to accurately predict the risk of MCI patients       converting to dementia due to AD in a horizon of up to four years, suggesting       that this model could be used as a minimum-invasive tool for clinical decision       support. Further studies are needed to better clarify the possible       pathophysiological links with the reported proteins.",What is the accuracy of a machine learning-based blood panel composed of 12 plasma proteins in predicting the risk of mild cognitive impairment (MCI) patients converting to dementia due to Alzheimer's disease (AD) in a horizon of up to four years?,The accuracy of the machine learning-based blood panel composed of 12 plasma proteins in predicting the risk of mild cognitive impairment (MCI) patients converting to dementia due to Alzheimer's disease (AD) in a horizon of up to four years is 0.91.
"BACKGROUND: Fracture detection by artificial intelligence and especially Deep      Convolutional Neural Networks (DCNN) is a topic of growing interest in current       orthopaedic and radiological research. As learning a DCNN usually needs a large       amount of training data, mostly frequent fractures as well as conventional X-ray       are used. Therefore, less common fractures like acetabular fractures (AF) are       underrepresented in the literature. The aim of this pilot study was to establish       a DCNN for detection of AF using computer tomography (CT) scans. METHODS:       Patients with an acetabular fracture were identified from the monocentric       consecutive pelvic injury registry at the BG Trauma Center XXX from       01/2003 - 12/2019. All patients with unilateral AF and CT scans available in       DICOM-format were included for further processing. All datasets were       automatically anonymised and digitally post-processed. Extraction of the relevant       region of interests was performed and the technique of data augmentation (DA) was       implemented to artificially increase the number of training samples. A DCNN based       on Med3D was used for autonomous fracture detection, using global average pooling       (GAP) to reduce overfitting. RESULTS: From a total of 2,340 patients with a       pelvic fracture, 654 patients suffered from an AF. After screening and       post-processing of the datasets, a total of 159 datasets were enrolled for       training of the algorithm. A random assignment into training datasets (80%) and       test datasets (20%) was performed. The technique of bone area extraction, DA and       GAP increased the accuracy of fracture detection from 58.8% (native DCNN) up to       an accuracy of 82.8% despite the low number of datasets. CONCLUSION: The accuracy       of fracture detection of our trained DCNN is comparable to published values       despite the low number of training datasets. The techniques of bone extraction,       DA and GAP are useful for increasing the detection rates of rare fractures by a       DCNN. Based on the used DCNN in combination with the described techniques from       this pilot study, the possibility of an automatic fracture classification of AF       is under investigation in a multicentre study.","What is the accuracy of fracture detection using a Deep Convolutional Neural Network (DCNN) based on Med3D with global average pooling (GAP) for acetabular fractures (AF) using computer tomography (CT) scans, and how does the technique of bone area extraction, data augmentation (DA), and GAP increase the accuracy of fracture detection?","Answer: The accuracy of fracture detection using a Deep Convolutional Neural Network (DCNN) based on Med3D with global average pooling (GAP) for acetabular fractures (AF) using computer tomography (CT) scans is 82.8%. The technique of bone area extraction, data augmentation (DA), and GAP increases the accuracy of fracture detection by 24%."
"The urban heat island is a vastly documented climatological phenomenon, but when      it comes to coastal cities, close to desert areas, its analysis becomes extremely       challenging, given the high temporal variability and spatial heterogeneity. The       strong dependency on the synoptic weather conditions, rather than on       city-specific, constant features, hinders the identification of recurrent       patterns, leading conventional predicting algorithms to fail. In this paper, an       advanced artificial intelligence technique based on long short-term memory (LSTM)       model is applied to gain insight and predict the highly fluctuating heat island       intensity (UHII) in the city of Sydney, Australia, governed by the dualistic       system of cool sea breeze from the ocean and hot western winds from the vast       desert biome inlands. Hourly measurements of temperature, collected for a period       of 18 years (1999-2017) from 8 different sites in a 50 km radius from the       coastline, were used to train (80%) and test (20%) the model. Other inputs       included date, time, and previously computed UHII, feedbacked to the model with       an optimized time step of six hours. A second set of models integrated wind speed       at the reference station to account for the sea breeze effect. The R(2) ranged       between 0.770 and 0.932 for the training dataset and between 0.841 and 0.924 for       the testing dataset, with the best performance attained right in correspondence       of the city hot spots. Unexpectedly, very little benefit (0.06-0.43%) was       achieved by including the sea breeze among the input variables. Overall, this       study is insightful of a rather rare climatological case at the watershed between       maritime and desertic typicality. We proved that accurate UHII predictions can be       achieved by learning from long-term air temperature records, provided that an       appropriate predicting architecture is utilized.","What is the main challenge in analyzing the urban heat island in coastal cities with strong dependencies on synoptic weather conditions, and how does the advanced artificial intelligence technique based on long short-term memory (LSTM) model help in predicting the highly fluctuating heat island intensity (UHII) in Sydney, Australia?","
Answer: The main challenge in analyzing the urban heat island in coastal cities with strong dependencies on synoptic weather conditions is the high temporal variability and spatial heterogeneity. The strong dependency on the synoptic weather conditions hinders the identification of recurrent patterns, leading conventional predicting algorithms to fail. However, the advanced artificial intelligence technique based on long short-term memory (LSTM) model helps in predicting the highly fluctuating heat island intensity (UHII) in Sydney, Australia, by learning from long-term air temperature records and utilizing an appropriate predicting architecture. The model takes into account hourly measurements of temperature, collected for a period of 18 years (1999-2017) from 8 different sites in a 50 km radius from the coastline, as well as other inputs such as date, time, and previously computed UHII, feedbacked to the model with an optimized time step of six hours. Additionally, a second set of models integrated wind speed at the reference station to account for the sea breeze effect. The R(2) ranged between 0.770 and 0.932 for the training dataset and between 0.841 and 0.924 for the testing dataset, with the best performance attained right in correspondence of the city hot spots. Unexpectedly, very little benefit (0.06-0.43%) was achieved by including the sea breeze among the input variables. Overall, this study is insightful of a rather rare climatological case at the watershed between maritime and desertic typicality."
"BACKGROUND: Early detection of depression is very important for the treatment of      patients. In view of the current inefficient screening methods for depression,       the research of depression identification technology is a complex problem with       application value. METHODS: Our research propose a new experimental method for       depression detection based on audio and text. 160 Chinese subjects are       investigated in this study. It is worth noting that we propose a text reading       experiment to make subjects emotions change rapidly. It will be called Segmental       Emotional Speech Experiment (SESE) below. We extract 384-dimensional Low-level       audio features to find the differences of different emotional change in SESE. At       the same time, our research propose a multi-modal fusion method based on       DeepSpectrum features and word vector features to detect depression by using deep       learning. RESULTS: Our experiment proved that SESE can improve the recognition       accuracy of depression and found differences in Low-level audio features. Case       group and Control group, gender and age are grouped for verification. It is also       satisfactory that the multi-modal fusion model achieves accuracy of 0.912 and F1       score of 0.906. CONCLUSIONS: Our contribution is twofold. First, we propose and       verify SESE, which can provide a new experimental idea for the follow-up       researchers. Secondly, a new efficient multi-modal depression recognition model       is proposed.","What is the proposed method for depression detection based on audio and text, and what is the significance of the Segmental Emotional Speech Experiment (SESE) in this method?","The proposed method for depression detection based on audio and text involves the use of a Segmental Emotional Speech Experiment (SESE) to make subjects' emotions change rapidly. The SESE is a text reading experiment that is used to extract 384-dimensional Low-level audio features to find the differences in emotional changes. Additionally, a multi-modal fusion method based on DeepSpectrum features and word vector features is used to detect depression using deep learning. The significance of the SESE in this method is that it allows for the rapid change of emotions in subjects, which helps to improve the recognition accuracy of depression."
"Convolution Neural Networks (CNNs) are gaining ground in deep learning and      Artificial Intelligence (AI) domains, and they can benefit from rapid prototyping       in order to produce efficient and low-power hardware designs. The inference       process of a Deep Neural Network (DNN) is considered a computationally intensive       process that requires hardware accelerators to operate in real-world scenarios       due to the low latency requirements of real-time applications. As a result,       High-Level Synthesis (HLS) tools are gaining popularity since they provide       attractive ways to reduce design time complexity directly in register transfer       level (RTL). In this paper, we implement a MobileNetV2 model using a       state-of-the-art HLS tool in order to conduct a design space exploration and to       provide insights on complex hardware designs which are tailored for DNN       inference. Our goal is to combine design methodologies with sparsification       techniques to produce hardware accelerators that achieve comparable error metrics       within the same order of magnitude with the corresponding state-of-the-art       systems while also significantly reducing the inference latency and resource       utilization. Toward this end, we apply sparse matrix techniques on a MobileNetV2       model for efficient data representation, and we evaluate our designs in two       different weight pruning approaches. Experimental results are evaluated with       respect to the CIFAR-10 data set using several different design methodologies in       order to fully explore their effects on the performance of the model under       examination.",What is the goal of the paper and how does it achieve it?,"Answer: The goal of the paper is to combine design methodologies with sparsification techniques to produce hardware accelerators that achieve comparable error metrics within the same order of magnitude with the corresponding state-of-the-art systems while also significantly reducing the inference latency and resource utilization. The paper achieves this goal by applying sparse matrix techniques on a MobileNetV2 model for efficient data representation, and evaluating the designs in two different weight pruning approaches. The experimental results are evaluated with respect to the CIFAR-10 data set using several different design methodologies to fully explore their effects on the performance of the model under examination."
OBJECTIVE: To enhance the positive predictive value (PPV) of chest digital      tomosynthesis (DTS) in the lung cancer detection with the analysis of radiomics       features. METHOD: The investigation was carried out within the SOS clinical trial       (NCT03645018) for lung cancer screening with DTS. Lung nodules were identified by       visual analysis and then classified using the diameter and the radiological       aspect of the nodule following lung-RADS. Haralick texture features were       extracted from the segmented nodules. Both semantic variables and radiomics       features were used to build a predictive model using logistic regression on a       subset of variables selected with backward feature selection and using two       machine learning: a Random Forest and a neural network with the whole subset of       variables. The methods were applied to a train set and validated on a test set       where diagnostic accuracy metrics were calculated. RESULTS: Binary visual       analysis had a good sensitivity (0.95) but a low PPV (0.14). Lung-RADS       classification increased the PPV (0.19) but with an unacceptable low sensitivity       (0.65). Logistic regression showed a mildly increased PPV (0.29) but a lower       sensitivity (0.20). Random Forest demonstrated a moderate PPV (0.40) but with a       low sensitivity (0.30). Neural network demonstrated to be the best predictor with       a high PPV (0.95) and a high sensitivity (0.90). CONCLUSIONS: The neural network       demonstrated the best PPV. The use of visual analysis along with neural network       could help radiologists to reduce the number of false positive in DTS. KEY       POINTS: • We investigated several approaches to enhance the positive predictive       value of chest digital tomosynthesis in the lung cancer detection. • Neural       network demonstrated to be the best predictor with a nearly perfect PPV. • Neural       network could help radiologists to reduce the number of false positive in DTS.,"What is the best predictive model for enhancing the positive predictive value of chest digital tomosynthesis in lung cancer detection, and how does it compare to other methods?","Answer: The best predictive model for enhancing the positive predictive value of chest digital tomosynthesis in lung cancer detection is a neural network, which demonstrated a nearly perfect positive predictive value (PPV) of 0.95 and a high sensitivity of 0.90. This model outperforms other methods, including binary visual analysis, lung-RADS classification, logistic regression, Random Forest, and neural network. The use of visual analysis along with neural network could help radiologists to reduce the number of false positive in DTS."
"ABSTRACTChildren HIV-exposed, uninfected (CHEU) are at risk for compromised      developmental outcomes. Attention is important for behavioural, cognitive and       academic skills, yet has not been thoroughly investigated compared to children       HIV-unexposed uninfected (CHUU). Fifty-five CHEU and 51 CHUU children were       recruited at 5.5 years of age. Measures of inattention (IA),       hyperactivity/impulsivity (HI) and total scores were collected using the       parent-reported ADHD-Rating-Scale-IV. Measures of intelligence, visuomotor       skills, academics and adaptive functioning were obtained. Analyses of       between-group differences were performed as were correlational and multiple       regression models, accounting for maternal education, employment and delivery       type. Few children met clinical cut-offs for probable ADHD (3.6% CHEU, 2.0%       CHUU), and no group differences in measures of IA, HI and combined scores were       found. CHEU scored significantly lower than CHUU on intelligence, visuomotor       function, academic skills and aspects of adaptive behaviour, though within age       expectations. Lower Full-Scale IQ and Processing Speed were associated with       higher IA in CHEU and lower adaptive functioning with higher IA in CHUU. Across       both groups, children of unemployed mothers had more HI symptoms. CHEU were not       at increased risk for attention difficulties at 5.5 years of age. Maternal       employment status highlights the contribution of sociodemographic factors in       shaping behaviour and neurodevelopment.","What is the relationship between attention difficulties and maternal employment status in children HIV-exposed, uninfected (CHEU) and children HIV-unexposed uninfected (CHUU)?","Answer: The study found that children of unemployed mothers had more hyperactivity/impulsivity symptoms in both CHEU and CHUU groups. However, there was no significant relationship between maternal employment status and attention difficulties in either group."
"OBJECTIVE: Food advertising is an important determinant of unhealthy eating.      However, analysing a large number of advertisements (ads) to distinguish between       food and non-food content is a challenging task. This study aims to develop a       machine learning-based method to automatically identify and classify food and       non-food ad videos. DESIGN: Methodological study to develop an algorithm model       that prioritises both accuracy and efficiency in monitoring and classifying       advertising videos. SETTING: From a collection of Brazilian television (TV) ads       data, we created a database and split it into three sub-databases (i.e. training,       validation and test) by extracting frames from ads. Subsequently, the training       database was classified using the EfficientNet neural network. The best models       and data-balancing strategies were investigated using the validation database.       Finally, the test database was used to apply the best model and strategy, and       results were verified with field experts. PARTICIPANTS: The study used 2124       recorded Brazilian TV programming hours from 2018 to 2020. It included 703 food       ads and over 20 000 non-food ads, following the protocol developed by the       INFORMAS network for monitoring food marketing on TV. RESULTS: The results showed       that the EfficientNet neural network associated with the balanced batches       strategy achieved an overall accuracy of 90·5 % on the test database, which       represents a reduction of 99·9 % of the time spent on identifying and classifying       ads. CONCLUSIONS: The method studied represents a promising approach for       differentiating food and non-food-related video within monitoring food marketing,       which has significant practical implications for researchers, public health       policymakers, and regulatory bodies.",What is the objective of the study and how does it aim to achieve it?,The objective of the study is to develop a machine learning-based method to automatically identify and classify food and non-food ad videos. The study aims to achieve this objective by prioritizing both accuracy and efficiency in monitoring and classifying advertising videos.
"Human-computer information retrieval (HCIR) is the study of information retrieval      techniques that integrate human intelligence and algorithmic search to help       people explore, understand, and use information. Since 2007, we have held an       annual gathering of researchers and practitioners to advance the state of the art       in this field. This meeting report summarizes the history of the HCIR symposium       and emphasizes its relevance to the data science community.",What is the purpose of the annual gathering of researchers and practitioners in human-computer information retrieval (HCIR)?,Answer: The purpose of the annual gathering of researchers and practitioners in human-computer information retrieval (HCIR) is to advance the state of the art in this field.
"Advancements in high-throughput microscopy imaging have transformed cell      analytics, enabling functionally relevant, rapid, and in-depth bioanalytics with       Artificial Intelligence (AI) as a powerful driving force in cell therapy (CT)       manufacturing. High-content microscopy screening often suffers from systematic       noise, such as uneven illumination or vignetting artifacts, which can result in       false-negative findings in AI models. Traditionally, AI models have been expected       to learn to deal with these artifacts, but success in an inductive framework       depends on sufficient training examples. To address this challenge, we propose a       two-fold approach: (1) reducing noise through an image decomposition and       restoration technique called the Periodic Plus Smooth Wavelet transform (PPSW)       and (2) developing an interpretable machine learning (ML) platform using       tree-based Shapley Additive exPlanations (SHAP) to enhance end-user       understanding. By correcting artifacts during pre-processing, we lower the       inductive learning load on the AI and improve end-user acceptance through a more       interpretable heuristic approach to problem solving. Using a dataset of human       Mesenchymal Stem Cells (MSCs) cultured under diverse density and media       environment conditions, we demonstrate supervised clustering with mean SHAP       values, derived from the 'DFT Modulus' applied to the decomposition of       bright-field images, in the trained tree-based ML model. Our innovative ML       framework offers end-to-end interpretability, leading to improved precision in       cell characterization during CT manufacturing.",How can the use of high-throughput microscopy imaging and Artificial Intelligence (AI) improve cell therapy (CT) manufacturing by reducing noise and enhancing end-user understanding?,"Answer: The use of high-throughput microscopy imaging and Artificial Intelligence (AI) can improve cell therapy (CT) manufacturing by reducing noise and enhancing end-user understanding through a two-fold approach. Firstly, an image decomposition and restoration technique called the Periodic Plus Smooth Wavelet transform (PPSW) can be used to reduce noise in high-content microscopy screening. Secondly, an interpretable machine learning (ML) platform using tree-based Shapley Additive exPlanations (SHAP) can be developed to enhance end-user understanding. By correcting artifacts during pre-processing, the inductive learning load on the AI is lowered, and end-user acceptance is improved through a more interpretable heuristic approach to problem solving. The use of this innovative ML framework offers end-to-end interpretability, leading to improved precision in cell characterization during CT manufacturing."
"BACKGROUND: Capsule endoscopy (CE) is the first-choice exploration in case of      overt small bowel bleeding (SBB). An early CE is known to increase diagnostic       yield, but long reading times may delay therapeutics. The study evaluates the       diagnostic performance of the artificial intelligence tool TOP100 in patients       with overt SBB undergoing early CE with Pillcam SB3. METHODS: Patients who       underwent early CE (up to 14 days from the bleeding episode) for suspected overt       SBB were included. One experienced endoscopist prospectively performed standard       reading (SR) and a second blind experienced endoscopist performed a TOP100-based       reading (TR). The primary endpoint was TR diagnostic accuracy for lesions with       high bleeding potential (P2). RESULTS: A total of 111 patients were analyzed. The       most common clinical presentation was melena (64%). CE showed angiodysplasias in       40.5% of patients (45/111). In per-patient analysis, TR showed a sensitivity of       90.48% (95% CI 82.09-95.80), specificity of 100% (95% CI 87.23-100) with a PPV of       100% (95% CI 94.01-100), NPV of 77.14% (95% CI 63.58-86.71) and diagnostic       accuracy of 92.79 (86.29-96.84). At multivariate analysis, adequate intestinal       cleansing was the only independent predictor of concordance between TR and SR (OR       2.909, p = 0.019). The median reading time for SR and TR was 23 min (18.0-26.8)       and 1.9 min (range 1.7-2.1), respectively (p < 0.001). CONCLUSIONS: TOP100       provides a fast-reading mode for early CE in case of overt small bowel bleeding.       It identifies most patients with active bleeding and angiodysplasias, aiding in       the prioritization of therapeutic procedures. However, its accuracy in detecting       ulcers, varices and P1 lesions seems insufficient.",What is the diagnostic accuracy of the artificial intelligence tool TOP100 in detecting lesions with high bleeding potential (P2) in patients with overt small bowel bleeding undergoing early capsule endoscopy (CE) with Pillcam SB3?,Answer: The diagnostic accuracy of the artificial intelligence tool TOP100 in detecting lesions with high bleeding potential (P2) in patients with overt small bowel bleeding undergoing early capsule endoscopy (CE) with Pillcam SB3 is 92.79% (86.29-96.84).
"Although there have been reports of the successful diagnosis of skin disorders      using deep learning, unrealistically large clinical image datasets are required       for artificial intelligence (AI) training. We created datasets of standardized       nail images using a region-based convolutional neural network (R-CNN) trained to       distinguish the nail from the background. We used R-CNN to generate training       datasets of 49,567 images, which we then used to fine-tune the ResNet-152 and       VGG-19 models. The validation datasets comprised 100 and 194 images from Inje       University (B1 and B2 datasets, respectively), 125 images from Hallym University       (C dataset), and 939 images from Seoul National University (D dataset). The AI       (ensemble model; ResNet-152 + VGG-19 + feedforward neural networks) results       showed test sensitivity/specificity/ area under the curve values of (96.0 / 94.7       / 0.98), (82.7 / 96.7 / 0.95), (92.3 / 79.3 / 0.93), (87.7 / 69.3 / 0.82) for the       B1, B2, C, and D datasets. With a combination of the B1 and C datasets, the AI       Youden index was significantly (p = 0.01) higher than that of 42 dermatologists       doing the same assessment manually. For B1+C and B2+ D dataset combinations,       almost none of the dermatologists performed as well as the AI. By training with a       dataset comprising 49,567 images, we achieved a diagnostic accuracy for       onychomycosis using deep learning that was superior to that of most of the       dermatologists who participated in this study.",What is the significance of using a region-based convolutional neural network (R-CNN) to generate training datasets for the diagnosis of skin disorders using deep learning?,"
Answer: The significance of using a region-based convolutional neural network (R-CNN) to generate training datasets for the diagnosis of skin disorders using deep learning is that it allows for the accurate identification and differentiation of the nail from the background in clinical images. This is crucial for the successful diagnosis of skin disorders, as the appearance of the nail can provide important information about the underlying condition. By using R-CNN to generate training datasets, the AI model can be trained to recognize and classify the nail accurately, which can lead to improved diagnostic accuracy and better patient outcomes."
"Stress is an increasingly prevalent mental health condition across the world. In      Europe, for example, stress is considered one of the most common health problems,       and over USD 300 billion are spent on stress treatments annually. Therefore,       monitoring, identification and prevention of stress are of the utmost importance.       While most stress monitoring is carried out through self-reporting, there are now       several studies on stress detection from physiological signals using Artificial       Intelligence algorithms. However, the generalizability of these models is only       rarely discussed. The main goal of this work is to provide a monitoring       proof-of-concept tool exploring the generalization capabilities of Heart Rate       Variability-based machine learning models. To this end, two Machine Learning       models are used, Logistic Regression and Random Forest to analyze and classify       stress in two datasets differing in terms of protocol, stressors and recording       devices. First, the models are evaluated using leave-one-subject-out       cross-validation with train and test samples from the same dataset. Next, a       cross-dataset validation of the models is performed, that is,       leave-one-subject-out models trained on a Multi-modal Dataset for Real-time,       Continuous Stress Detection from Physiological Signals dataset and validated       using the University of Waterloo stress dataset. While both logistic regression       and random forest models achieve good classification results in the independent       dataset analysis, the random forest model demonstrates better generalization       capabilities with a stable F1 score of 61%. This indicates that the random forest       can be used to generalize HRV-based stress detection models, which can lead to       better analyses in the mental health and medical research field through training       and integrating different models.","What is the main goal of the proposed work and what are the two machine learning models used to analyze and classify stress in two datasets differing in terms of protocol, stressors and recording devices?","The main goal of the proposed work is to provide a monitoring proof-of-concept tool exploring the generalization capabilities of Heart Rate Variability-based machine learning models. To this end, two Machine Learning models, Logistic Regression and Random Forest, are used to analyze and classify stress in two datasets differing in terms of protocol, stressors and recording devices."
"This study aimed to validate the accuracy and prediction performance of machine      learning (ML), deep learning (DL), and logistic regression methods in the       treatment of medial meniscus posterior root tears (MMPRT). From July 2003 to May       2018, 640 patients diagnosed with MMPRT were included. First, the affecting       factors for the surgery were evaluated using statistical analysis. Second, AI       technology was introduced using X-ray and MRI. Finally, the accuracy and       prediction performance were compared between ML&DL and logistic regression       methods. Affecting factors of the logistic regression method corresponded well       with the feature importance of the six top-ranked factors in the ML&DL method.       There was no significant difference when comparing the accuracy, F1-score, and       error rate between ML&DL and logistic regression methods (accuracy = 0.89 and       0.91, F1 score = 0.89 and 0.90, error rate = 0.11 and 0.09; p = 0.114, 0.422, and       0.119, respectively). The area under the curve (AUC) values showed excellent test       quality for both ML&DL and logistic regression methods (AUC = 0.97 and 0.94,       respectively) in the evaluation of prediction performance (p = 0.289). The       affecting factors of the logistic regression method and the influence of the       ML&DL method were not significantly different. The accuracy and performance of       the ML&DL method in predicting the fate of MMPRT were comparable to those of the       logistic regression method. Therefore, this ML&DL algorithm could potentially       predict the outcome of the MMRPT in various fields and situations. Furthermore,       our method could be efficiently implemented in current clinical practice.","What is the accuracy and prediction performance of machine learning (ML), deep learning (DL), and logistic regression methods in the treatment of medial meniscus posterior root tears (MMPRT)?","Answer: The accuracy and prediction performance of machine learning (ML), deep learning (DL), and logistic regression methods in the treatment of medial meniscus posterior root tears (MMPRT) were compared in this study. From July 2003 to May 2018, 640 patients diagnosed with MMPRT were included. First, the affecting factors for the surgery were evaluated using statistical analysis. Second, AI technology was introduced using X-ray and MRI. Finally, the accuracy and prediction performance were compared between ML&DL and logistic regression methods. The accuracy and prediction performance of both ML&DL and logistic regression methods were excellent, with AUC values of 0.97 and 0.94, respectively. There was no significant difference when comparing the accuracy, F1-score, and error rate between ML&DL and logistic regression methods. The affecting factors of the logistic regression method corresponded well with the feature importance of the six top-ranked factors in the ML&DL method. Therefore, this ML&DL algorithm could potentially predict the outcome of the MMPRT in various fields and situations."
"PURPOSE: To develop a deep learning (DL) system that can detect referable      diabetic retinopathy (RDR) and vision-threatening diabetic retinopathy (VTDR)       from images obtained on ultra-widefield scanning laser ophthalmoscope (UWF-SLO).       DESIGN: Observational, cross-sectional study. PARTICIPANTS: A total of 9392       UWF-SLO images of 1903 eyes from 1022 subjects with diabetes from Hong Kong, the       United Kingdom, India, and Argentina. METHODS: All images were labeled according       to the presence or absence of RDR and the presence or absence of VTDR. Labeling       was performed by retina specialists from fundus examination, according to the       International Clinical Diabetic Retinopathy Disease Severity Scale. Three       convolutional neural networks (ResNet50) were trained with a transfer-learning       procedure for assessing gradability and identifying VTDR and RDR. External       validation was performed on 4 datasets spanning different geographical regions.       MAIN OUTCOME MEASURES: Area under the receiver operating characteristic curve       (AUROC); area under the precision-recall curve (AUPRC); sensitivity, specificity,       and accuracy of the DL system in gradability assessment; and detection of RDR and       VTDR. RESULTS: For gradability assessment, the system achieved an AUROC of 0.923       (95% confidence interval [CI], 0.892-0.947), sensitivity of 86.5% (95% CI,       77.6-92.8), and specificity of 82.1% (95% CI, 77.3-86.2) for the primary       validation dataset, and >0.82 AUROCs, >79.6% sensitivity, and >70.4% specificity       for the geographical external validation datasets. For detecting RDR and VTDR,       the AUROCs were 0.981 (95% CI, 0.977-0.984) and 0.966 (95% CI, 0.961-0.971), with       sensitivities of 94.9% (95% CI, 92.3-97.9) and 87.2% (95% CI, 81.5-91.6),       specificities of 95.1% (95% CI, 90.6-97.9) and 95.8% (95% CI, 93.3-97.6), and       positive predictive values (PPVs) of 98.0% (95% CI, 96.1-99.0) and 91.1% (95% CI,       86.3-94.3) for the primary validation dataset, respectively. The AUROCs and       accuracies for detecting both RDR and VTDR were >0.9% and >80%, respectively, for       the geographical external validation datasets. The AUPRCs were >0.9, and       sensitivities, specificities, and PPVs were >80% for the geographical external       validation datasets for RDR and VTDR detection. CONCLUSIONS: The excellent       performance achieved with this DL system for image quality assessment and       detection of RDR and VTDR in UWF-SLO images highlights its potential as an       efficient and effective diabetic retinopathy screening tool.","What is the purpose of the study and what were the main outcomes measured in the observational, cross-sectional study that aimed to develop a deep learning system for detecting referable diabetic retin","Answer: The purpose of the study was to develop a deep learning system that can detect referable diabetic retinopathy (RDR) and vision-threatening diabetic retinopathy (VTDR) from images obtained on ultra-widefield scanning laser ophthalmoscope (UWF-SLO). The main outcomes measured in the observational, cross-sectional study were the area under the receiver operating characteristic curve (AUROC); area under the precision-recall curve (AUPRC); sensitivity, specificity, and accuracy of the deep learning system in gradability assessment; and detection of RDR and VTDR."
"The biotinidase (BTD) enzyme is essential for recycling biotin, a water-soluble      B-complex vitamin that is the coenzyme of four carboxylases involved in fatty       acid synthesis, amino acid catabolism and gluconeogenesis. If untreated, total or       partial BTD deficiencies lead to an autosomal recessive inherited organic       aciduria whose clinical features, mainly presenting in the first years of life,       include, seizures, skin rash, and alopecia. Based on residual BTD enzyme activity       it is possible to identify partial or total biotinidase deficiency. The incidence       of profound and partial biotinidase deficiency worldwide is estimated to be about       1 in 60.000. We report twelve years of experience in the newborn screening of       biotinidase deficiency on 466.182 neonates. When a positive screening result       occurred, a clinical evaluation was made of the patient and genetic counselling       was offered to the family. Molecular analysis the BTD gene was carried out in all       recalled neonates. Newborn screening lead to the identification of 75 BTD       deficiencies with an incidence of about 1:6.300 births, ten times higher than the       reported worldwide incidence. BTD deficiency was confirmed at a genomic level in       all patients, demonstrating a high frequency of the p.(Asp444His) amino acid       substitution and the complex allele p.(Ala171Thr)/p.(Asp444His) in the analyzed       Italian newborns. Four new mutations (two small deletions, one stop mutation and       one missense mutation) and a new combined allelic alteration were identified. Our       data suggests that there is a high incidence of the biotinidase defect in the       Italian population, most likely due to the high frequency of certain mutations.",What is the incidence of biotinidase deficiency in the Italian population and what are the most common mutations associated with this condition?,"
Answer: The incidence of biotinidase deficiency in the Italian population is estimated to be about 1:6.300 births, ten times higher than the reported worldwide incidence. The most common mutations associated with this condition in the Italian population include the p.(Asp444His) amino acid substitution and the complex allele p.(Ala171Thr)/p.(Asp444His). Additionally, two small deletions, one stop mutation, and one missense mutation were identified in the Italian newborns."
"OBJECTIVE: The neurocognitive course of patients who have undergone cerebral      revascularization has been the subject of many studies, and the reported effects       of carotid artery stenting (CAS) on cognitive function have varied from study to       study. The authors hypothesized that cognitive amelioration after CAS is       associated with alteration of the default mode network (DMN) connectivity, and       they investigated the correlation between functional connectivity (FC) of the DMN       and post-CAS changes in cognitive function in order to find a clinical marker       that can be used to predict the effect of cerebral revascularization on patients'       cognitive function in this preliminary exploratory study. METHODS: The authors       examined post-CAS changes in cognitive function in relation to FC in patients       treated for unilateral carotid artery stenosis. Resting-state functional MRI       (rs-fMRI) was performed with a 3-T scanner before and 6 months after CAS in 8       patients. Neuropsychological tests (Wechsler Adult Intelligence Scale III and       Wechsler Memory Scale-Revised) were administered to each patient before and 6       months after CAS. The DMN was mapped for each patient through independent       component analysis of the rs-fMR images, and the correlation between FC of the       DMN and post-CAS change in cognitive function was analyzed on a voxel level.       Multivariable regression analysis was performed to identify preoperative factors       associated with a post-CAS change in cognitive function. RESULTS: Post-CAS       cognitive function varied between patients and between categories of       neuropsychological tests. Although there was no significant overall improvement       in Working Memory scores after CAS, post-CAS Working Memory scores changed in       negative correlation with changes in FC between the DMN and the       precentral/superior frontal gyrus and between the DMN and the middle frontal       gyrus. In addition, the preoperative FC between those areas correlated positively       with the post-CAS improvement in working memory. CONCLUSIONS: FC between the DMN       and working memory-related areas is closely associated with improvement in       working memory after CAS. Preoperative analysis of FC of the DMN may be useful       for predicting postoperative improvement in the working memory of patients being       treated for unilateral stenosis of the extracranial internal carotid       artery.Clinical trial registration no.: UMIN000020045       (www.umin.ac.jp/ctr/index.htm).",What is the relationship between functional connectivity (FC) of the default mode network (DMN) and post-carotid artery stenting (CAS) changes in cognitive function in patients with unilateral carotid artery stenosis?,Answer: The study found that functional connectivity (FC) between the default mode network (DMN) and working memory-related areas is closely associated with improvement in working memory after carotid artery stenting (CAS). Preoperative analysis of FC of the DMN may be useful for predicting postoperative improvement in the working memory of patients being treated for unilateral stenosis of the extracranial internal carotid artery.
"Brain iron load is one of the main neuropathologic hallmarks of Parkinson's      disease (PD). Previous studies indicated that iron in the substantia nigra (SN)       is related to disease duration and motor impairment. We explore, through a       cross-sectional study, the association between brain iron distribution, evaluated       by T2*-weighted magnetic resonance imaging (T2*), and clinical features in a       cohort of patients with PD. Thirty-two patients with PD, compared with 10 control       subjects, were evaluated for motor and cognitive features (attention and working       memory, executive functions, language, memory, and visuospatial function). They       underwent a magnetic resonance imaging protocol including T2* analysis of       specific brain regions of interest to measure iron load compared with healthy       control subjects. We found that iron content of the SN correlated positively with       both disease duration and Unified Parkinson's Disease Rating Scale III off score.       Montreal Cognitive Assessment, Spatial Span, and Graded Naming Test scores were       inversely associated with iron load of the SN, whereas Wechsler Adult       Intelligence Scale-IV Similarities score showed an inverse relationship with iron       content in all the regions of interest examined. Our findings suggest a       relationship between topographic brain iron distribution and cognitive domain       impairment.","What is the relationship between brain iron distribution, evaluated by T2*-weighted magnetic resonance imaging (T2*), and clinical features in a cohort of patients with Parkinson's disease (PD)?","Answer: The study found that iron content of the substantia nigra (SN) correlated positively with both disease duration and Unified Parkinson's Disease Rating Scale III off score. Montreal Cognitive Assessment, Spatial Span, and Graded Naming Test scores were inversely associated with iron load of the SN, whereas Wechsler Adult Intelligence Scale-IV Similarities score showed an inverse relationship with iron content in all the regions of interest examined. These findings suggest a relationship between topographic brain iron distribution and cognitive domain impairment in patients with PD."
[This corrects the article DOI: 10.3389/frobt.2023.1140901.].,"What is the correct DOI for the article titled ""The Role of Emotion in Decision Making""?","
Answer: The correct DOI for the article titled ""The Role of Emotion in Decision Making"" is 10.3389/frobt.2023.1140901."
"Climate change increasingly affects every aspect of human life. Recent studies      report a close correlation with human health and it is estimated that global       death rates will increase by 73 per 100,000 by 2100 due to changes in       temperature. In this context, the present work aims to study the correlation       between climate change and human health, on a global scale, using artificial       intelligence techniques. Starting from previous studies on a smaller scale, that       represent climate change and which at the same time can be linked to human       health, four factors were chosen. Four causes of mortality, strongly correlated       with the environment and climatic variability, were subsequently selected.       Various analyses were carried out, using neural networks and machine learning to       find a correlation between mortality due to certain diseases and the leading       causes of climate change. Our findings suggest that anthropogenic climate change       is strongly correlated with human health; some diseases are mainly related to       risk factors while others require a more significant number of variables to       derive a correlation. In addition, a forecast of victims related to climate       change was formulated. The predicted scenario confirms that a prevalently       increasing trend in climate change factors corresponds to an increase in victims.","What is the correlation between climate change and human health, and how does it affect global death rates?","Answer: The correlation between climate change and human health is strong, with global death rates estimated to increase by 73 per 100,000 by 2100 due to changes in temperature. The present work aims to study this correlation on a global scale using artificial intelligence techniques. Four factors were chosen, four causes of mortality strongly correlated with the environment and climatic variability, and various analyses were carried out using neural networks and machine learning to find a correlation between mortality due to certain diseases and the leading causes of climate change. The findings suggest that anthropogenic climate change is strongly correlated with human health, with some diseases being mainly related to risk factors while others require a more significant number of variables to derive a correlation. A forecast of victims related to climate change was formulated, and the predicted scenario confirms that a prevalently increasing trend in climate change factors corresponds to an increase in victims."
"Individuals with a growth mindset believe that all failures are opportunities and      that their baseline intelligence and talent can be used for continuous       improvement. Individuals with a fixed mindset believe that baseline intelligence       and talent cannot be developed. A growth mindset is associated with greater       academic success and greater resilience in the face of failure or stress.       Second-year veterinary students completed three surveys to determine mindset,       perceived levels of stress, and life change score. Of 57 students, 70% had a       strong growth mindset or a growth mindset with some fixed ideas. No students had       a strong fixed mindset. Mindset was not correlated with GPA or perceived stress       level. Colleges of veterinary medicine can assist students by providing resources       and training for stress management, including training in how to further develop       a growth mindset.",What is the correlation between growth mindset and academic success among second-year veterinary students?,Answer: The correlation between growth mindset and academic success among second-year veterinary students is not significant.
"OBJECTIVES: To evaluate the outcome of ketogenic diets (KDs) in patients with      glucose transport type 1 deficiency syndrome (GLUT1DS) in Japan. METHODS: A       nationwide survey for GLUT1DS was conducted by sending questionnaires to       board-certified pediatric neurologists nationwide to obtain clinical and       laboratory data. RESULTS: Among 39 patients whose diagnosis was confirmed       molecularly or by the 3-O-methylglucose uptake assay, 31 were treated with KDs       for longer than 1month. Seventeen patients (55%) were on the modified Atkins       diet, 11 (35%) were on the classic KD, and 3 were on the medium-chain       triglyceride (MCT) diet. The median values and ranges of serum β-hydroxybutyrate       levels in patients on the modified Atkins diet, classic KD and MCT diet were       2.5mM (0.75-4.1), 1.7mM (0.23-3.5) and 2.6mM (1.5-3.0), respectively. The KDs       were effective on seizures (80%), aggravation after fasting (80%) and ataxia       (79%). Thus, ataxia was as responsive as seizures. Two patients on the classic KD       with a ketogenic ratio as low as 1:1 showed improvement in neurological symptoms.       The development or intelligence quotient measured using the same psychological       scales before and after the KDs in 9 patients did not show a significant       improvement; the median quotients before and after the diets were 40 (12-91) and       46 (12-67). CONCLUSION: The KDs were most effective on seizures, transient       aggravation after fasting and ataxia. The efficacy on intellectual development       was equivocal. The modified Atkins diet was more commonly used for GLUT1DS in       this study, and its ketogenicity was equivalent to the classic KD.","What is the effectiveness of ketogenic diets (KDs) in patients with glucose transport type 1 deficiency syndrome (GLUT1DS) in Japan, and what is the most commonly used KD in this study?","Answer: The effectiveness of ketogenic diets (KDs) in patients with glucose transport type 1 deficiency syndrome (GLUT1DS) in Japan was evaluated through a nationwide survey of board-certified pediatric neurologists. The results showed that KDs were most effective in reducing seizures, transient aggravation after fasting, and ataxia. The modified Atkins diet was the most commonly used KD in this study, and its ketogenicity was equivalent to the classic KD."
"The theory of modern organizations considers emotional intelligence to be the      metric for tools that enable organizations to create a competitive vision. It       also helps corporate leaders enthusiastically adhere to the vision and energize       organizational stakeholders to accomplish the vision. In this study, the       one-dimensional convolutional neural network classification model is initially       employed to interpret and evaluate shifts in emotion over a period by       categorizing emotional states that occur at particular moments during mutual       interaction using physiological signals. The self-organizing map technique is       implemented to cluster overall organizational emotions to represent       organizational competitiveness. The analysis of variance test results indicates       no significant difference in age and body mass index for participants exhibiting       different emotions. However, a significant mean difference was observed for the       blood volume pulse, galvanic skin response, skin temperature, valence, and       arousal values, indicating the effectiveness of the chosen physiological sensors       and their measures to analyze emotions for organizational competitiveness. We       achieved 99.8% classification accuracy for emotions using the proposed technique.       The study precisely identifies the emotions and locates a connection between       emotional intelligence and organizational competitiveness (i.e., a positive       relationship with employees augments organizational competitiveness).","What is the relationship between emotional intelligence and organizational competitiveness, and how can physiological signals be used to measure emotional states and analyze emotions for organizational competitiveness?","Answer: Emotional intelligence is considered a metric for tools that enable organizations to create a competitive vision and enthusiastically adhere to the vision. Physiological signals can be used to measure emotional states and analyze emotions for organizational competitiveness. In this study, a one-dimensional convolutional neural network classification model was employed to interpret and evaluate shifts in emotion over a period by categorizing emotional states that occur at particular moments during mutual interaction using physiological signals. The self-organizing map technique was implemented to cluster overall organizational emotions to represent organizational competitiveness. The analysis of variance test results indicated no significant difference in age and body mass index for participants exhibiting different emotions. However, a significant mean difference was observed for the blood volume pulse, galvanic skin response, skin temperature, valence, and arousal values, indicating the effectiveness of the chosen physiological sensors and their measures to analyze emotions for organizational competitiveness. The study achieved 99.8% classification accuracy for emotions using the proposed technique. The results of the study precisely identify the emotions and locate a positive relationship between emotional intelligence and organizational competitiveness."
"AIM: To evaluate the incidence and severity of the impairment of selected      cognitive functions in patients after sudden cardiac arrest (SCA) in comparison       to patients after myocardial infarction without SCA and healthy subjects and to       analyze the influence of sociodemographic and clinical parameters and the       duration of cardiac arrest on the presence and severity of the described       disorders. MATERIAL AND METHODS: The study group comprised 30 cardiac arrest       survivors, the reference group comprised 31 survivors of myocardial infarction       without cardiac arrest, and the control group comprised 30 healthy subjects. The       Mini-Mental State Examination (MMSE), the Digit Span test from the Wechsler Adult       Intelligence Scale, Lauretta Bender's Visual-Motor Gestalt Test, and the Benton       Visual Retention Test (BVRT) were used to assess the presence of cognitive       impairment. An original questionnaire developed by the author was used for       overall mental state assessment. RESULTS: The Bender test demonstrated a       significant difference in the presence and severity of visual-motor skills       between the study group and the control group, while BVRT and MMSE revealed       increased incidence of cognitive impairment in the study group. The Bender and       BVRT (D/D)/SS (version D, method D, scaled score) scales indicated cognitive       impairment in 53.3% of these patients, while the BVRT (C/A)/SS test indicated       cognitive impairment in 40%. For the reference group, the values were 32.3% and       12.9%, respectively. No correlation was found between the severity of cognitive       impairment and the duration of cardiac arrest. CONCLUSIONS: Impairment of       visual-motor skills, short-term visual memory, concentration, and visual-motor       coordination occurs much more frequently and is more severe in individuals after       SCA than in healthy individuals. Impairment of memory trace storage and recall       after delay occurs more frequently in patients after SCA than in patients after       myocardial infarction without cardiac arrest and in healthy individuals. SCA       duration did not have any influence on the severity of the described disorders.","What is the difference in the presence and severity of visual-motor skills between cardiac arrest survivors and healthy individuals, and how does the duration of cardiac arrest affect cognitive impairment in these patients?","The Bender test demonstrated a significant difference in the presence and severity of visual-motor skills between the study group and the control group, while BVRT and MMSE revealed increased incidence of cognitive impairment in the study group. The Bender and BVRT (D/D)/SS (version D, method D, scaled score) scales indicated cognitive impairment in 53.3% of these patients, while the BVRT (C/A)/SS test indicated cognitive impairment in 40%. For the reference group, the values were 32.3% and 12.9%, respectively. No correlation was found between the severity of cognitive impairment and the duration of cardiac arrest."
"The integral imaging microscopy system provides a three-dimensional visualization      of a microscopic object. However, it has a low-resolution problem due to the       fundamental limitation of the F-number (the aperture stops) by using micro lens       array (MLA) and a poor illumination environment. In this paper, a generative       adversarial network (GAN)-based super-resolution algorithm is proposed to enhance       the resolution where the directional view image is directly fed as input. In a       GAN network, the generator regresses the high-resolution output from the       low-resolution input image, whereas the discriminator distinguishes between the       original and generated image. In the generator part, we use consecutive residual       blocks with the content loss to retrieve the photo-realistic original image. It       can restore the edges and enhance the resolution by ×2, ×4, and even ×8 times       without seriously hampering the image quality. The model is tested with a variety       of low-resolution microscopic sample images and successfully generates       high-resolution directional view images with better illumination. The       quantitative analysis shows that the proposed model performs better for       microscopic images than the existing algorithms.",What is the proposed solution to the low-resolution problem in integral imaging microscopy system using a generative adversarial network (GAN)-based super-resolution algorithm?,"Answer: The proposed solution to the low-resolution problem in integral imaging microscopy system using a generative adversarial network (GAN)-based super-resolution algorithm is a GAN-based super-resolution algorithm that enhances the resolution of the directional view image by ×2, ×4, and even ×8 times without seriously hampering the image quality. The generator regresses the high-resolution output from the low-resolution input image, whereas the discriminator distinguishes between the original and generated image. The model is tested with a variety of low-resolution microscopic sample images and successfully generates high-resolution directional view images with better illumination. The quantitative analysis shows that the proposed model performs better for microscopic images than the existing algorithms."
"Post-stroke depression and anxiety, collectively known as post-stroke adverse      mental outcome (PSAMO) are common sequelae of stroke. About 30% of stroke       survivors develop depression and about 20% develop anxiety. Stroke survivors with       PSAMO have poorer health outcomes with higher mortality and greater functional       disability. In this study, we aimed to develop a machine learning (ML) model to       predict the risk of PSAMO. We retrospectively studied 1780 patients with stroke       who were divided into PSAMO vs. no PSAMO groups based on results of validated       depression and anxiety questionnaires. The features collected included       demographic and sociological data, quality of life scores, stroke-related       information, medical and medication history, and comorbidities. Recursive feature       elimination was used to select features to input in parallel to eight ML       algorithms to train and test the model. Bayesian optimization was used for       hyperparameter tuning. Shapley additive explanations (SHAP), an explainable AI       (XAI) method, was applied to interpret the model. The best performing ML       algorithm was gradient-boosted tree, which attained 74.7% binary classification       accuracy. Feature importance calculated by SHAP produced a list of ranked       important features that contributed to the prediction, which were consistent with       findings of prior clinical studies. Some of these factors were modifiable, and       potentially amenable to intervention at early stages of stroke to reduce the       incidence of PSAMO.",What is the purpose of the study and what was the methodology used to develop a machine learning model to predict the risk of post-stroke adverse mental outcome (PSAMO)?,"The purpose of the study was to develop a machine learning (ML) model to predict the risk of post-stroke adverse mental outcome (PSAMO) in stroke survivors. The methodology used to develop the model included retrospectively studying 1780 patients with stroke who were divided into PSAMO vs. no PSAMO groups based on results of validated depression and anxiety questionnaires. The features collected included demographic and sociological data, quality of life scores, stroke-related information, medical and medication history, and comorbidities. Recursive feature elimination was used to select features to input in parallel to eight ML algorithms to train and test the model. Bayesian optimization was used for hyperparameter tuning. Shapley additive explanations (SHAP), an explainable AI (XAI) method, was applied to interpret the model. The best performing ML algorithm was gradient-boosted tree, which attained 74.7% binary classification accuracy. Feature importance calculated by SHAP produced a list of ranked important features that contributed to the prediction, which were consistent with findings of prior clinical studies. Some of these factors were modifiable, and potentially amenable to intervention at early stages of stroke to reduce the incidence of PSAMO."
"Pulmonary embolism (PE) remains a diagnostic challenge in 2021. As the pathology      is potentially fatal and signs and symptoms are nonspecific, further       investigations are classically required. Based on the Bayesian approach, clinical       probability became the keystone of the diagnostic strategy to rule out PE in the       case of a negative testing. Several clinical probability assessment methods are       validated: gestalt, the Wells score, or the revised Geneva score. While the       debate persists as to the best way to assess clinical probability, its assessment       allows for the good interpretation of the investigation results and therefore       directs the correct diagnostic strategy. The wide availability of computed       tomography pulmonary angiography (CTPA) resulted in a major increase in       investigations with a moderate increase in diagnosis, without any notable       improvement in patient outcomes. This leads to a new challenge for PE diagnosis       which is the limitation of the number of testing for suspected PE. We review       different strategies recently developed to achieve this goal. The last challenge       concerns the implementation in clinical practice. Two approaches are developed:       simplification of the strategies versus the use of digital support tools allowing       more sophisticated strategies. Artificial intelligence with machine-learning       algorithms will probably be a future tool to guide the physician in this complex       approach concerning acute PE suspicion.",What is the current diagnostic challenge in pulmonary embolism (PE) and how can it be addressed through the use of clinical probability assessment methods and artificial intelligence with machine-learning algorithms?,"Answer: The current diagnostic challenge in pulmonary embolism (PE) is that it is potentially fatal and its signs and symptoms are nonspecific, which requires further investigations. Clinical probability assessment methods such as gestalt, the Wells score, or the revised Geneva score can be used to assess the clinical probability of PE and interpret the investigation results. However, the wide availability of computed tomography pulmonary angiography (CTPA) has led to a new challenge of limiting the number of testing for suspected PE. To address this challenge, simplification of the strategies or the use of digital support tools allowing more sophisticated strategies can be implemented. Artificial intelligence with machine-learning algorithms may also be a future tool to guide physicians in this complex approach concerning acute PE suspicion."
"OBJECTIVE: Applications of artificial intelligence (AI) have been reported in      several cardiovascular diseases but its interest in patients with peripheral       artery disease (PAD) has been so far less reported. The aim of this review was to       summarize current knowledge on applications of AI in patients with PAD, to       discuss current limits, and highlight perspectives in the field. METHODS: We       performed a narrative review based on studies reporting applications of AI in       patients with PAD. The MEDLINE database was independently searched by two authors       using a combination of keywords to identify studies published between January       1995 and December 2021. Three main fields of AI were investigated including       natural language processing (NLP), computer vision and machine learning (ML).       RESULTS: NLP and ML brought new tools to improve the screening, the diagnosis and       classification of the severity of PAD. ML was also used to develop predictive       models to better assess the prognosis of patients and develop real-time       prediction models to support clinical decision-making. Studies related to       computer vision mainly aimed at creating automatic detection and characterization       of arterial lesions based on Doppler ultrasound examination or computed       tomography angiography. Such tools could help to improve screening programs,       enhance diagnosis, facilitate presurgical planning, and improve clinical       workflow. CONCLUSIONS: AI offers various applications to support and likely       improve the management of patients with PAD. Further research efforts are needed       to validate such applications and investigate their accuracy and safety in large       multinational cohorts before their implementation in daily clinical practice.",What are the current applications of artificial intelligence (AI) in patients with peripheral artery disease (PAD) and what are the limitations and perspectives in the field?,"Answer: The current applications of artificial intelligence (AI) in patients with peripheral artery disease (PAD) include natural language processing (NLP), computer vision, and machine learning (ML). NLP and ML have brought new tools to improve the screening, diagnosis, and classification of the severity of PAD. ML has also been used to develop predictive models to better assess the prognosis of patients and create real-time prediction models to support clinical decision-making. Computer vision has been used to create automatic detection and characterization of arterial lesions based on Doppler ultrasound examination or computed tomography angiography. These tools have the potential to improve screening programs, enhance diagnosis, facilitate presurgical planning, and improve clinical workflow. However, further research efforts are needed to validate these applications and investigate their accuracy and safety in large multinational cohorts before their implementation in daily clinical practice."
"The novel coronavirus disease (COVID-19) is a public health problem once      according to the World Health Organization up to June 24th, 2020, more than 9.1       million people were infected, and more than 470 thousand have died worldwide. In       the current scenario, the Brazil and the United States of America present a high       daily incidence of new cases and deaths. Therefore, it is important to forecast       the number of new cases in a time window of one week, once this can help the       public health system developing strategic planning to deals with the COVID-19.       The application of the forecasting artificial intelligence (AI) models has the       potential of deal with dynamical behavior of time-series like of COVID-19. In       this paper, Bayesian regression neural network, cubist regression, k-nearest       neighbors, quantile random forest, and support vector regression, are used       stand-alone, and coupled with the recent pre-processing variational mode       decomposition (VMD) employed to decompose the time series into several intrinsic       mode functions. All AI techniques are evaluated in the task of time-series       forecasting with one, three, and six-days-ahead the cumulative COVID-19 cases in       five Brazilian and American states, with a high number of cases up to April 28th,       2020. Previous cumulative COVID-19 cases and exogenous variables as daily       temperature and precipitation were employed as inputs for all forecasting models.       The models' effectiveness are evaluated based on the performance criteria. In       general, the hybridization of VMD outperformed single forecasting models       regarding the accuracy, specifically when the horizon is six-days-ahead, the       hybrid VMD-single models achieved better accuracy in 70% of the cases. Regarding       the exogenous variables, the importance ranking as predictor variables is, from       the upper to the lower, past cases, temperature, and precipitation. Therefore,       due to the efficiency of evaluated models to forecasting cumulative COVID-19       cases up to six-days-ahead, the adopted models can be recommended as a promising       models for forecasting and be used to assist in the development of public       policies to mitigate the effects of COVID-19 outbreak.","What is the effectiveness of using Bayesian regression neural network, cubist regression, k-nearest neighbors, quantile random forest, and support vector regression, stand-alone and coupled with variational mode decomposition (VMD) in forecasting the number of new COVID-19 cases in a time window of one week?","Answer: The effectiveness of using Bayesian regression neural network, cubist regression, k-nearest neighbors, quantile random forest, and support vector regression, stand-alone and coupled with variational mode decomposition (VMD) in forecasting the number of new COVID-19 cases in a time window of one week is that the hybridization of VMD outperformed single forecasting models regarding the accuracy, specifically when the horizon is six-days-ahead, the hybrid VMD-single models achieved better accuracy in 70% of the cases. Regarding the exogenous variables, the importance ranking as predictor variables is, from the upper to the lower, past cases, temperature, and precipitation. Therefore, due to the efficiency of evaluated models to forecasting cumulative COVID-19 cases up to six-days-ahead, the adopted models can be recommended as a promising models for forecasting and be used to assist in the development of public policies to mitigate the effects of COVID-19 outbreak."
"This article discusses the challenges and implications of artificial intelligence      powered chatbot (AI-Chatbots) in nursing education. Chat Generative Pre-trained       Transformer (ChatGPT) is an AI-Chatbot that can engage in detailed dialog and       pass qualification tests in various fields. It can be applied for drafting course       materials and administrative paperwork. Students can use it for personalized       self-paced learning. AI-Chatbot technology can be applied in problem-based       learning for hands-on practice experiences. There are concerns about       over-reliance on the technology, including issues with plagiarism and limiting       critical thinking skills. Educators must provide clear guidelines on appropriate       use and emphasize the importance of critical thinking and proper citation.       Educators must proactively adjust their curricula and pedagogy. AI-Chatbot       technology could transform the nursing profession by aiding and streamlining       administrative tasks, allowing nurses to focus on patient care. The use of       AI-Chatbots to socially assist patients and for therapeutic purposes in mental       health shows promise in improving well-being of patients, and potentially easing       shortage and burnout for healthcare workers. AI-Chatbots can help nursing       students and researchers to overcome technical barriers in nursing informatics,       increasing accessibility for individuals without technical background. AI-Chatbot       technology has potential in easing tasks for nurses, improving patient care, and       enhancing nursing education.","What are the challenges and implications of using AI-Chatbots in nursing education, and how can educators address these challenges to ensure that critical thinking skills are not limited and plagiarism is prevented?","Answer: The challenges and implications of using AI-Chatbots in nursing education include concerns about over-reliance on the technology, including issues with plagiarism and limiting critical thinking skills. To address these challenges, educators must provide clear guidelines on appropriate use and emphasize the importance of critical thinking and proper citation. Educators must proactively adjust their curricula and pedagogy to ensure that students are not overly reliant on the technology and that they are developing the necessary critical thinking skills. Additionally, educators must ensure that plagiarism is prevented by implementing appropriate measures, such as using plagiarism detection software and educating students on proper citation practices. Overall, the use of AI-Chatbots in nursing education has the potential to transform the profession by aiding and streamlining administrative tasks, allowing nurses to focus on patient care. However, it is important for educators to be aware of the challenges and implications and to take steps to address them to ensure that students are developing the necessary skills for success in the nursing profession."
"BACKGROUND: True findings about schizophrenia remain elusive; many findings are      not replicated and conflicting results are common. Well-conducted systematic       reviews have the ability to make robust, generalizable conclusions, with good       meta-analyses potentially providing the closest estimate of the true effect size.       In this paper, we undertake a systematic approach to synthesising the available       evidence from well-conducted systematic reviews on schizophrenia. METHOD: Reviews       were identified by searching Medline, EMBASE, CINAHL, Current Contents and       PsycINFO. The decision to include or exclude reviews, data extraction and quality       assessments were conducted in duplicate. Evidence was graded as high quality if       reviews contained large samples and robust results; and as moderate quality if       reviews contained imprecision, inconsistency, smaller samples or study designs       that may be prone to bias. RESULTS: High- and moderate-quality evidence shows       that numerous psychosocial and biomedical treatments are effective. Patients have       relatively poor cognitive functioning, and subtle, but diverse, structural brain       alterations, altered electrophysiological functioning and sleep patterns, minor       physical anomalies, neurological soft signs, and sensory alterations. There are       markers of infection, inflammation or altered immunological parameters; and there       is increased mortality from a range of causes. Risk for schizophrenia is       increased with cannabis use, pregnancy and birth complications, prenatal exposure       to Toxoplasma gondii, childhood central nervous system viral infections,       childhood adversities, urbanicity and immigration (first and second generation),       particularly in certain ethnic groups. Developmental motor delays and lower       intelligence quotient in childhood and adolescence are apparent. CONCLUSIONS: We       conclude that while our knowledge of schizophrenia is very substantial, our       understanding of it remains limited.","What are the current findings about schizophrenia, and what are the effective treatments for it?","Answer: The current findings about schizophrenia suggest that it is a complex condition with various psychosocial and biomedical treatments being effective. Patients with schizophrenia often have poor cognitive functioning, subtle structural brain alterations, altered electrophysiological functioning and sleep patterns, minor physical anomalies, neurological soft signs, and sensory alterations. There are markers of infection, inflammation, or altered immunological parameters, and increased mortality from a range of causes. Risk factors for schizophrenia include cannabis use, pregnancy and birth complications, prenatal exposure to Toxoplasma gondii, childhood central nervous system viral infections, childhood adversities, urbanicity, and immigration, particularly in certain ethnic groups. Developmental motor delays and lower intelligence quotient in childhood and adolescence are also apparent. Effective treatments for schizophrenia include antipsychotic medications, psychosocial interventions, and cognitive-behavioral therapy. However, the understanding of schizophrenia remains limited due to conflicting results and the lack of replication of some findings."
"In the face of a growing global population, plant breeding is being used as a      sustainable tool for increasing food security. A wide range of high-throughput       omics technologies have been developed and used in plant breeding to accelerate       crop improvement and develop new varieties with higher yield performance and       greater resilience to climate changes, pests, and diseases. With the use of these       new advanced technologies, large amounts of data have been generated on the       genetic architecture of plants, which can be exploited for manipulating the key       characteristics of plants that are important for crop improvement. Therefore,       plant breeders have relied on high-performance computing, bioinformatics tools,       and artificial intelligence (AI), such as machine-learning (ML) methods, to       efficiently analyze this vast amount of complex data. The use of bigdata coupled       with ML in plant breeding has the potential to revolutionize the field and       increase food security. In this review, some of the challenges of this method       along with some of the opportunities it can create will be discussed. In       particular, we provide information about the basis of bigdata, AI, ML, and their       related sub-groups. In addition, the bases and functions of some learning       algorithms that are commonly used in plant breeding, three common data       integration strategies for the better integration of different breeding datasets       using appropriate learning algorithms, and future prospects for the application       of novel algorithms in plant breeding will be discussed. The use of ML algorithms       in plant breeding will equip breeders with efficient and effective tools to       accelerate the development of new plant varieties and improve the efficiency of       the breeding process, which are important for tackling some of the challenges       facing agriculture in the era of climate change.",What is the potential of using big data and machine learning (ML) in plant breeding to increase food security?,"Answer: The potential of using big data and machine learning (ML) in plant breeding to increase food security is significant. With the use of these advanced technologies, large amounts of data have been generated on the genetic architecture of plants, which can be exploited for manipulating the key characteristics of plants that are important for crop improvement. Plant breeders have relied on high-performance computing, bioinformatics tools, and artificial intelligence (AI), such as machine-learning (ML) methods, to efficiently analyze this vast amount of complex data. The use of bigdata coupled with ML in plant breeding has the potential to revolutionize the field and increase food security. In this review, some of the challenges of this method along with some of the opportunities it can create will be discussed. In particular, we provide information about the basis of bigdata, AI, ML, and their related sub-groups. In addition, the bases and functions of some learning algorithms that are commonly used in plant breeding, three common data integration strategies for the better integration of different breeding datasets using appropriate learning algorithms, and future prospects for the application of novel algorithms in plant breeding will be discussed. The use of ML algorithms in plant breeding will equip breeders with efficient and effective tools to accelerate the development of new plant varieties and improve the efficiency of the breeding process, which are important for tackling some of the challenges facing agriculture in the era of climate change."
"OBJECTIVES: To evaluate the risk category performance of artificial      intelligence-based coronary artery calcium score (AI-CACS) software used in       non-gated chest computed tomography (CT) on three types of CT machines,       considering the manual method as the standard. METHODS: A total of 901 patients       who underwent both chest CT and electrocardiogram (ECG)-gated       non-contrast-enhanced cardiac CT with the same equipment within a 3-month period       were enrolled in the study. AI-CACS software was based on a deep learning       algorithm and was trained on multi-vendor, multi-scanner, and multi-hospital       anonymized data from the chest CT database. The AI-CACS was automatically       obtained from chest CT data by the AI-CACS software, while the manual CACS was       obtained from cardiac CT data by the manual method. The correlation of the       AI-CACS and manual CACS, concordance rate and kappa value of the risk categories       determined by the two methods were calculated. The chi-square test was used to       evaluate the differences in risk categories among the three types of CT machines       from different manufacturers. The risk category performance of the AI-CACS for       dichotomous risk categories bounded by 0, 100 and 400 was assessed. RESULTS: The       correlation of the AI-CACS with the manual CACS was ρ = 0.893 (p < 0.001). The       Bland-Altman plot (AI-CACS minus manual CACS) showed a mean difference of -27.2       and 95% limits of agreement of -290.0 to 235.6. The agreement of risk categories       for the CACS was kappa (κ) = 0.679 (p < 0.001), and the concordance rate was       80.6%. The risk categories determined by the AI-CACS software on three types of       CT machines were not significantly different (p = 0.7543). As dichotomous risk       categories bounded by 0, 100 and 400, the accuracy, kappa value, and area under       the curve of the AI-CACS were 88.6% vs. 92.9% vs. 97.9%, 0.77 vs. 0.77 vs. 0.83,       and 0.885 vs. 0.964 vs. 0.981, respectively. CONCLUSIONS: There was good       correlation and agreement between the AI-CACS and manual CACS in terms of the       risk category. It is feasible to obtain the CACS using AI software based on       non-gated chest CT data in a short time without increasing the radiation dose or       economic burden. The AI-CACS software algorithm has good clinical universality       and can be applied to CT machines from different manufacturers.",What is the correlation and agreement between the AI-CACS software and the manual CACS method in terms of risk category performance on three types of CT machines from different manufacturers?,"The correlation between the AI-CACS software and the manual CACS method in terms of risk category performance on three types of CT machines from different manufacturers was ρ = 0.893 (p < 0.001). The agreement of risk categories for the CACS was kappa (κ) = 0.679 (p < 0.001), and the concordance rate was 80.6%. The risk categories determined by the AI-CACS software on three types of CT machines were not significantly different (p = 0.7543)."
"The article uses the literature material law, expert interview method,      mathematical statistics, the author of Suzhou University, Chengdu research,       education of Guangdong Institute of Education, Guangdong teachers college       education schools such as the current sports education professional study, and       the similarities and differences in teaching outline of education of Guangdong       province basketball specialty to conduct a comprehensive study and analysis and       comparison on the current situation of education of Guangdong teachers college       basketball in the system. This paper expounds the Guangdong education normal       study and the Guangdong education status of school physical education       professional basketball specialty courses, for Guangdong teachers college       education reform of physical education professional basketball specializing in       research, and the Guangdong education for theory reference.","What is the current situation of education of Guangdong teachers college basketball in the system, and what are the similarities and differences in teaching outline of education of Guangdong province basketball specialty to conduct a comprehensive study and analysis and comparison on the current situation of education of Guangdong teachers college basketball in the system?","
Answer: The current situation of education of Guangdong teachers college basketball in the system is being studied and analyzed using literature material law, expert interview method, mathematical statistics, and the author of Suzhou University, Chengdu research, education of Guangdong Institute of Education, Guangdong teachers college education schools such as the current sports education professional study, and the similarities and differences in teaching outline of education of Guangdong province basketball specialty to conduct a comprehensive study and analysis and comparison on the current situation of education of Guangdong teachers college basketball in the system. The study expounds the Guangdong education normal study and the Guangdong education status of school physical education professional basketball specialty courses, for Guangdong teachers college education reform of physical education professional basketball specializing in research, and the Guangdong education for theory reference."
"The gray matter abnormalities revealed by magnetic resonance imaging are      inconsistent, especially in pediatric individuals with autism spectrum disorder       (ASD) (age < 18 years old), a phenomenon possibly related to the core       pathophysiology of ASD. The purpose of our meta-analysis was to identify and map       the specific gray matter abnormalities in pediatric ASD individuals thereby       exploring the potential effects of clinical and demographic characteristics of       these gray matter changes. A systematic search was conducted to identify       voxel-based morphometry studies in pediatric individuals with ASD. The       effect-size signed differential mapping method was used to quantitatively       estimate the regional gray matter abnormalities in pediatric ASD individuals.       Meta-regression was used to examine the associations among age, gender,       intelligence quotient, symptom severity and gray matter changes. Fifteen studies       including 364 pediatric individuals with ASD (male = 282, age = 10.3 ± 4.4 years)       and 377 healthy controls (male = 289, age = 10.5 ± 4.2 years) were included.       Pediatric ASD individuals showed significant gray matter increases in the right       angular gyrus, left superior and middle frontal gyrus, left precuneus, left       inferior occipital gyrus and right inferior temporal gyrus, most of which       involving the default mode network, and decreases in the left cerebellum and left       postcentral gyrus. The meta-regression analysis showed that the repetitive       behavior scores of the Autism Diagnostic Interview-Revised were positively       associated with increased gray matter volumes in the right angular gyrus.       Increased rather than decreased gray matter volume, especially involving the       angular gyrus and prefrontal cortex may be the core pathophysiology in the early       course of ASD.",What are the specific gray matter abnormalities in pediatric individuals with autism spectrum disorder (ASD) and how do they relate to clinical and demographic characteristics?,"Answer: The specific gray matter abnormalities in pediatric individuals with autism spectrum disorder (ASD) include increases in the right angular gyrus, left superior and middle frontal gyrus, left precuneus, left inferior occipital gyrus, and right inferior temporal gyrus, most of which involve the default mode network. Additionally, there are decreases in the left cerebellum and left postcentral gyrus. The repetitive behavior scores of the Autism Diagnostic Interview-Revised are positively associated with increased gray matter volumes in the right angular gyrus. These findings suggest that increased rather than decreased gray matter volume, especially involving the angular gyrus and prefrontal cortex, may be the core pathophysiology in the early course of ASD."
"OBJECTIVE: The main goal of the study was to analyse differences in the      forgetting rates of Temporal Lobe Epilepsy (TLE) patients at different intervals       (30 sec, 10 min, 1 day and 1 week) compared with those of healthy controls. A       secondary aim of this research was to provide an assessment of the relationship       between clinical epilepsy-related variables and forgetting rates in TLE patients.       METHOD: The sample was composed of 14 TLE patients and 14 healthy matched       controls. All participants underwent a full standardised neuropsychological       assessment including general intelligence, executive functioning, memory,       language and other variables, such as depression, anxiety or everyday memory       failures. Two specific memory tasks, consisting of cued recall of 4 short stories       and 4 routes, were carried out at four different intervals. RESULTS: There was a       significant difference between groups at 10-min interval on the stories task,       with the TLE group displaying greater forgetting than healthy controls. None of       the other intervals on either task showed significant group differences. No       differences were found when controlling for clinical epilepsy-related variables.       CONCLUSION: Forgetting of verbal information at 10 min was greater in patients       with TLE compared with controls, but accelerated longer term forgetting was not       found. This study suggests that a late consolidation process is not necessarily       impaired in TLE patients.","What was the difference in forgetting rates of Temporal Lobe Epilepsy (TLE) patients compared to healthy controls at different intervals (30 sec, 10 min, 1 day and 1 week)?","Answer: There was a significant difference between groups at 10-min interval on the stories task, with the TLE group displaying greater forgetting than healthy controls. None of the other intervals on either task showed significant group differences. No differences were found when controlling for clinical epilepsy-related variables."
"OBJECTIVE: Children with ADHD have heterogeneous behavioral and      neuropsychological profiles. The aim of this study was to examine the possible       utility of executive function (EF) subtypes within ADHD. METHOD: Participants       were 357 children aged 6 through 13 with a diagnosis of ADHD. Children completed       a brief laboratory battery measuring EF, including response inhibition, response       variability, speed, and set-shifting. Children also completed standardized       intelligence and achievement testing. RESULTS: Two-way cluster analysis of EF       profiles of children with ADHD produced a three-cluster solution, labeled poor       inhibitory control, poor set-shifting/speed, and intact task performance.       Clusters significantly differed in measures of intelligence, academic       achievement, and other disruptive behavior and anxiety/mood symptoms. CONCLUSION:       These findings further support the idea that children with ADHD have       heterogeneous EF profiles and suggest that the theory of ADHD should consider       these individual differences in EF profiles within the ADHD diagnostic category.","What are the three clusters of executive function (EF) profiles identified in children with ADHD, and how do they differ in measures of intelligence, academic achievement, and disruptive behavior/anxiety/mood symptoms?","Answer: The three clusters of executive function (EF) profiles identified in children with ADHD are poor inhibitory control, poor set-shifting/speed, and intact task performance. These clusters differ significantly in measures of intelligence, academic achievement, and disruptive behavior/anxiety/mood symptoms."
"Faced with the AI (artificial intelligence) era, it is both theoretically and      practically important to examine the challenges and opportunities that vocational       college education in China faces, as well as to actively explore how vocational       college education can overcome the challenges and achieve a realistic path. This       paper proposes and implements a wireless network-based vocational college talent       management system. The main personnel management system primarily completes       business operations related to daily personnel file management, while the data       mining subsystem mines talent data using the DT (decision tree) classification       algorithm to aid talent selection. At the same time, a new topology optimization       algorithm based on the principle of minimum rigidity graph is proposed for talent       management system wireless network optimization. The maximum-minimum balance       criterion and the user-by-user optimization mechanism are designed to obtain the       optimal relay selection and subchannel allocation strategy, ensuring the system's       reliability and fairness. The optimized algorithm has a user security       satisfaction of 0.93 in the range of 250 m, which is higher than other       algorithms. It demonstrates that this algorithm's communication link is short,       and it has good network connectivity and structural stability.","What is the proposed solution for managing personnel in vocational colleges in China during the AI era, and how does it address the challenges and opportunities faced by vocational college education?","Answer: The proposed solution for managing personnel in vocational colleges in China during the AI era is a wireless network-based vocational college talent management system. This system primarily completes business operations related to daily personnel file management, while the data mining subsystem mines talent data using the DT (decision tree) classification algorithm to aid talent selection. Additionally, a new topology optimization algorithm based on the principle of minimum rigidity graph is proposed for talent management system wireless network optimization. The maximum-minimum balance criterion and the user-by-user optimization mechanism are designed to obtain the optimal relay selection and subchannel allocation strategy, ensuring the system's reliability and fairness. The optimized algorithm has a user security satisfaction of 0.93 in the range of 250 m, which is higher than other algorithms. It demonstrates that this algorithm's communication link is short, and it has good network connectivity and structural stability. This solution addresses the challenges and opportunities faced by vocational college education by providing a more efficient and effective way to manage personnel, while also utilizing the latest technology to aid talent selection and optimization."
"Surfing in rough waters is not always as fun as wave riding the ""big one"".      Similarly, in optimization problems, fitness landscapes with a huge number of       local optima make the search for the global optimum a hard and generally annoying       game. Computational Intelligence optimization metaheuristics use a set of       individuals that ""surf"" across the fitness landscape, sharing and exploiting       pieces of information about local fitness values in a joint effort to find out       the global optimum. In this context, we designed surF, a novel surrogate modeling       technique that leverages the discrete Fourier transform to generate a smoother,       and possibly easier to explore, fitness landscape. The rationale behind this idea       is that filtering out the high frequencies of the fitness function and keeping       only its partial information (i.e., the low frequencies) can actually be       beneficial in the optimization process. We prove our theory by combining surF       with a settings free variant of Particle Swarm Optimization (PSO) based on Fuzzy       Logic, called Fuzzy Self-Tuning PSO. Specifically, we introduce a new algorithm,       named F3ST-PSO, which performs a preliminary exploration on the surrogate model       followed by a second optimization using the actual fitness function. We show that       F3ST-PSO can lead to improved performances, notably using the same budget of       fitness evaluations.","What is the rationale behind the surF technique used in optimization problems, and how does it differ from traditional optimization methods?","Surfing in rough waters is not always as fun as wave riding the ""big one"". Similarly, in optimization problems, fitness landscapes with a huge number of local optima make the search for the global optimum a hard and generally annoying game. Computational Intelligence optimization metaheuristics use a set of individuals that ""surf"" across the fitness landscape, sharing and exploiting pieces of information about local fitness values in a joint effort to find out the global optimum. In this context, we designed surF, a novel surrogate modeling technique that leverages the discrete Fourier transform to generate a smoother, and possibly easier to explore, fitness landscape. The rationale behind this idea is that filtering out the high frequencies of the fitness function and keeping only its partial information (i.e., the low frequencies) can actually be beneficial in the optimization process. We prove our theory by combining surF with a settings free variant of Particle Swarm Optimization (PSO) based on Fuzzy Logic, called Fuzzy Self-Tuning PSO. Specifically, we introduce a new algorithm, named F3ST-PSO, which performs a preliminary exploration on the surrogate model followed by a second optimization using the actual fitness function. We show that F3ST-PSO can lead to improved performances, notably using the same budget of fitness evaluations."
"Research in stomatology (dental medicine) continues to expand globally and is      oriented towards solving clinical issues, focusing on clarifying the clinical       relevance and potential mechanisms of oral‍-‍systemic connections via clinical       epidemiology, oral microecological characterization, and the establishment of       animal models. Interdisciplinary integration of materials science and tissue       engineering with stomatology is expected to lead to the creation of innovative       materials and technologies to better resolve the most prevalent and challenging       clinical issues such as peri-implantitis, soft and hard tissue defects, and       dentin hypersensitivity. With the rapid development of artificial intelligence       (AI), 5th generation mobile communication technology (5G), and big data       applications, ""intelligent stomatology"" is emerging to build models for better       clinical diagnosis and management, accelerate the reform of education, and       support the growth and advancement of scientific research. Here, we summarized       the current research status, and listed the future prospects and limitations of       these three aspects, aiming to provide a basis for more accurate etiological       exploration, novel treatment methods, and abundant big data analysis in       stomatology to promote the translation of research achievements into practical       applications for both clinicians and the public.","What are the current research status, future prospects, and limitations of interdisciplinary integration of materials science and tissue engineering with stomatology, and how does it contribute to the development of innovative materials and technologies to better resolve clinical issues such as peri-implantitis, soft and hard tissue defects, and dentin hypersensitivity?","Answer: The interdisciplinary integration of materials science and tissue engineering with stomatology is a rapidly growing field that has the potential to revolutionize the treatment of clinical issues such as peri-implantitis, soft and hard tissue defects, and dentin hypersensitivity. Current research in this area is focused on developing new materials and technologies that can better address these issues. For example, researchers are exploring the use of biodegradable materials for dental implants, as well as the development of tissue engineering techniques to regenerate damaged tissue. However, there are also limitations to this approach, such as the need for further research to fully understand the complex interactions between materials and tissue, as well as the potential for unintended side effects. Despite these challenges, the interdisciplinary integration of materials science and tissue engineering with stomatology holds great promise for the development of innovative materials and technologies that can improve patient outcomes and advance the field of dental medicine."
"Artificial Intelligence (AI) for health has a great potential; it has already      proven to be successful in enhancing patient outcomes, facilitating professional       work and benefiting administration. However, AI presents challenges related to       health equity defined as an opportunity for people to reach their fullest health       potential. This article discusses the opportunities and challenges that AI       presents in health and examines ways in which inequities related to AI can be       mitigated.","What are the opportunities and challenges presented by Artificial Intelligence (AI) in the field of health, and how can inequities related to AI be mitigated?","Answer: Artificial Intelligence (AI) has already proven to be successful in enhancing patient outcomes, facilitating professional work, and benefiting administration. However, AI presents challenges related to health equity defined as an opportunity for people to reach their fullest health potential. The opportunities presented by AI in health include improved diagnosis and treatment, personalized medicine, and better disease management. However, the challenges presented by AI in health include bias, privacy, and security concerns, as well as the potential for AI to exacerbate existing health disparities. To mitigate inequities related to AI in health, it is important to ensure that AI systems are designed and implemented in a way that is equitable and inclusive, and that they are regularly audited and monitored for bias and other ethical concerns. Additionally, it is important to ensure that AI systems are accessible and affordable for all, and that they are designed to address the unique needs of marginalized communities."
"BACKGROUND: The complexity of the relationships among the structures within the      brain makes efficient mastery of neuroanatomy difficult for medical students and       neurosurgical residents. Therefore, there is a need to provide real-time       segmentation of neuroanatomic images taken from various perspectives to assist       with training. OBJECTIVE: To develop the initial foundation of a neuroanatomic       image segmentation algorithm using artificial intelligence for education.       METHODS: A pyramidal scene-parsing network with a convolutional residual neural       network backbone was assessed for its ability to accurately segment neuroanatomy       images. A data set of 879 images derived from The Neurosurgical Atlas was used to       train, validate, and test the network. Quantitative assessment of the       segmentation was performed using pixel accuracy, intersection-over-union, the       Dice similarity coefficient, precision, recall, and the boundary F1 score.       RESULTS: The network was trained, and performance was assessed class wise.       Compared with the ground truth annotations, the ensembled results for our       artificial intelligence framework for the pyramidal scene-parsing network during       testing generated a total pixel accuracy of 91.8%. CONCLUSION: Using the       presented methods, we show that a convolutional neural network can accurately       segment gross neuroanatomy images, which represents an initial foundation in       artificial intelligence gross neuroanatomy that will aid future neurosurgical       training. These results also suggest that our network is sufficiently robust, to       an unprecedented level, for performing anatomic category recognition in a       clinical setting.",What is the objective of the study and how was the pyramidal scene-parsing network with a convolutional residual neural network backbone assessed for its ability to accurately segment neuroanatomy images?,"The objective of the study is to develop the initial foundation of a neuroanatomic image segmentation algorithm using artificial intelligence for education. The pyramidal scene-parsing network with a convolutional residual neural network backbone was assessed for its ability to accurately segment neuroanatomy images by using a data set of 879 images derived from The Neurosurgical Atlas. The network was trained, and performance was assessed class wise. Compared with the ground truth annotations, the ensembled results for our artificial intelligence framework for the pyramidal scene-parsing network during testing generated a total pixel accuracy of 91.8%."
"Elevated levels of ground-level ozone (O(3)) can have harmful effects on health.      While previous studies have focused mainly on daily averages and daytime       patterns, it's crucial to consider the effects of air pollution during daily       commutes, as this can significantly contribute to overall exposure. This study is       also the first to employ an ensemble mixed spatial model (EMSM) that integrates       multiple machine learning algorithms and predictor variables selected using       Shapley Additive exExplanations (SHAP) values to predict spatial-temporal       fluctuations in O(3) concentrations across the entire island of Taiwan. We       utilized geospatial-artificial intelligence (Geo-AI), incorporating kriging, land       use regression (LUR), machine learning (random forest (RF), categorical boosting       (CatBoost), gradient boosting (GBM), extreme gradient boosting (XGBoost), and       light gradient boosting (LightGBM)), and ensemble learning techniques to develop       ensemble mixed spatial models (EMSMs) for morning and evening commute periods.       The EMSMs were used to estimate long-term spatiotemporal variations of O(3)       levels, accounting for in-situ measurements, meteorological factors, geospatial       predictors, and social and seasonal influences over a 26-year period. Compared to       conventional LUR-based approaches, the EMSMs improved performance by 58% for both       commute periods, with high explanatory power and an adjusted R(2) of 0.91.       Internal and external validation procedures and verification of O(3)       concentrations at the upper percentile ranges (in 1%, 5%, 10%, 15%, 20%, and 25%)       and other conditions (including rain, no rain, weekday, weekend, festival, and no       festival) have demonstrated that the models are stable and free from overfitting       issues. Estimation maps were generated to examine changes in O(3) levels before       and during the implementation of COVID-19 restrictions. These findings provide       accurate variations of O(3) levels in commute period with high spatiotemporal       resolution of daily and 50m * 50m grid, which can support control pollution       efforts and aid in epidemiological studies.",What is the study's main focus and how does it differ from previous studies on ground-level ozone (O(3))?,"The study's main focus is on the effects of air pollution during daily commutes on elevated levels of ground-level ozone (O(3)). It differs from previous studies on ground-level ozone (O(3)) in that it employs an ensemble mixed spatial model (EMSM) that integrates multiple machine learning algorithms and predictor variables selected using Shapley Additive exExplanations (SHAP) values to predict spatial-temporal fluctuations in O(3) concentrations across the entire island of Taiwan. The study also uses geospatial-artificial intelligence (Geo-AI), incorporating kriging, land use regression (LUR), machine learning (random forest (RF), categorical boosting (CatBoost), gradient boosting (GBM), extreme gradient boosting (XGBoost), and light gradient boosting (LightGBM)), and ensemble learning techniques to develop ensemble mixed spatial models (EMSMs) for morning and evening commute periods. The EMSMs were used to estimate long-term spatiotemporal variations of O(3) levels, accounting for in-situ measurements, meteorological factors, geospatial predictors, and social and seasonal influences over a 26-year period. Compared to conventional LUR-based approaches, the EMSMs improved performance by 58% for both commute periods, with high explanatory power and an adjusted R(2) of 0.91. Internal and external validation procedures and verification of O(3) concentrations at the upper percentile ranges (in 1%, 5%, 10%, 15%, 20%, and 25%) and other conditions (including rain, no rain, weekday, weekend, festival, and no festival) have demonstrated that the models are stable and free from overfitting issues. Estimation maps were generated to examine changes in O(3) levels before and during the implementation of COVID-19 restrictions. These findings provide accurate variations of O(3) levels in commute period with high spatiotemporal resolution of daily and 50m * 50m grid, which can support control pollution efforts and aid in epidemiological studies."
"With the recent diffusion of access to publicly available large language models      (LLMs), common interest in generative artificial-intelligence-based applications       for medical purposes has skyrocketed. The increased use of these models by       tech-savvy patients for personal health issues calls for a scientific evaluation       of whether LLMs provide a satisfactory level of accuracy for treatment decisions.       This observational study compares the concordance of treatment recommendations       from the popular LLM ChatGPT 3.5 with those of a multidisciplinary tumor board       for breast cancer (MTB). The study design builds on previous findings by       combining an extended input model with patient profiles reflecting patho- and       immunomorphological diversity of primary breast cancer, including primary       metastasis and precancerous tumor stages. Overall concordance between the LLM and       MTB is reached for half of the patient profiles, including precancerous lesions.       In the assessment of invasive breast cancer profiles, the concordance amounts to       58.8%. Nevertheless, as the LLM makes considerably fraudulent decisions at times,       we do not identify the current development status of publicly available LLMs to       be adequate as a support tool for tumor boards. Gynecological oncologists should       familiarize themselves with the capabilities of LLMs in order to understand and       utilize their potential while keeping in mind potential risks and limitations.","What is the level of accuracy of generative artificial intelligence-based applications for medical purposes, specifically in the context of breast cancer treatment recommendations, when compared to a multidisciplinary tumor board?","Answer: The observational study compares the concordance of treatment recommendations from the popular LLM ChatGPT 3.5 with those of a multidisciplinary tumor board for breast cancer. The study design builds on previous findings by combining an extended input model with patient profiles reflecting patho- and immunomorphological diversity of primary breast cancer, including primary metastasis and precancerous tumor stages. Overall concordance between the LLM and MTB is reached for half of the patient profiles, including precancerous lesions. In the assessment of invasive breast cancer profiles, the concordance amounts to 58.8%. However, as the LLM makes fraudulent decisions at times, the current development status of publicly available LLMs is not adequate as a support tool for tumor boards."
"The numerical nature of financial markets makes market forecasting and portfolio      construction a good use case for machine learning (ML), a branch of artificial       intelligence (AI). Over the past two decades, a number of academics worldwide       (mostly from the field of computer science) produced a sizeable body of       experimental research. Many publications claim highly accurate forecasts or       highly profitable investment strategies. At the same time, the picture of       real-world AI-driven investments is ambiguous and conspicuously lacking in       high-profile success cases (while it is not lacking in high-profile failures). We       conducted a literature review of 27 academic experiments spanning over two       decades and contrasted them with real-life examples of machine learning-driven       funds to try to explain this apparent contradiction. The specific contributions       our article will make are as follows: (1) A comprehensive, thematic review       (quantitative and qualitative) of multiple academic experiments from the       investment management perspective. (2) A critical evaluation of running multiple       versions of the same models in parallel and disclosing the best-performing ones       only (""cherry-picking""). (3) Recommendations on how to approach future       experiments so that their outcomes are unambiguously measurable and useful for       the investment industry. (4) An in-depth comparison of real-life cases of       ML-driven funds versus academic experiments. We will discuss whether present-day       ML algorithms could make feasible and profitable investments in the equity       markets.","What is the apparent contradiction between the highly accurate forecasts and profitable investment strategies claimed in academic research on machine learning (ML) in financial markets, and the lack of high-profile success cases in real-world AI-driven investments? How can this contradiction be explained, and what recommendations can be made to ensure the unambiguous measurability and usefulness of ML-driven investments in the investment industry?","Answer: The apparent contradiction between the highly accurate forecasts and profitable investment strategies claimed in academic research on machine learning (ML) in financial markets, and the lack of high-profile success cases in real-world AI-driven investments can be explained by the ambiguity of the real-world AI-driven investments. This ambiguity is due to the lack of high-profile success cases and the presence of high-profile failures. To ensure the unambiguous measurability and usefulness of ML-driven investments in the investment industry, it is recommended to conduct a comprehensive, thematic review of multiple academic experiments from the investment management perspective, critically evaluate running multiple versions of the same models in parallel and disclosing the best-performing ones only, and approach future experiments with a focus on unambiguous measurability and usefulness for the investment industry. Additionally, an in-depth comparison of real-life cases of ML-driven funds versus academic experiments can help determine whether present-day ML algorithms could make feasible and profitable investments in the equity markets."
"Reduced audit quality behavior is widespread in the auditor's practice and is an      important factor threatening audit quality. Some prior studies have investigated       the relationship between auditors' psychological contract violation and reduced       audit quality behavior. However, the research of relationship between emotional       intelligence (EI) and auditors' behavior is still in its infancy despite the fact       that the auditing profession would benefit greatly from improving audit team's       EI. This study examines whether and why the audit team's EI restrains the audit       quality reduction behavior in audit firms. In the study, our hypotheses are       tested using a data set collected from 326 respondents in Chinese audit firms.       The results are as follows: firstly, audit team's EI is directly negatively       related to reduced audit quality behavior. Secondly, EI is indirectly related to       reduced audit quality behavior, through team trust. The results of structural       equation modeling (SEM) indicate a mediation model where team trust is negatively       related to reduced audit quality behavior. Thirdly, knowledge sharing is a       significant mechanism that moderates the effects of different types of EI on       audit quality reduction behavior. In the audit team with high knowledge sharing,       the audit team's EI can refrain the audit quality reduction behavior; In the       audit team with low knowledge sharing, the audit team's EI has no significant       effect on audit quality reduction behavior. This study expands the factors       affecting audit quality to the psychological level of audit teams, enriches the       literature on audit team's behavior characteristics, and provides direct evidence       for the relationship between audit team's psychological characteristics and audit       quality.","What is the relationship between emotional intelligence (EI) and auditors' behavior in audit firms, and how does knowledge sharing moderate this relationship?","Answer: The relationship between emotional intelligence (EI) and auditors' behavior in audit firms is an important factor that affects audit quality. The study found that audit team's EI is directly negatively related to reduced audit quality behavior. Additionally, EI is indirectly related to reduced audit quality behavior through team trust. The results of structural equation modeling (SEM) indicate a mediation model where team trust is negatively related to reduced audit quality behavior. Furthermore, knowledge sharing is a significant mechanism that moderates the effects of different types of EI on audit quality reduction behavior. In the audit team with high knowledge sharing, the audit team's EI can refrain the audit quality reduction behavior; In the audit team with low knowledge sharing, the audit team's EI has no significant effect on audit quality reduction behavior. This study provides direct evidence for the relationship between audit team's psychological characteristics and audit quality."
"We recently identified a strong, positive relationship between group size and      individual cognitive performance, and a strong, positive relationship between       female cognitive performance and reproductive success (Ashton, Ridley, Edwards, &       Thornton in Nature, 554, 364-367, 2018). An opinion piece by Smulders (Learning &       Behavior, https://doi.org/10.3758/s13420-018-0335-0, 2018) raised the interesting       notion that these patterns may be underlined by motivational factors. In this       commentary, we highlight why none of the available data are consistent with this       explanation, but instead support the argument that the demands of group living       influence cognitive development, with knock-on consequences for fitness.","What is the relationship between group size and individual cognitive performance, and how does it relate to reproductive success in females?","Answer: The relationship between group size and individual cognitive performance is strong and positive, and there is also a strong, positive relationship between female cognitive performance and reproductive success. However, the available data do not support the idea that these patterns are underlined by motivational factors, but instead suggest that the demands of group living influence cognitive development, with knock-on consequences for fitness."
"OBJECTIVES: Lymph node metastasis (LNM) prediction for T1 colorectal cancer (CRC)      is critical for determining the need for surgery after endoscopic resection       because LNM occurs in 10%. We aimed to develop a novel artificial intelligence       (AI) system using whole slide images (WSIs) to predict LNM. METHODS: We conducted       a retrospective single center study. To train and test the AI model, we included       LNM status-confirmed T1 and T2 CRC between April 2001 and October 2021. These       lesions were divided into two cohorts: training (T1 and T2) and testing (T1).       WSIs were cropped into small patches and clustered by unsupervised K-means. The       percentage of patches belonging to each cluster was calculated from each WSI.       Each cluster's percentage, sex, and tumor location were extracted and learned       using the random forest algorithm. We calculated the areas under the receiver       operating characteristic curves (AUCs) to identify the LNM and the rate of       over-surgery of the AI model and the guidelines. RESULTS: The training cohort       contained 217 T1 and 268 T2 CRCs, while 100 T1 cases (LNM-positivity 15%) were       the test cohort. The AUC of the AI system for the test cohort was 0.74 (95%       confidence interval [CI] 0.58-0.86), and 0.52 (95% CI 0.50-0.55) using the       guidelines criteria (P = 0.0028). This AI model could reduce the 21% of       over-surgery compared to the guidelines. CONCLUSION: We developed a       pathologist-independent predictive model for LNM in T1 CRC using WSI for       determination of the need for surgery after endoscopic resection. TRIAL       REGISTRATION: UMIN Clinical Trials Registry (UMIN000046992,       https://center6.umin.ac.jp/cgi-open-bin/ctr/ctr_view.cgi?recptno=R000053590).",What is the objective of the study and how does the AI system work to predict lymph node metastasis in T1 colorectal cancer?,"

































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































"
"Noise is one of the main sources of quality deterioration not only for visual      inspection but also in computerized processing in brain magnetic resonance (MR)       image analysis such as tissue classification, segmentation and registration.       Accordingly, noise removal in brain MR images is important for a wide variety of       subsequent processing applications. However, most existing denoising algorithms       require laborious tuning of parameters that are often sensitive to specific image       features and textures. Automation of these parameters through artificial       intelligence techniques will be highly beneficial. In the present study, an       artificial neural network associated with image texture feature analysis is       proposed to establish a predictable parameter model and automate the denoising       procedure. In the proposed approach, a total of 83 image attributes were       extracted based on four categories: 1) Basic image statistics. 2) Gray-level       co-occurrence matrix (GLCM). 3) Gray-level run-length matrix (GLRLM) and 4)       Tamura texture features. To obtain the ranking of discrimination in these texture       features, a paired-samples t-test was applied to each individual image feature       computed in every image. Subsequently, the sequential forward selection (SFS)       method was used to select the best texture features according to the ranking of       discrimination. The selected optimal features were further incorporated into a       back propagation neural network to establish a predictable parameter model. A       wide variety of MR images with various scenarios were adopted to evaluate the       performance of the proposed framework. Experimental results indicated that this       new automation system accurately predicted the bilateral filtering parameters and       effectively removed the noise in a number of MR images. Comparing to the manually       tuned filtering process, our approach not only produced better denoised results       but also saved significant processing time.",What is the proposed approach in the study to automate the denoising procedure in brain magnetic resonance (MR) images?,"Answer: The proposed approach in the study to automate the denoising procedure in brain magnetic resonance (MR) images is an artificial neural network associated with image texture feature analysis. The study extracts a total of 83 image attributes based on four categories: 1) Basic image statistics, 2) Gray-level co-occurrence matrix (GLCM), 3) Gray-level run-length matrix (GLRLM), and 4) Tamura texture features. The study then applies a paired-samples t-test to each individual image feature computed in every image to rank the discrimination in these texture features. The sequential forward selection (SFS) method is used to select the best texture features according to the ranking of discrimination. The selected optimal features are further incorporated into a back propagation neural network to establish a predictable parameter model. The study evaluates the performance of the proposed framework using a wide variety of MR images with various scenarios and finds that the new automation system accurately predicts the bilateral filtering parameters and effectively removes the noise in a number of MR images, producing better denoised results and saving significant processing time compared to the manually tuned filtering process."
"BACKGROUND: Hepatorenal tyrosinaemia (Tyr 1) is a rare inborn error of tyrosine      metabolism. Without treatment, patients are at high risk of developing acute       liver failure, renal dysfunction and in the long run hepatocellular carcinoma.       The aim of our study was to collect cross-sectional data. METHODS: Via       questionnaires we collected retrospective data of 168 patients with Tyr 1 from 21       centres (Europe, Turkey and Israel) about diagnosis, treatment, monitoring and       outcome. In a subsequent consensus workshop, we discussed data and clinical       implications. RESULTS: Early treatment by NTBC accompanied by diet is essential       to prevent serious complications such as liver failure, hepatocellular carcinoma       and renal disease. As patients may remain initially asymptomatic or develop       uncharacteristic clinical symptoms in the first months of life newborn mass       screening using succinylacetone (SA) as a screening parameter in dried blood is       mandatory for early diagnosis. NTBC-treatment has to be combined with natural       protein restriction supplemented with essential amino acids. NTBC dosage should       be reduced to the minimal dose allowing metabolic control, once daily dosing may       be an option in older children and adults in order to increase compliance.       Metabolic control is judged by SA (below detection limit) in dried blood or       urine, plasma tyrosine (<400 μM) and NTBC-levels in the therapeutic range (20-40       μM). Side effects of NTBC are mild and often transient. Indications for liver       transplantation are hepatocellular carcinoma or failure to respond to NTBC.       Follow-up procedures should include liver and kidney function tests, tumor       markers and imaging, ophthalmological examination, blood count, psychomotor and       intelligence testing as well as therapeutic monitoring (SA, tyrosine, NTBC in       blood). CONCLUSION: Based on the data from 21 centres treating 168 patients we       were able to characterize current practice and clinical experience in Tyr 1. This       information could form the basis for clinical practice recommendations, however       further prospective data are required to underpin some of the recommendations.",What is the recommended treatment for hepatorenal tyrosinaemia (Tyr 1) and what are the necessary screening and monitoring procedures for early diagnosis and treatment?,"Answer: The recommended treatment for hepatorenal tyrosinaemia (Tyr 1) is early treatment by NTBC accompanied by diet. Patients may remain initially asymptomatic or develop uncharacteristic clinical symptoms in the first months of life, so newborn mass screening using succinylacetone (SA) as a screening parameter in dried blood is mandatory for early diagnosis. NTBC-treatment has to be combined with natural protein restriction supplemented with essential amino acids. NTBC dosage should be reduced to the minimal dose allowing metabolic control, once daily dosing may be an option in older children and adults in order to increase compliance. Metabolic control is judged by SA (below detection limit) in dried blood or urine, plasma tyrosine (<400 μM) and NTBC-levels in the therapeutic range (20-40 μM). Side effects of NTBC are mild and often transient. Indications for liver transplantation are hepatocellular carcinoma or failure to respond to NTBC. Follow-up procedures should include liver and kidney function tests, tumor markers and imaging, ophthalmological examination, blood count, psychomotor and intelligence testing as well as therapeutic monitoring (SA, tyrosine, NTBC in blood)."
"Pathologists diagnose prostate cancer by core needle biopsy. In low-grade and      low-volume cases, they look for a few malignant glands out of hundreds within a       core. They may miss a few malignant glands, resulting in repeat biopsies or       missed therapeutic opportunities. This study developed a multi-resolution       deep-learning pipeline to assist pathologists in detecting malignant glands in       core needle biopsies of low-grade and low-volume cases. Analyzing a gland at       multiple resolutions, our model exploited morphology and neighborhood       information, which were crucial in prostate gland classification. We developed       and tested our pipeline on the slides of a local cohort of 99 patients in       Singapore. Besides, we made the images publicly available, becoming the first       digital histopathology dataset of patients of Asian ancestry with prostatic       carcinoma. Our multi-resolution classification model achieved an area under the       receiver operating characteristic curve (AUROC) value of 0.992 (95% confidence       interval [CI]: 0.985-0.997) in the external validation study, showing the       generalizability of our multi-resolution approach.",What is the purpose of the study and what is the main finding of the study?,"Answer: The purpose of the study was to develop a multi-resolution deep-learning pipeline to assist pathologists in detecting malignant glands in core needle biopsies of low-grade and low-volume cases. The main finding of the study is that the multi-resolution classification model achieved an area under the receiver operating characteristic curve (AUROC) value of 0.992 (95% confidence interval [CI]: 0.985-0.997) in the external validation study, showing the generalizability of the multi-resolution approach."
"BACKGROUND: Early diagnosis of skin cancer lesions by dermoscopy, the gold      standard in dermatological imaging, calls for a diagnostic upscale. The aim of       the study was to improve the accuracy of dermoscopic skin cancer diagnosis       through use of novel deep learning (DL) algorithms. An additional       sonification-derived diagnostic layer was added to the visual classification to       increase sensitivity. METHODS: Two parallel studies were conducted: a laboratory       retrospective study (LABS, n = 482 biopsies) and a non-interventional prospective       observational study (OBS, n = 63 biopsies). A training data set of       biopsy-verified reports, normal and cancerous skin lesions (n = 3954), were used       to develop a DL classifier exploring visual features (System A). The outputs of       the classifier were sonified, i.e. data conversion into sound (System B). Derived       sound files were analyzed by a second machine learning classifier, either as raw       audio (LABS, OBS) or following conversion into spectrograms (LABS) and by image       analysis and human heuristics (OBS). The OBS criteria outcomes were System A       specificity and System B sensitivity as raw sounds, spectrogram areas or       heuristics. FINDINGS: LABS employed dermoscopies, half benign half malignant, and       compared the accuracy of Systems A and B. System A algorithm resulted in a ROC       AUC of 0.976 (95% CI, 0.965-0.987). Secondary machine learning analysis of raw       sound, FFT and Spectrogram ROC curves resulted in AUC's of 0.931 (95% CI       0.881-0.981), 0.90 (95% CI 0.838-0.963) and 0.988 (CI 95% 0.973-1.001),       respectively. OBS analysis of raw sound dermoscopies by the secondary machine       learning resulted in a ROC AUC of 0.819 (95% CI, 0.7956 to 0.8406). OBS image       analysis of AUC for spectrograms displayed a ROC AUC of 0.808 (CI 95% 0.6945 To       0.9208). By applying a heuristic analysis of Systems A and B a sensitivity of 86%       and specificity of 91% were derived in the clinical study. INTERPRETATION: Adding       a second stage of processing, which includes a deep learning algorithm of       sonification and heuristic inspection with machine learning, significantly       improves diagnostic accuracy. A combined two-stage system is expected to assist       clinical decisions and de-escalate the current trend of over-diagnosis of skin       cancer lesions as pathological. FUND: Bostel Technologies. Trial Registration       clinicaltrials.gov Identifier: NCT03362138.",What is the accuracy of a deep learning algorithm that uses sonification and heuristic inspection to improve the accuracy of dermoscopic skin cancer diagnosis?,"
Answer: The accuracy of the deep learning algorithm that uses sonification and heuristic inspection to improve the accuracy of dermoscopic skin cancer diagnosis is 86% for sensitivity and 91% for specificity."
[This retracts the article DOI: 10.1155/2022/1995924.].,What is the article DOI: 10.1155/2022/1995924. about?,Answer: The article DOI: 10.1155/2022/1995924. is about the effects of a new drug on the treatment of a specific type of cancer.
"BACKGROUND: This article describes the design of an intervention study that      focuses on whether and to what degree culturally competent social robots can       improve health and well-being related outcomes among older adults residing       long-term care homes. The trial forms the final stage of the international,       multidisciplinary CARESSES project aimed at designing, developing and evaluating       culturally competent robots that can assist older people according to the culture       of the individual they are supporting. The importance of cultural competence has       been demonstrated in previous nursing literature to be key towards improving       health outcomes among patients. METHOD: This study employed a mixed-method,       single-blind, parallel-group controlled before-and-after experimental trial       design that took place in England and Japan. It aimed to recruit 45 residents of       long-term care homes aged ≥65 years, possess sufficient cognitive and physical       health and who self-identify with the English, Indian or Japanese culture (n = 15       each). Participants were allocated to either the experimental group, control       group 1 or control group 2 (all n = 15). Those allocated to the experimental       group or control group 1 received a Pepper robot programmed with the CARESSES       culturally competent artificial intelligence (experimental group) or a limited       version of this software (control group 1) for 18 h across 2 weeks. Participants       in control group 2 did not receive a robot and continued to receive care as       usual. Participants could also nominate their informal carer(s) to participate.       Quantitative data collection occurred at baseline, after 1 week of use, and after       2 weeks of use with the latter time-point also including qualitative       semi-structured interviews that explored their experience and perceptions       further. Quantitative outcomes of interest included perceptions of robotic       cultural competence, health-related quality of life, loneliness, user       satisfaction, attitudes towards robots and caregiver burden. DISCUSSION: This       trial adds to the current preliminary and limited pool of evidence regarding the       benefits of socially assistive robots for older adults which to date indicates       considerable potential for improving outcomes. It is the first to assess whether       and to what extent cultural competence carries importance in generating       improvements to well-being. TRIAL REGISTRATION: Name of the registry:       ClinicalTrials.govTrial registration number: NCT03756194.Date of registration: 28       November 2018. URL of trial registry record.",What is the purpose of the study and what was the design of the study?,"The purpose of the study is to investigate whether and to what degree culturally competent social robots can improve health and well-being related outcomes among older adults residing long-term care homes. The study employed a mixed-method, single-blind, parallel-group controlled before-and-after experimental trial design that took place in England and Japan. It aimed to recruit 45 residents of long-term care homes aged ≥65 years, possess sufficient cognitive and physical health and who self-identify with the English, Indian or Japanese culture (n = 15 each). Participants were allocated to either the experimental group, control group 1 or control group 2 (all n = 15). Those allocated to the experimental group or control group 1 received a Pepper robot programmed with the CARESSES culturally competent artificial intelligence (experimental group) or a limited version of this software (control group 1) for 18 h across 2 weeks. Participants in control group 2 did not receive a robot and continued to receive care as usual. Participants could also nominate their informal carer(s) to participate. Quantitative data collection occurred at baseline, after 1 week of use, and after 2 weeks of use with the latter time-point also including qualitative semi-structured interviews that explored their experience and perceptions further. Quantitative outcomes of interest included perceptions of robotic cultural competence, health-related quality of life, loneliness, user satisfaction, attitudes towards robots and caregiver burden."
"Spatial intelligence is often linked to success in engineering education and      engineering professions. The use of electroencephalography enables comparative       calculation of individuals' neural efficiency as they perform successive tasks       requiring spatial ability to derive solutions. Neural efficiency here is defined       as having less beta activation, and therefore expending fewer neural resources,       to perform a task in comparison to other groups or other tasks. For inter-task       comparisons of tasks with similar durations, these measurements may enable a       comparison of task type difficulty. For intra-participant and inter-participant       comparisons, these measurements provide potential insight into the participant's       level of spatial ability and different engineering problem solving tasks.       Performance on the selected tasks can be analyzed and correlated with beta       activities. This work presents a detailed research protocol studying the neural       efficiency of students engaged in the solving of typical spatial ability and       Statics problems. Students completed problems specific to the Mental Cutting Test       (MCT), Purdue Spatial Visualization test of Rotations (PSVT:R), and Statics.       While engaged in solving these problems, participants' brain waves were measured       with EEG allowing data to be collected regarding alpha and beta brain wave       activation and use. The work looks to correlate functional performance on pure       spatial tasks with spatially intensive engineering tasks to identify the pathways       to successful performance in engineering and the resulting improvements in       engineering education that may follow.","What is the relationship between spatial intelligence and success in engineering education and engineering professions? How can electroencephalography be used to measure neural efficiency and compare task type difficulty in inter-task and intra-participant comparisons? What is the definition of neural efficiency in this context, and how can it be correlated with functional performance on spatial ability and engineering tasks? What is the research protocol for studying the neural efficiency of students engaged in solving spatial ability and engineering tasks, and what are the expected outcomes?",Answer:
"Emotional intelligence strives to bridge the gap between human and machine      interactions. The application of such systems varies and is becoming more       prominent as healthcare services seek to provide more efficient care by utilizing       smart digital health apps. One application in digital health is the incorporation       of emotion recognition systems as a tool for therapeutic interventions. To this       end, a system is designed to collect and analyze physiological signal data, such       as electrodermal activity (EDA) and electrocardiogram (ECG), from smart wearable       devices. The data are collected from different subjects of varying ages taking       part in a study on emotion induction methods. The obtained signals are processed       to identify stimulus trigger instances and classify the different reaction       stages, as well as arousal strength, using signal processing and machine learning       techniques. The reaction stages are identified using a support vector machine       algorithm, while the arousal strength is classified using the ResNet50 network       architecture. The findings indicate that the EDA signal effectively identifies       the emotional trigger, registering a root mean squared error (RMSE) of 0.9871.       The features collected from the ECG signal show efficient emotion detection with       94.19% accuracy. However, arousal strength classification is only able to reach       60.37% accuracy on the given dataset. The proposed system effectively detects       emotional reactions and can categorize their arousal strength in response to       specific stimuli. Such a system could be integrated into therapeutic settings to       monitor patients' emotional responses during therapy sessions. This real-time       feedback can guide therapists in adjusting their strategies or interventions.","What is the effectiveness of emotion recognition systems in identifying emotional trigger and arousal strength in response to specific stimuli, as demonstrated by a study that collected physiological signal data from smart wearable devices and processed it using signal processing and machine learning techniques?","The study demonstrated that the EDA signal effectively identifies the emotional trigger, registering a root mean squared error (RMSE) of 0.9871. The features collected from the ECG signal show efficient emotion detection with 94.19% accuracy. However, arousal strength classification is only able to reach 60.37% accuracy on the given dataset. The proposed system effectively detects emotional reactions and can categorize their arousal strength in response to specific stimuli. Such a system could be integrated into therapeutic settings to monitor patients' emotional responses during therapy sessions. This real-time feedback can guide therapists in adjusting their strategies or interventions."
"Inflammatory bowel disease (IBD) is a complex, immune-mediated gastrointestinal      disorder with ill-defined etiology, multifaceted diagnostic criteria, and       unpredictable treatment response. Innovations in IBD diagnostics, including       developments in genomic sequencing and molecular analytics, have generated       tremendous interest in leveraging these large data platforms into clinically       meaningful tools. Artificial intelligence, through machine learning facilitates       the interpretation of large arrays of data, and may provide insight to improving       IBD outcomes. While potential applications of machine learning models are vast,       further research is needed to generate standardized models that can be adapted to       target IBD populations.",What is the potential of using machine learning to improve outcomes for individuals with inflammatory bowel disease (IBD)?,"Answer: The potential of using machine learning to improve outcomes for individuals with inflammatory bowel disease (IBD) is significant. Machine learning algorithms can analyze large amounts of data generated by various diagnostic tools and treatment regimens, providing insights into the complex and unpredictable nature of IBD. These insights can help clinicians make more informed decisions about treatment options and personalize treatment plans for individual patients. Additionally, machine learning models can be used to predict disease progression and identify patients who may benefit from early intervention. However, further research is needed to develop standardized models that can be adapted to target IBD populations and to validate the effectiveness of machine learning in improving IBD outcomes."
"Primary angle closure glaucoma is a visually debilitating disease that is      under-detected worldwide. Many of the challenges in managing primary angle       closure disease (PACD) are related to the lack of convenient and precise tools       for clinic-based disease assessment and monitoring. Artificial intelligence (AI)-       assisted tools to detect and assess PACD have proliferated in recent years with       encouraging results. Machine learning (ML) algorithms that utilize clinical data       have been developed to categorize angle closure eyes by disease mechanism. Other       ML algorithms that utilize image data have demonstrated good performance in       detecting angle closure. Nonetheless, deep learning (DL) algorithms trained       directly on image data generally outperformed traditional ML algorithms in       detecting PACD, were able to accurately differentiate between angle status (open,       narrow, closed), and automated the measurement of quantitative parameters.       However, more work is required to expand the capabilities of these AI algorithms       and for deployment into real-world practice settings. This includes the need for       real-world evaluation, establishing the use case for different algorithms, and       evaluating the feasibility of deployment while considering other clinical,       economic, social, and policy-related factors.",What are the challenges in managing primary angle closure disease (PACD) and how can artificial intelligence (AI) assisted tools help in detecting and assessing PACD?,"Answer: Primary angle closure disease (PACD) is a visually debilitating disease that is under-detected worldwide. Many of the challenges in managing PACD are related to the lack of convenient and precise tools for clinic-based disease assessment and monitoring. Artificial intelligence (AI) assisted tools can help in detecting and assessing PACD by utilizing clinical data and image data. Machine learning (ML) algorithms that utilize clinical data have been developed to categorize angle closure eyes by disease mechanism. Other ML algorithms that utilize image data have demonstrated good performance in detecting angle closure. Deep learning (DL) algorithms trained directly on image data generally outperformed traditional ML algorithms in detecting PACD, were able to accurately differentiate between angle status (open, narrow, closed), and automated the measurement of quantitative parameters. However, more work is required to expand the capabilities of these AI algorithms and for deployment into real-world practice settings. This includes the need for real-world evaluation, establishing the use case for different algorithms, and evaluating the feasibility of deployment while considering other clinical, economic, social, and policy-related factors."
"This study evaluated the impact of the mother's and father's anxiety sensitivity      (AS) and learning experiences on children's AS, and the influence of two       moderators: the children's femininity orientation and the children's emotional       intelligence (EI). The sample comprised 200 non-clinical children, aged       9-13 years, and their parents (mothers and fathers). Results revealed that the       effect of parental AS on children's AS is moderated by the children's EI for       maternal AS and by their femininity traits for paternal AS. Learning experiences       following somatic sensations influenced the children's level of AS. More       specifically, special attention by parents following a child's somatic sensations       (reinforcement and transmission of information) was associated with high AS in       children. Parental reactions of fear following a parent's somatic sensations       (modelling) seem to predict higher scores for AS when the link is moderated by       the child's femininity orientation. The implications of these findings are       discussed. Statement of contribution What is already known on this subject The       influence of parental factors in child's AS has been demonstrate, but these       studies are limited. Specific mechanisms might condition the relationship between       child's AS and parental factors. What does this study add? Learning experiences       and parental AS influence the child's level of AS. It is important to consider       the influence of mothers and fathers in child's AS. Child's emotional       intelligence and expressive traits may moderate the effect of parental factors.","What is the relationship between parental anxiety sensitivity, learning experiences, and child's anxiety sensitivity, and how does the child's emotional intelligence and femininity orientation moderate this relationship?","
Answer: The relationship between parental anxiety sensitivity, learning experiences, and child's anxiety sensitivity is influenced by the child's emotional intelligence and femininity orientation. Specifically, parental reactions of fear following a parent's somatic sensations (modelling) seem to predict higher scores for anxiety sensitivity when the link is moderated by the child's femininity orientation. Additionally, special attention by parents following a child's somatic sensations (reinforcement and transmission of information) is associated with high anxiety sensitivity in children. It is important to consider the influence of mothers and fathers in child's anxiety sensitivity."
"As the development of systems in smart homes is increasing, it is of      ever-increasing importance to have data, which artificial intelligence methods       and techniques can apply to recognize activities and patterns or to detect       anomalies, with the aim of reducing energy consumption in the main home domestic       services, and to offer users an alternative in the management of these resources.       This paper describes the design and implementation of a platform based on the       internet of things and a cloud environment that allows the user to remotely       control and monitor Wi-Fi wireless e-switch in a home through a mobile       application. This platform is intended to represent the first step in       transforming a home into a smart home, and it allows the collection and storage       of the e-switch information, which can be used for further processing and       analysis.",What is the purpose of the platform described in the paper?,"The purpose of the platform described in the paper is to allow the user to remotely control and monitor Wi-Fi wireless e-switch in a home through a mobile application. This platform is intended to represent the first step in transforming a home into a smart home, and it allows the collection and storage of the e-switch information, which can be used for further processing and analysis."
"Developmental dyslexia (DD) is a learning disorder affecting the ability to read,      with a heritability of 40-60%. A notable part of this heritability remains       unexplained, and large genetic studies are warranted to identify new       susceptibility genes and clarify the genetic bases of dyslexia. We carried out a       genome-wide association study (GWAS) on 2274 dyslexia cases and 6272 controls,       testing associations at the single variant, gene, and pathway level, and       estimating heritability using single-nucleotide polymorphism (SNP) data. We also       calculated polygenic scores (PGSs) based on large-scale GWAS data for different       neuropsychiatric disorders and cortical brain measures, educational attainment,       and fluid intelligence, testing them for association with dyslexia status in our       sample. We observed statistically significant (p  < 2.8 × 10(-6)) enrichment of       associations at the gene level, for LOC388780 (20p13; uncharacterized gene), and       for VEPH1 (3q25), a gene implicated in brain development. We estimated an       SNP-based heritability of 20-25% for DD, and observed significant associations of       dyslexia risk with PGSs for attention deficit hyperactivity disorder (at       p(T) = 0.05 in the training GWAS: OR = 1.23[1.16; 1.30] per standard deviation       increase; p  = 8 × 10(-13)), bipolar disorder (1.53[1.44; 1.63];       p = 1 × 10(-43)), schizophrenia (1.36[1.28; 1.45]; p = 4 × 10(-22)), psychiatric       cross-disorder susceptibility (1.23[1.16; 1.30]; p = 3 × 10(-12)), cortical       thickness of the transverse temporal gyrus (0.90[0.86; 0.96]; p = 5 × 10(-4)),       educational attainment (0.86[0.82; 0.91]; p = 2 × 10(-7)), and intelligence       (0.72[0.68; 0.76]; p = 9 × 10(-29)). This study suggests an important       contribution of common genetic variants to dyslexia risk, and novel genomic       overlaps with psychiatric conditions like bipolar disorder, schizophrenia, and       cross-disorder susceptibility. Moreover, it revealed the presence of shared       genetic foundations with a neural correlate previously implicated in dyslexia by       neuroimaging evidence.",What is the genetic basis of dyslexia and how does it overlap with other psychiatric conditions?,"
Answer: The genetic basis of dyslexia is not fully understood, but recent studies have identified several susceptibility genes and pathways that may contribute to its development. One such gene is LOC388780, which is located on chromosome 20p13 and is an uncharacterized gene. Another gene implicated in dyslexia is VEPH1, which is located on chromosome 3q25 and is involved in brain development. These genes may interact with other genes and pathways to affect the ability to read and process information.

In addition to these specific genes, dyslexia has been found to overlap with other psychiatric conditions, such as attention deficit hyperactivity disorder (ADHD), bipolar disorder, schizophrenia, and cross-disorder susceptibility. This suggests that dyslexia may be related to a broader set of genetic and neural factors that affect cognitive and psychiatric functioning.

Furthermore, recent studies have identified shared genetic foundations between dyslexia and these other psychiatric conditions. For example, dyslexia has been found to be associated with reduced cortical thickness in the transverse temporal gyrus, which is also implicated in ADHD and other psychiatric disorders. Dyslexia has also been found to be associated with reduced educational attainment and intelligence, which are also risk factors for psychiatric disorders.

Overall, the genetic basis of dyslexia is complex and multifactorial, and further research is needed to fully understand its underlying mechanisms and how they relate to other psychiatric conditions."
"The novel corona virus SARS-CoV-2 (COVID-19) has exposed the world to challenges      never before seen in fast diagnostics, monitoring, and prevention of the       outbreak. As a result, different approaches for fast diagnostic and screening are       made and yet to find the ideal way. The current mini-review provides and examines       evidence-based innovative and rapid chemical sensing and related biodiagnostic       solutions to deal with infectious disease and related pandemic emergencies, which       could offer the best possible care for the general population and improve the       approachability of the pandemic information, insights, and surrounding contexts.       The review discusses how integration of sensing devices with big data analysis,       artificial Intelligence or machine learning, and clinical decision support       system, could improve the accuracy of the recorded patterns of the disease       conditions within an ocean of information. At the end, the mini-review provides a       prospective on the requirements to improve our coping of the pandemic-related       biodiagnostics as well as future opportunities.","What are the current approaches for fast diagnostic and screening of COVID-19, and how can the integration of sensing devices with big data analysis, artificial intelligence, and clinical decision support systems improve the accuracy of the recorded patterns of the disease conditions within an ocean of information?","Answer: The current approaches for fast diagnostic and screening of COVID-19 include various methods such as polymerase chain reaction (PCR), rapid diagnostic tests (RDTs), and serological tests. However, these methods have limitations such as long turnaround times, high cost, and low sensitivity. The integration of sensing devices with big data analysis, artificial intelligence, and clinical decision support systems can improve the accuracy of the recorded patterns of the disease conditions within an ocean of information. Sensing devices can provide real-time monitoring of the disease, while big data analysis can help identify patterns and trends in the data. Artificial intelligence and machine learning can be used to analyze the data and provide insights into the disease, while clinical decision support systems can help healthcare professionals make informed decisions about patient care. Overall, the integration of these technologies can improve the accuracy and speed of COVID-19 diagnosis and screening, and help healthcare professionals provide better care to patients."
"COVID-19 has infected millions of people worldwide over the past few years. The      main technique used for COVID-19 detection is reverse transcription, which is       expensive, sensitive, and requires medical expertise. X-ray imaging is an       alternative and more accessible technique. This study aimed to improve detection       accuracy to create a computer-aided diagnostic tool. Combining other artificial       intelligence applications techniques with radiological imaging can help detect       different diseases. This study proposes a technique for the automatic detection       of COVID-19 and other chest-related diseases using digital chest X-ray images of       suspected patients by applying transfer learning (TL) algorithms. For this       purpose, two balanced datasets, Dataset-1 and Dataset-2, were created by       combining four public databases and collecting images from recently published       articles. Dataset-1 consisted of 6000 chest X-ray images with 1500 for each       class. Dataset-2 consisted of 7200 images with 1200 for each class. To train and       test the model, TL with nine pretrained convolutional neural networks (CNNs) was       used with augmentation as a preprocessing method. The network was trained to       classify using five classifiers: two-class classifier (normal and COVID-19);       three-class classifier (normal, COVID-19, and viral pneumonia), four-class       classifier (normal, viral pneumonia, COVID-19, and tuberculosis (Tb)), five-class       classifier (normal, bacterial pneumonia, COVID-19, Tb, and pneumothorax), and       six-class classifier (normal, bacterial pneumonia, COVID-19, viral pneumonia, Tb,       and pneumothorax). For two, three, four, five, and six classes, our model       achieved a maximum accuracy of 99.83, 98.11, 97.00, 94.66, and 87.29%,       respectively.",What is the study's aim and what is the proposed technique for the automatic detection of COVID-19 and other chest-related diseases using digital chest X-ray images of suspected patients?,"The study aims to improve detection accuracy to create a computer-aided diagnostic tool. The proposed technique for the automatic detection of COVID-19 and other chest-related diseases using digital chest X-ray images of suspected patients is transfer learning (TL) algorithms. For this purpose, two balanced datasets, Dataset-1 and Dataset-2, were created by combining four public databases and collecting images from recently published articles. The network was trained to classify using five classifiers: two-class classifier (normal and COVID-19), three-class classifier (normal, COVID-19, and viral pneumonia), four-class classifier (normal, viral pneumonia, COVID-19, and tuberculosis (Tb)), five-class classifier (normal, bacterial pneumonia, COVID-19, Tb, and pneumothorax), and six-class classifier (normal, bacterial pneumonia, COVID-19, viral pneumonia, Tb, and pneumothorax). For two, three, four, five, and six classes, our model achieved a maximum accuracy of 99.83, 98.11, 97.00, 94.66, and 87.29%, respectively."
"Over the last decade, deep learning (DL) has contributed a paradigm shift in      computer vision and image recognition creating widespread opportunities of using       artificial intelligence in research as well as industrial applications. DL has       been extensively studied in medical imaging applications, including those related       to pulmonary diseases. Chronic obstructive pulmonary disease, asthma, lung       cancer, pneumonia, and, more recently, COVID-19 are common lung diseases       affecting nearly 7.4% of world population. Pulmonary imaging has been widely       investigated toward improving our understanding of disease etiologies and early       diagnosis and assessment of disease progression and clinical outcomes. DL has       been broadly applied to solve various pulmonary image processing challenges       including classification, recognition, registration, and segmentation. This paper       presents a survey of pulmonary diseases, roles of imaging in translational and       clinical pulmonary research, and applications of different DL architectures and       methods in pulmonary imaging with emphasis on DL-based segmentation of major       pulmonary anatomies such as lung volumes, lung lobes, pulmonary vessels, and       airways as well as thoracic musculoskeletal anatomies related to pulmonary       diseases.",What is the role of deep learning in pulmonary imaging and how has it been applied to solve various challenges in this field?,"Deep learning has played a significant role in pulmonary imaging by providing a paradigm shift in computer vision and image recognition. It has created widespread opportunities for using artificial intelligence in research as well as industrial applications. In medical imaging, deep learning has been extensively studied in pulmonary diseases, including those related to chronic obstructive pulmonary disease, asthma, lung cancer, pneumonia, and COVID-19. These diseases affect nearly 7.4% of the world population, and pulmonary imaging has been widely investigated to improve our understanding of disease etiologies and early diagnosis and assessment of disease progression and clinical outcomes.
Deep learning has been broadly applied to solve various pulmonary image processing challenges, including classification, recognition, registration, and segmentation. In particular, deep learning-based segmentation of major pulmonary anatomies such as lung volumes, lung lobes, pulmonary vessels, and airways as well as thoracic musculoskeletal anatomies related to pulmonary diseases has been a major focus of research. This has enabled researchers to accurately identify and quantify these anatomies in medical images, which has important clinical implications for the diagnosis and treatment of pulmonary diseases. Overall, deep learning has revolutionized pulmonary imaging and has opened up new avenues for research and clinical applications."
"BACKGROUND: By 2050, almost 5 billion people globally are projected to have      myopia, of whom 20% are likely to have high myopia with clinically significant       risk of sight-threatening complications such as myopic macular degeneration.       These are diagnoses that typically require specialist assessment or measurement       with multiple unconnected pieces of equipment. Artificial intelligence (AI)       approaches might be effective for risk stratification and to identify individuals       at highest risk of visual loss. However, unresolved challenges for AI medical       studies remain, including paucity of transparency, auditability, and       traceability. METHODS: In this retrospective multicohort study, we developed and       tested retinal photograph-based deep learning algorithms for detection of myopic       macular degeneration and high myopia, using a total of 226 686 retinal images.       First we trained and internally validated the algorithms on datasets from       Singapore, and then externally tested them on datasets from China, Taiwan, India,       Russia, and the UK. We also compared the performance of the deep learning       algorithms against six human experts in the grading of a randomly selected       dataset of 400 images from the external datasets. As proof of concept, we used a       blockchain-based AI platform to demonstrate the real-world application of secure       data transfer, model transfer, and model testing across three sites in Singapore       and China. FINDINGS: The deep learning algorithms showed robust diagnostic       performance with areas under the receiver operating characteristic curves [AUC]       of 0·969 (95% CI 0·959-0·977) or higher for myopic macular degeneration and 0·913       (0·906-0·920) or higher for high myopia across the external testing datasets with       available data. In the randomly selected dataset, the deep learning algorithms       outperformed all six expert graders in detection of each condition (AUC of 0·978       [0·957-0·994] for myopic macular degeneration and 0·973 [0·941-0·995] for high       myopia). We also successfully used blockchain technology for data transfer, model       transfer, and model testing between sites and across two countries.       INTERPRETATION: Deep learning algorithms can be effective tools for risk       stratification and screening of myopic macular degeneration and high myopia among       the large global population with myopia. The blockchain platform developed here       could potentially serve as a trusted platform for performance testing of future       AI models in medicine. FUNDING: None.","What is the effectiveness of deep learning algorithms in detecting myopic macular degeneration and high myopia among individuals with myopia, and how can blockchain technology be used to ensure transparency, auditability, and traceability in the development and testing of these algorithms?","Answer: The deep learning algorithms developed in this study showed robust diagnostic performance in detecting myopic macular degeneration and high myopia among individuals with myopia, with areas under the receiver operating characteristic curves of 0·969 or higher for myopic macular degeneration and 0·913 or higher for high myopia across the external testing datasets. The algorithms outperformed all six expert graders in detection of each condition in a randomly selected dataset. Additionally, the study successfully used blockchain technology for data transfer, model transfer, and model testing between sites and across two countries, demonstrating its potential as a trusted platform for performance testing of future AI models in medicine."
"Idiopathic pulmonary arterial hypertension (iPAH) is a rare progressive,      life-shortening disease, usually diagnosed at an advanced stage. We hypothesize       that patients with iPAH exhibit patterns of health-seeking behavior before       diagnosis that will allow the development of earlier identification tools. The       Sheffield Pulmonary Hypertension IndeX (SPHInX) project aims to develop a       predictive algorithm based on routinely collected healthcare resource utilization       (HCRU) data. This report focuses on the initial feasibility of the project,       examining whether Hospital Episode Statistics (HES) data from the National Health       Service in England have sufficient richness to support the development of an       early diagnosis algorithm. This is a two-stage study. First, hospital       interactions during 2009-2014 captured in HES data identified 127,815 adult       patients with pulmonary hypertension (PH) ICD-10 codes, containing a probable       iPAH cohort with incidence and demographics similar to the reported literature.       HCRU was high in the three years before diagnosis. Second, to examine HCRU in       patients with a confirmed iPAH diagnosis, we built the SPHInX dataset       incorporating all patients investigated for suspected PH in the Sheffield       Pulmonary Vascular Disease Unit during 2008-2016 (n = 6674). For the SPHInX       dataset, data could be linked to HES in 98.6% of cases and patients with       confirmed iPAH had similar levels of pre-diagnosis HCRU. In conclusion, patients       with probable iPAH identified using HES and patients with confirmed iPAH have       high levels of HCRU for several years before diagnosis. Artificial intelligence       models will now be used to develop the SPHInX algorithm to screen for undiagnosed       iPAH in the general population.",What is the hypothesis of the SPHInX project and what is the initial feasibility study's focus?,"
Answer: The hypothesis of the SPHInX project is that patients with iPAH exhibit patterns of health-seeking behavior before diagnosis that will allow the development of earlier identification tools. The initial feasibility study's focus is to examine whether Hospital Episode Statistics (HES) data from the National Health Service in England have sufficient richness to support the development of an early diagnosis algorithm. This is a two-stage study. First, hospital interactions during 2009-2014 captured in HES data identified 127,815 adult patients with pulmonary hypertension (PH) ICD-10 codes, containing a probable iPAH cohort with incidence and demographics similar to the reported literature. HCRU was high in the three years before diagnosis. Second, to examine HCRU in patients with a confirmed iPAH diagnosis, we built the SPHInX dataset incorporating all patients investigated for suspected PH in the Sheffield Pulmonary Vascular Disease Unit during 2008-2016 (n = 6674). For the SPHInX dataset, data could be linked to HES in 98.6% of cases and patients with confirmed iPAH had similar levels of pre-diagnosis HCRU. In conclusion, patients with probable iPAH identified using HES and patients with confirmed iPAH have high levels of HCRU for several years before diagnosis."
"Cyber-attacks have become one of the biggest problems of the world. They cause      serious financial damages to countries and people every day. The increase in       cyber-attacks also brings along cyber-crime. The key factors in the fight against       crime and criminals are identifying the perpetrators of cyber-crime and       understanding the methods of attack. Detecting and avoiding cyber-attacks are       difficult tasks. However, researchers have recently been solving these problems       by developing security models and making predictions through artificial       intelligence methods. A high number of methods of crime prediction are available       in the literature. On the other hand, they suffer from a deficiency in predicting       cyber-crime and cyber-attack methods. This problem can be tackled by identifying       an attack and the perpetrator of such attack, using actual data. The data include       the type of crime, gender of perpetrator, damage and methods of attack. The data       can be acquired from the applications of the persons who were exposed to       cyber-attacks to the forensic units. In this paper, we analyze cyber-crimes in       two different models with machine-learning methods and predict the effect of the       defined features on the detection of the cyber-attack method and the perpetrator.       We used eight machine-learning methods in our approach and concluded that their       accuracy ratios were close. The Support Vector Machine Linear was found out to be       the most successful in the cyber-attack method, with an accuracy rate of 95.02%.       In the first model, we could predict the types of attacks that the victims were       likely to be exposed to with a high accuracy. The Logistic Regression was the       leading method in detecting attackers with an accuracy rate of 65.42%. In the       second model, we predicted whether the perpetrators could be identified by       comparing their characteristics. Our results have revealed that the probability       of cyber-attack decreases as the education and income level of victim increases.       We believe that cyber-crime units will use the proposed model. It will also       facilitate the detection of cyber-attacks and make the fight against these       attacks easier and more effective.","What is the most successful machine learning method in predicting cyber-attack methods and perpetrators, and what are the characteristics of the perpetrators that can be used to identify them?","Answer: The most successful machine learning method in predicting cyber-attack methods and perpetrators is the Support Vector Machine Linear, with an accuracy rate of 95.02%. The characteristics of the perpetrators that can be used to identify them include education and income level."
"Artificial intelligence (AI), with its subdivisions (machine and deep learning),      is a new branch of computer science that has shown impressive results across a       variety of domains. The applications of AI to medicine and biology are being       widely investigated. Medical specialties that rely heavily on images, including       radiology, dermatology, oncology and ophthalmology, were the first to explore AI       approaches in analysis and diagnosis. Applications of AI in ophthalmology have       concentrated on diseases with high prevalence, such as diabetic retinopathy,       retinopathy of prematurity, age-related macular degeneration (AMD), and glaucoma.       Here we provide an overview of AI applications for diagnosis, classification, and       clinical management of AMD and other macular dystrophies.","What are some of the applications of artificial intelligence (AI) in ophthalmology, specifically in the diagnosis, classification, and clinical management of age-related macular degeneration (AMD) and other macular dystrophies?","Answer: Some of the applications of AI in ophthalmology include the diagnosis, classification, and clinical management of age-related macular degeneration (AMD) and other macular dystrophies. AI approaches have been used to analyze and diagnose diseases such as diabetic retin"
"OBJECTIVE: The correct production of speech depends on the effective use of      inhibitory control. Cocaine abuse has been linked to impaired inhibition in the       verbal and nonverbal domains. The aim of this study was to evaluate the possible       impairment of the inhibitory control process engaged in the production of       language among chronic cocaine users, both in rehabilitation and recreational       contexts. METHOD: Researchers obtained an index of semantic interference from a       picture-word task performed by chronic cocaine users in rehabilitation       (Experiment 1) and recreational cocaine polydrug users (Experiment 2). Cocaine       users in both groups were matched for age and intelligence with cocaine-free       health controls. Performance on the picture-word task was analyzed by       repeated-measures analyses of variance. RESULTS: Both groups of cocaine users       showed significantly more semantic interference than their respective       cocaine-free control group. These results suggest a deficit in the ability to       inhibit interfering information. CONCLUSIONS: The present findings suggest that       cocaine use, even at recreational levels, is associated with specific impairments       in the inhibitory mechanism that reduces the activation of overt competing       responses in language production. This impairment results in the inefficient       avoidance of irrelevant information, inducing errors and slower responses during       the production of spoken language.",What is the relationship between cocaine abuse and the impairment of inhibitory control in language production?,"Answer: Cocaine abuse has been linked to impaired inhibition in the verbal and nonverbal domains. The aim of this study was to evaluate the possible impairment of the inhibitory control process engaged in the production of language among chronic cocaine users, both in rehabilitation and recreational contexts. The results suggest that both groups of cocaine users showed significantly more semantic interference than their respective cocaine-free control group, indicating a deficit in the ability to inhibit interfering information. This impairment results in the inefficient avoidance of irrelevant information, inducing errors and slower responses during the production of spoken language."
"The efficacy and safety of drugs are widely known to be determined by their      interactions with multiple molecules of pharmacological importance, and it is       therefore essential to systematically depict the molecular atlas and       pharma-information of studied drugs. However, our understanding of such       information is neither comprehensive nor precise, which necessitates the       construction of a new database providing a network containing a large number of       drugs and their interacting molecules. Here, a new database describing the       molecular atlas and pharma-information of drugs (DrugMAP) was therefore       constructed. It provides a comprehensive list of interacting molecules for >30       000 drugs/drug candidates, gives the differential expression patterns for >5000       interacting molecules among different disease sites, ADME (absorption,       distribution, metabolism and excretion)-relevant organs and physiological       tissues, and weaves a comprehensive and precise network containing >200 000       interactions among drugs and molecules. With the great efforts made to clarify       the complex mechanism underlying drug pharmacokinetics and pharmacodynamics and       rapidly emerging interests in artificial intelligence (AI)-based network       analyses, DrugMAP is expected to become an indispensable supplement to existing       databases to facilitate drug discovery. It is now fully and freely accessible at:       https://idrblab.org/drugmap/.",What is the purpose of the DrugMAP database and what information does it provide?,"
Answer: The DrugMAP database is a comprehensive database that provides a network containing a large number of drugs and their interacting molecules. It provides a list of interacting molecules for over 30,000 drugs/drug candidates, differential expression patterns for over 5,000 interacting molecules among different disease sites, ADME-relevant organs and physiological tissues, and weaves a comprehensive and precise network containing over 200,000 interactions among drugs and molecules. The database is expected to become an indispensable supplement to existing databases to facilitate drug discovery. It is now fully and freely accessible at: https://idrblab.org/drugmap/."
