{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyanshu28/MediMind-INLPT-WS2023/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = 'PubMed Data/PubMedTextFiles/abstract-intelligen-set-2013.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 64133 lines in the data.\n",
            "There are 10687 paragraphs in the data.\n"
          ]
        }
      ],
      "source": [
        "# Open the text file\n",
        "with open(file_path, 'r', encoding='utf-8') as in_file:\n",
        "    # Read the entire text file\n",
        "    data = in_file.read()\n",
        "\n",
        "    # Split the data into lines\n",
        "    lines = data.split('\\n')\n",
        "    print(f'There are {len(lines)} lines in the data.')\n",
        "\n",
        "    # Split the data into paragraphs\n",
        "    paragraphs = data.split('\\n\\n')\n",
        "    print(f'There are {len(paragraphs)} paragraphs in the data.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Define a function to parse the data\n",
        "def parse_data(data):\n",
        "    # Split the data into records\n",
        "    records = data.split('\\n\\n')\n",
        "    # Initialize a list to store the parsed data\n",
        "    parsed_data = []\n",
        "    # Iterate over each record\n",
        "    for record in records:\n",
        "        # Split the record into lines\n",
        "        lines = record.split('\\n')\n",
        "        # Initialize an empty dictionary to store the record data\n",
        "        record_data = {}\n",
        "        # Iterate over each line\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                # Split the line into key and value\n",
        "                key, value = line.split(':', 1)\n",
        "                # Remove leading and trailing whitespace\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                # Add the key-value pair to the record data\n",
        "                record_data[key] = value\n",
        "        # Add the record data to the parsed data\n",
        "        parsed_data.append(record_data)\n",
        "    # Return the parsed data\n",
        "    return parsed_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "dict contains fields not in fieldnames: 'Amemiya A, editors. GeneReviews(®) [Internet]. Seattle (WA)', 'In'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Write the parsed data to the csv file\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_data\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[1;34m(self, rowdicts)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterows\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdicts):\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowdicts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
            "\u001b[1;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'Amemiya A, editors. GeneReviews(®) [Internet]. Seattle (WA)', 'In'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Open the text file and csv file\n",
        "with open('input.txt', 'r', encoding='utf-8') as in_file, open('output.csv', 'w', newline='', encoding='utf-8') as out_file:\n",
        "    # Read the entire text file\n",
        "    data = in_file.read()\n",
        "    # Parse the data\n",
        "    parsed_data = parse_data(data)\n",
        "    # Create a csv writer object\n",
        "    writer = csv.DictWriter(out_file, fieldnames=['Title', 'Author Information', 'Clinical Characteristics', 'Clinical Information', 'Copyright', 'PMID'])\n",
        "    # Write the header to the csv file\n",
        "    writer.writeheader()\n",
        "    # Write the parsed data to the csv file\n",
        "    writer.writerows(parsed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Define a function to parse the data\n",
        "def parse_data(data):\n",
        "    # Split the data into records\n",
        "    records = data.split('\\n\\n')\n",
        "    # Initialize a list to store the parsed data\n",
        "    parsed_data = []\n",
        "    # Iterate over each record\n",
        "    for record in records:\n",
        "        # Split the record into lines\n",
        "        lines = record.split('\\n')\n",
        "        # Initialize an empty dictionary to store the record data\n",
        "        record_data = {}\n",
        "        # Iterate over each line\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                # Split the line into key and value\n",
        "                key, value = line.split(':', 1)\n",
        "                # Remove leading and trailing whitespace\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                # Add the key-value pair to the record data\n",
        "                record_data[key] = value\n",
        "        # Add the record data to the parsed data\n",
        "        parsed_data.append(record_data)\n",
        "    # Return the parsed data\n",
        "    return parsed_data\n",
        "\n",
        "# Open the text file and csv file\n",
        "with open('input.txt', 'r', encoding='utf-8') as in_file, open('output.csv', 'w', newline='', encoding='utf-8') as out_file:\n",
        "    # Read the entire text file\n",
        "    data = in_file.read()\n",
        "    # Parse the data\n",
        "    parsed_data = parse_data(data)\n",
        "    # Create a csv writer object\n",
        "    writer = csv.DictWriter(out_file, fieldnames=['Title', 'Author Information', 'Clinical Characteristics', 'Clinical Information', 'Copyright', 'PMID'])\n",
        "    # Write the header to the csv file\n",
        "    writer.writeheader()\n",
        "    # Write the parsed data to the csv file\n",
        "    for row in parsed_data:\n",
        "        # Only write rows that contain the correct fields\n",
        "        if all(field in row for field in writer.fieldnames):\n",
        "            writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1623\n",
            "----------------------------------------\n",
            "Toxicol Appl Pharmacol. 2013 Sep 15;271(3):309-23. doi: \n",
            "10.1016/j.taap.2010.03.019. Epub 2010 Mar 29.\n",
            "\n",
            "Approaches to advancing quantitative human health risk assessment of \n",
            "environmental chemicals in the post-genomic era.\n",
            "\n",
            "Chiu WA(1), Euling SY, Scott CS, Subramaniam RP.\n",
            "\n",
            "Author information:\n",
            "(1)National Center for Environmental Assessment, U.S. Environmental Protection \n",
            "Agency, Washington DC, 20460, USA. Electronic address: chiu.weihsueh@epa.gov.\n",
            "\n",
            "The contribution of genomics and associated technologies to human health risk \n",
            "assessment for environmental chemicals has focused largely on elucidating \n",
            "mechanisms of toxicity, as discussed in other articles in this issue. However, \n",
            "there is interest in moving beyond hazard characterization to making more direct \n",
            "impacts on quantitative risk assessment (QRA)--i.e., the determination of \n",
            "toxicity values for setting exposure standards and cleanup values. We propose \n",
            "that the evolution of QRA of environmental chemicals in the post-genomic era \n",
            "will involve three, somewhat overlapping phases in which different types of \n",
            "approaches begin to mature. The initial focus (in Phase I) has been and \n",
            "continues to be on \"augmentation\" of weight of evidence--using genomic and \n",
            "related technologies qualitatively to increase the confidence in and scientific \n",
            "basis of the results of QRA. Efforts aimed towards \"integration\" of these data \n",
            "with traditional animal-based approaches, in particular quantitative predictors, \n",
            "or surrogates, for the in vivo toxicity data to which they have been anchored \n",
            "are just beginning to be explored now (in Phase II). In parallel, there is a \n",
            "recognized need for \"expansion\" of the use of established biomarkers of \n",
            "susceptibility or risk of human diseases and disorders for QRA, particularly for \n",
            "addressing the issues of cumulative assessment and population risk. Ultimately \n",
            "(in Phase III), substantial further advances could be realized by the \n",
            "development of novel molecular and pathway-based biomarkers and statistical and \n",
            "in silico models that build on anticipated progress in understanding the \n",
            "pathways of human diseases and disorders. Such efforts would facilitate a \n",
            "gradual \"reorientation\" of QRA towards approaches that more directly link \n",
            "environmental exposures to human outcomes.\n",
            "\n",
            "Published by Elsevier Inc.\n",
            "\n",
            "DOI: 10.1016/j.taap.2010.03.019\n",
            "PMID: 20353796 [Indexed for MEDLINE]\n",
            "\n",
            "----------------------------------------\n",
            "Assessment. 2013 Apr;20(2):242-52. doi: 10.1177/1073191111411658. Epub 2011 Jun \n",
            "10.\n",
            "\n",
            "How much power and speed is measured in this test?\n",
            "\n",
            "Partchev I(1), De Boeck P, Steyer R.\n",
            "\n",
            "Author information:\n",
            "(1)K. U. Leuven, Leuven, Belgium.\n",
            "\n",
            "An old issue in psychological assessment is to what extent power and speed each \n",
            "are measured by a given intelligence test. Starting from accuracy and response \n",
            "time data, an approach based on posterior time limits (cut-offs of recorded \n",
            "response time) leads to three kinds of recoded data: time data (whether or not \n",
            "the response precedes the cut-off), time-accuracy data (whether or not a \n",
            "response is correct and precedes the cut-off), and accuracy data (as \n",
            "time-accuracy data, but coded as missing when not preceding the time cut-off). \n",
            "Each type of data can be modeled as binary responses. Speed and power are \n",
            "investigated through the effect of posterior time limits on two main aspects: \n",
            "(a) the latent variable that is measured: whether it is more power-related or \n",
            "more speed-related; (b) how well the latent variable (of whatever kind) is \n",
            "measured through the item(s). As empirical data, we use responses and response \n",
            "times for a verbal analogies test. The main findings are that, independent of \n",
            "the posterior time limit, basically the same latent speed trait was measured \n",
            "through the time data, and basically the same latent power trait was measured \n",
            "through the accuracy data, while for the time-accuracy data the nature of the \n",
            "latent trait moved from power to speed when the posterior time limit was \n",
            "reduced. It was also found that a reduction of the posterior time limit had no \n",
            "negative effect on the reliability of the latent trait measures (of whatever \n",
            "kind).\n",
            "\n",
            "DOI: 10.1177/1073191111411658\n",
            "PMID: 21665882 [Indexed for MEDLINE]\n",
            "\n",
            "----------------------------------------\n",
            "Autism. 2013 Mar;17(2):172-83. doi: 10.1177/1362361311409960. Epub 2011 Jun 29.\n",
            "\n",
            "Does central coherence relate to the cognitive performance of children with \n",
            "autism in dynamic assessments?\n",
            "\n",
            "Aljunied M(1), Frederickson N.\n",
            "\n",
            "Author information:\n",
            "(1)University College London, UK, and Ministry of Education, Singapore.\n",
            "\n",
            "Central coherence refers to an in-built propensity to form meaningful links over \n",
            "a wide range of stimuli and to generalize over as wide a range of contexts as \n",
            "possible. In children with autism this ability is diminished, and the impact of \n",
            "central coherence deficits in children with autism have previously been observed \n",
            "using static measures of learning, such as reading comprehension test \n",
            "performance. In this study, the relationship between central coherence and more \n",
            "dynamic indicators of learning are investigated. The responses of 52 children \n",
            "with autism (mean age 9:10 years) on a test of central coherence and a dynamic \n",
            "assessment task were analysed. All the children showed significant improvements \n",
            "in dynamic assessment test scores after mediation; however, among those with \n",
            "below average nonverbal intelligence scores, weak central coherence was \n",
            "significantly associated with smaller gains in performance after teaching. \n",
            "Implications for the validity of dynamic assessments for children with autism \n",
            "are discussed.\n",
            "\n",
            "DOI: 10.1177/1362361311409960\n",
            "PMID: 21715547 [Indexed for MEDLINE]\n",
            "\n",
            "----------------------------------------\n",
            "Food Chem Toxicol. 2013 Apr;54:50-8. doi: 10.1016/j.fct.2011.06.052. Epub 2011 \n",
            "Jun 23.\n",
            "\n",
            "Effects of docosahexaenoic acid and methylmercury on child's brain development \n",
            "due to consumption of fish by Finnish mother during pregnancy: a probabilistic \n",
            "modeling approach.\n",
            "\n",
            "Leino O(1), Karjalainen AK, Tuomisto JT.\n",
            "\n",
            "Author information:\n",
            "(1)National Institute for Health and Welfare, Terveyden ja hyvinvoinnin laitos, \n",
            "Kuopio, Finland. Olli.leino@thl.fi\n",
            "\n",
            "Fish contains both beneficial substances e.g. docosahexaenoic acids but also \n",
            "harmful compounds e.g. methylmercury. Importantly, the health effects caused by \n",
            "these two substances can be evaluated in one common end point, intelligence \n",
            "quotient (IQ), providing a more transparent analysis. We estimated health \n",
            "effects of maternal fish consumption on child's central nervous system by \n",
            "creating a model with three alternative maternal fish consumption scenarios \n",
            "(lean fish, fatty fish, and current fish consumption). Additionally, we analyzed \n",
            "impacts of both regular fish consumption and extreme fish consumption habits. At \n",
            "the individual level, the simulated net effects were small, encompassing a range \n",
            "of one IQ point in all scenarios. Fatty fish consumption, however, clearly \n",
            "generated a beneficial net IQ effect, and lean fish consumption evoked an \n",
            "adverse net IQ effect. In view of the current fish consumption pattern of \n",
            "Finnish mothers the benefits and risks seem to more or less compensate each \n",
            "other. This study clearly shows the significance of which fish species are \n",
            "consumed during pregnancy and lactation, and the results can be generalized to \n",
            "apply to typical western population fish consumption habits.\n",
            "\n",
            "Copyright © 2011 Elsevier Ltd. All rights reserved.\n",
            "\n",
            "DOI: 10.1016/j.fct.2011.06.052\n",
            "PMID: 21723361 [Indexed for MEDLINE]\n",
            "\n",
            "dict_keys(['Assessment. 2013 Apr;20(2)', 'Author information', 'response time) leads to three kinds of recoded data', 'investigated through the effect of posterior time limits on two main aspects', '(a) the latent variable that is measured', 'DOI', 'PMID'])\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import ray\n",
        "import re\n",
        "from ray.data import from_items\n",
        "\n",
        "# Define a function to parse the data\n",
        "def parse_data(data):\n",
        "    # Split the data into records\n",
        "    # print(data)\n",
        "    # records = data.split('\\n\\n')\n",
        "    # Initialize a list to store the parsed data\n",
        "    pattern = r'\\n\\n\\d+\\.\\s'\n",
        "    # # Split the data into records\n",
        "    records = re.split(pattern, data)\n",
        "    records.pop(0)\n",
        "    print(len(records))\n",
        "    parsed_data = []\n",
        "    # Iterate over each record\n",
        "    for i,record in enumerate(records):\n",
        "        print(\"--\"*20)\n",
        "        print(record)\n",
        "        if i >= 3:\n",
        "            break\n",
        "        \n",
        "        # Split the record into lines\n",
        "        # print(record)\n",
        "        \n",
        "        lines = record.split('\\n')\n",
        "        \n",
        "        # Initialize an empty dictionary to store the record data\n",
        "        record_data = {}\n",
        "        # Initialize a variable to store the current key\n",
        "        current_key = None\n",
        "        # Iterate over each line\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                # Split the line into key and value\n",
        "                key, value = line.split(':', 1)\n",
        "                # Remove leading and trailing whitespace\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                # Add the key-value pair to the record data\n",
        "                record_data[key] = value\n",
        "                # Update the current key\n",
        "                current_key = key\n",
        "            elif current_key is not None:\n",
        "                # If the line does not contain a colon, add it to the current field\n",
        "                record_data[current_key] += ' ' + line.strip()\n",
        "        # Add the record data to the parsed data\n",
        "        parsed_data.append(record_data)\n",
        "    # Return the parsed data\n",
        "    return parsed_data\n",
        "\n",
        "# Open the text file and csv file\n",
        "with open(file_path, 'r', encoding='utf-8') as in_file, open('output.csv', 'w', newline='', encoding='utf-8') as out_file:\n",
        "    # Read the entire text file\n",
        "    data = in_file.read()\n",
        "    # Parse the data\n",
        "    parsed_data = parse_data(data)\n",
        "    # Create a csv writer object\n",
        "\n",
        "print(parsed_data[1].keys())\n",
        "\n",
        "# dataset = from_items(parsed_data)\n",
        "\n",
        "# # Print the first few records of the dataset\n",
        "# print(dataset.take(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import re\n",
        "from ray.data import from_items\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Define a function to parse the data\n",
        "def parse_data(data):\n",
        "    # Define the regular expression pattern\n",
        "    pattern = r'(?<=\\n)\\d+\\.\\s'\n",
        "    # Split the data into records\n",
        "    records = re.split(pattern, data)\n",
        "    # Initialize a list to store the parsed data\n",
        "    parsed_data = []\n",
        "    # Iterate over each record\n",
        "    for record in records:\n",
        "        # Split the record into lines\n",
        "        lines = record.split('\\n')\n",
        "        # Initialize an empty dictionary to store the record data\n",
        "        record_data = {}\n",
        "        # Initialize a variable to store the current key\n",
        "        current_key = None\n",
        "        # Iterate over each line\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                # Split the line into key and value\n",
        "                key, value = line.split(':', 1)\n",
        "                # Remove leading and trailing whitespace\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                # Add the key-value pair to the record data\n",
        "                record_data[key] = value\n",
        "                # Update the current key\n",
        "                current_key = key\n",
        "            elif current_key is not None:\n",
        "                # If the line does not contain a colon, add it to the current field\n",
        "                record_data[current_key] += ' ' + line.strip()\n",
        "        # Add the record data to the parsed data if it is not empty\n",
        "        if record_data:\n",
        "            parsed_data.append(record_data)\n",
        "    # Return the parsed data\n",
        "    return parsed_data\n",
        "\n",
        "# Open the text file\n",
        "with open('input.txt', 'r', encoding='utf-8') as in_file:\n",
        "    # Read the entire text file\n",
        "    data = in_file.read()\n",
        "    # Parse the data\n",
        "    parsed_data = parse_data(data)\n",
        "\n",
        "# Convert the parsed data into a Ray dataset\n",
        "dataset = from_items(parsed_data)\n",
        "\n",
        "# Print the first few records of the dataset\n",
        "print(dataset.take(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOKwDXBMJ3Yq7Bq+hZC/VYI",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
